{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061930ec-9660-4dae-aeb6-5c46325f6cc0",
   "metadata": {
    "id": "061930ec-9660-4dae-aeb6-5c46325f6cc0"
   },
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b18d4-0a15-400c-b9d6-36ad12925233",
   "metadata": {
    "id": "144b18d4-0a15-400c-b9d6-36ad12925233"
   },
   "source": [
    "## 파이썬으로 간단하게 구현한 퍼셉트론-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3226bdd1-30e7-498f-9846-bba187b8038a",
   "metadata": {
    "id": "3226bdd1-30e7-498f-9846-bba187b8038a",
    "outputId": "7fafbefb-4d30-447a-c6e4-c382e361c960"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X=np.array([1,0,1])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ac59493-900c-443d-8f86-320faf21702f",
   "metadata": {
    "id": "9ac59493-900c-443d-8f86-320faf21702f",
    "outputId": "f43b992d-8ae1-4408-bce7-f01d9781348f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  1. ,  1. ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W=np.array([\n",
    "    -0.5,\n",
    "    1.0,\n",
    "    1.0\n",
    "])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9855f20-085d-453e-bfbe-5c9c916bbc9a",
   "metadata": {
    "id": "e9855f20-085d-453e-bfbe-5c9c916bbc9a",
    "outputId": "bc28ceec-788c-4017-aa4d-06cab2de37be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=sum(X*W)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952b2e9-d5bd-4854-a75b-e895f78a044f",
   "metadata": {
    "id": "0952b2e9-d5bd-4854-a75b-e895f78a044f"
   },
   "source": [
    "## 파이썬으로 간단하게 구현한 퍼셉트론-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74503d9a-c713-429a-83ab-e3b5aa8fbab0",
   "metadata": {
    "id": "74503d9a-c713-429a-83ab-e3b5aa8fbab0",
    "outputId": "d7ca078e-e13c-4684-acff-8f6e856497b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X=np.array([\n",
    "            [1,0,0],\n",
    "            [1,0,1],\n",
    "            [1,1,0],\n",
    "            [1,1,1]\n",
    "])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0c70aa1-0e76-4b06-85e6-5e3967c894dc",
   "metadata": {
    "id": "b0c70aa1-0e76-4b06-85e6-5e3967c894dc",
    "outputId": "cdcee897-aaf7-49ff-e75e-b8f0fbbf0215"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  1. ,  1. ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W=np.array([\n",
    "    -0.5,\n",
    "    1.0,\n",
    "    1.0\n",
    "])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37f67dc5-f8b6-4619-aac6-4b274b5fe367",
   "metadata": {
    "id": "37f67dc5-f8b6-4619-aac6-4b274b5fe367",
    "outputId": "398066d8-6cf6-4663-f34c-9fb8fad60754"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  0.5,  0.5,  1.5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.sum(X*W, axis=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e2471-1409-43e2-b88d-4576bea0e7ef",
   "metadata": {
    "id": "8d5e2471-1409-43e2-b88d-4576bea0e7ef"
   },
   "source": [
    "## SLP를 통한 AND, OR, NOT 게이트 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c4b58-fe21-4fca-a559-0223c4904644",
   "metadata": {
    "id": "154c4b58-fe21-4fca-a559-0223c4904644"
   },
   "source": [
    "### 퍼셉트론 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f3cbc30-c961-4f84-a699-abd3e0389723",
   "metadata": {
    "id": "3f3cbc30-c961-4f84-a699-abd3e0389723",
    "outputId": "597ddbe5-7525-4616-c15e-23285f680de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ AND Perceptron +++\n",
      "[0 0] >>> 0\n",
      "[0 1] >>> 0\n",
      "[1 0] >>> 0\n",
      "[1 1] >>> 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def ANDperceptron(x1, x2):\n",
    "    w1, w2, theta = 0.5, 0.5, 0.7\n",
    "    tmp = x1*w1 + x2*w2\n",
    "    if tmp <= theta:\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1\n",
    "\n",
    "inputData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "print(\"+++ AND Perceptron +++\")\n",
    "for xs in inputData:\n",
    "    print(str(xs) + \" >>> \" + str(ANDperceptron(xs[0], xs[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ce972a2-7a72-4d88-8825-63abe05b4afd",
   "metadata": {
    "id": "5ce972a2-7a72-4d88-8825-63abe05b4afd",
    "outputId": "df71ef0c-4833-4aa2-90c9-e7cca6e52d9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ AND Perceptron +++\n",
      "[0 0] >>> 0\n",
      "[0 1] >>> 0\n",
      "[1 0] >>> 0\n",
      "[1 1] >>> 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def ANDperceptron(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.7\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    elif tmp > 0:\n",
    "        return 1\n",
    "\n",
    "inputData = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "print(\"+++ AND Perceptron +++\")\n",
    "\n",
    "for xs in inputData:\n",
    "    print(str(xs) + \" >>> \" + str(ANDperceptron(xs[0], xs[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0cfad6e-f5cc-4ced-af45-372808e91379",
   "metadata": {
    "id": "b0cfad6e-f5cc-4ced-af45-372808e91379",
    "outputId": "4ba999e3-a34f-4c46-d0f2-c76ea62cbd08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [2.14037745 1.2763927 ]\n",
      "b: -9\n",
      "\n",
      "Test:\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#데이터 생성\n",
    "rng = np.random.RandomState(123)\n",
    "d = 2\n",
    "N = 10\n",
    "mean = 5\n",
    "x1 = rng.randn(N, d) + np.array([0, 0])\n",
    "x2 = rng.randn(N, d) + np.array([mean, mean])\n",
    "x = np.concatenate((x1, x2), axis=0)\n",
    "\n",
    "#단순 퍼셉트론\n",
    "w = np.zeros(d)\n",
    "b = 0\n",
    "\n",
    "def y(x):\n",
    "    return step(np.dot(w, x) + b)\n",
    "\n",
    "def step(x):\n",
    "    return 1 * (x > 0)\n",
    "\n",
    "def t(i):\n",
    "    if i < N:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "while True:\n",
    "    classified = True\n",
    "    for i in range(N * 2):\n",
    "        delta_w = (t(i) - y(x[i])) * x[i]\n",
    "        delta_b = (t(i) - y(x[i]))\n",
    "        w += delta_w\n",
    "        b += delta_b\n",
    "        classified *= all(delta_w == 0) * (delta_b == 0)\n",
    "    if classified:\n",
    "        break\n",
    "\n",
    "print('w:', w)\n",
    "print('b:', b)\n",
    "print('\\nTest:')\n",
    "print(y([0, 0]))  # => 0\n",
    "print(y([5, 5]))  # => 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3bd6ff-59da-4c86-b749-b576deff8876",
   "metadata": {
    "id": "cf3bd6ff-59da-4c86-b749-b576deff8876"
   },
   "source": [
    "### AND 게이트 단순 퍼셉트론 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f40d0d43-e891-4445-9bd8-2049ac2fb945",
   "metadata": {
    "id": "f40d0d43-e891-4445-9bd8-2049ac2fb945",
    "outputId": "a4fe8de6-18d2-4629-ea26-6c3a20e678ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1]\n",
      "0번째 반복 입니다\n",
      "0.01\n",
      "1번째 반복 입니다\n",
      "0.01\n",
      "2번째 반복 입니다\n",
      "-0.02\n",
      "3번째 반복 입니다\n",
      "0.01\n",
      "4번째 반복 입니다\n",
      "0.0\n",
      "[0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# AND 데이터 만들기\n",
    "\n",
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "# 1단계 : 가중치 초기화\n",
    "\n",
    "w = np.zeros(1 + X.shape[1])\n",
    "\n",
    "# 최초 분류\n",
    "\n",
    "input_signal = np.dot(X, w[1:]) + w[0]\n",
    "def out_put(X):\n",
    "    return np.where(X >= 0.0, 1, 0)\n",
    "pred_y = out_put(input_signal)\n",
    "print(pred_y)\n",
    "\n",
    "# 가중치 최적화 시작\n",
    "\n",
    "errors = []\n",
    "epoch = 100\n",
    "for time in range(0, epoch):\n",
    "    print(\"{0}번째 반복 입니다\".format(time))\n",
    "    eta = 0.01\n",
    "    idx = 0\n",
    "    for xi, target in zip(X, y):\n",
    "        updated_w = eta * (target - pred_y[idx])\n",
    "        w[1:] += updated_w * xi\n",
    "        w[0] += updated_w\n",
    "        idx += 1\n",
    "    input_signal = np.dot(X, w[1:]) + w[0]\n",
    "    pred_y = out_put(input_signal)\n",
    "    errors.append(((y - pred_y).sum()) / 100)\n",
    "    print(((y - pred_y).sum()) / 100)\n",
    "    if ((y - pred_y).sum()) / 100 == 0:\n",
    "        break\n",
    "\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8d8cb-4c8a-4f60-9b06-e9dcf0984650",
   "metadata": {
    "id": "5cf8d8cb-4c8a-4f60-9b06-e9dcf0984650"
   },
   "source": [
    "### OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d2c1f9e-f29b-4cab-8a9d-681523c40eb6",
   "metadata": {
    "id": "0d2c1f9e-f29b-4cab-8a9d-681523c40eb6",
    "outputId": "577e382c-33dc-4ce3-bc50-fb59c6879a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1]\n",
      "0번째 반복 입니다\n",
      "0.03\n",
      "1번째 반복 입니다\n",
      "-0.01\n",
      "2번째 반복 입니다\n",
      "-0.01\n",
      "3번째 반복 입니다\n",
      "-0.01\n",
      "4번째 반복 입니다\n",
      "0.0\n",
      "[0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# OR 데이터 만들기\n",
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([0, 1, 1, 1])\n",
    "\n",
    "# 1단계 : 가중치 초기화\n",
    "w = np.zeros(1 + X.shape[1])\n",
    "\n",
    "# 최초 분류\n",
    "input_signal = np.dot(X, w[1:]) + w[0]\n",
    "\n",
    "def out_put(X):\n",
    "    return np.where(X >= 0.0, 1, 0)\n",
    "\n",
    "pred_y = out_put(input_signal)\n",
    "print(pred_y)\n",
    "\n",
    "# 가중치 최적화 시작\n",
    "errors = []\n",
    "epoch = 100\n",
    "for time in range(0, epoch):\n",
    "    print(\"{0}번째 반복 입니다\".format(time))\n",
    "    eta = 0.01\n",
    "    idx = 0\n",
    "    for xi, target in zip(X, y):\n",
    "        updated_w = eta * (target - pred_y[idx])\n",
    "        w[1:] += updated_w * xi\n",
    "        w[0] += updated_w\n",
    "        idx += 1\n",
    "    input_signal = np.dot(X, w[1:]) + w[0]\n",
    "    pred_y = out_put(input_signal)\n",
    "    errors.append(((y - pred_y).sum()) / 100)\n",
    "    print(((y - pred_y).sum()) / 100)\n",
    "    if ((y - pred_y).sum()) / 100 == 0:\n",
    "        break\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db55e34-b689-472c-9783-59bfe852bd8b",
   "metadata": {
    "id": "9db55e34-b689-472c-9783-59bfe852bd8b"
   },
   "source": [
    "### NAND 게이트 단순 퍼셉트론 화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8551fe99-bf7d-4f94-8ff5-9a3d27fa8bc4",
   "metadata": {
    "id": "8551fe99-bf7d-4f94-8ff5-9a3d27fa8bc4",
    "outputId": "5ab74014-d1e5-4fc9-a429-0589f92efbe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1]\n",
      "0번째 반복 입니다\n",
      "0.01\n",
      "1번째 반복 입니다\n",
      "0.01\n",
      "2번째 반복 입니다\n",
      "0.01\n",
      "3번째 반복 입니다\n",
      "0.0\n",
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# NAND 데이터 만들기\n",
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([1, 0, 0, 0])\n",
    "\n",
    "# 1단계 : 가중치 초기화\n",
    "w = np.zeros(1 + X.shape[1])\n",
    "\n",
    "# 최초 분류\n",
    "input_signal = np.dot(X, w[1:]) + w[0]\n",
    "\n",
    "def out_put(X):\n",
    "    return np.where(X >= 0.0, 1, 0)\n",
    "\n",
    "pred_y = out_put(input_signal)\n",
    "print(pred_y)\n",
    "\n",
    "# 가중치 최적화 시작\n",
    "errors = []\n",
    "epoch = 100\n",
    "for time in range(0, epoch):\n",
    "    print(\"{0}번째 반복 입니다\".format(time))\n",
    "    eta = 0.01\n",
    "    idx = 0\n",
    "    for xi, target in zip(X, y):\n",
    "        updated_w = eta * (target - pred_y[idx])\n",
    "        w[1:] += updated_w * xi\n",
    "        w[0] += updated_w\n",
    "        idx += 1\n",
    "    input_signal = np.dot(X, w[1:]) + w[0]\n",
    "    pred_y = out_put(input_signal)\n",
    "    errors.append(((y - pred_y).sum()) / 100)\n",
    "    print(((y - pred_y).sum()) / 100)\n",
    "    if ((y - pred_y).sum()) / 100 == 0:\n",
    "        break\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea87508-2202-4471-81c7-3a7ea9884546",
   "metadata": {
    "id": "3ea87508-2202-4471-81c7-3a7ea9884546"
   },
   "source": [
    "### Quiz- NOR  게이트 단순 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f999c31-2ae5-43a9-a3a7-c8fc1febd948",
   "metadata": {
    "id": "5f999c31-2ae5-43a9-a3a7-c8fc1febd948",
    "outputId": "c5ffe5fc-f0d4-4f06-e742-018cf4b28ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1]\n",
      "0번째 반복 입니다\n",
      "0.03\n",
      "1번째 반복 입니다\n",
      "-0.01\n",
      "2번째 반복 입니다\n",
      "0.0\n",
      "[1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Quiz- NOR 데이터 만들기\n",
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([1, 1, 1, 0])\n",
    "\n",
    "# 1단계 : 가중치 초기화\n",
    "w = np.zeros(1 + X.shape[1])\n",
    "\n",
    "# 최초 분류\n",
    "input_signal = np.dot(X, w[1:]) + w[0]\n",
    "\n",
    "def out_put(X):\n",
    "    return np.where(X >= 0.0, 1, 0)\n",
    "\n",
    "pred_y = out_put(input_signal)\n",
    "print(pred_y)\n",
    "\n",
    "# 가중치 최적화 시작\n",
    "errors = []\n",
    "epoch = 100\n",
    "for time in range(0, epoch):\n",
    "    print(\"{0}번째 반복 입니다\".format(time))\n",
    "    eta = 0.01\n",
    "    idx = 0\n",
    "    for xi, target in zip(X, y):\n",
    "        updated_w = eta * (target - pred_y[idx])\n",
    "        w[1:] += updated_w * xi\n",
    "        w[0] += updated_w\n",
    "        idx += 1\n",
    "    input_signal = np.dot(X, w[1:]) + w[0]\n",
    "    pred_y = out_put(input_signal)\n",
    "    errors.append(((y - pred_y).sum()) / 100)\n",
    "    print(((y - pred_y).sum()) / 100)\n",
    "    if ((y - pred_y).sum()) / 100 == 0:\n",
    "        break\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35311f7-358e-45ba-bd0e-c862ac41f7b6",
   "metadata": {
    "id": "f35311f7-358e-45ba-bd0e-c862ac41f7b6"
   },
   "source": [
    "### XOR 게이트 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea36ed5c-b317-46fd-bc6d-7e56c8fd477e",
   "metadata": {
    "id": "ea36ed5c-b317-46fd-bc6d-7e56c8fd477e"
   },
   "outputs": [],
   "source": [
    "def AND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.7\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    elif tmp > 0:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def OR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([0.5, 0.5])\n",
    "    b = -0.2\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def NAND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    w = np.array([-0.5, -0.5])\n",
    "    b = 0.7\n",
    "    tmp = np.sum(w*x) + b\n",
    "    if tmp <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def XOR(x1, x2):\n",
    "    s1 = NAND(x1, x2)\n",
    "    s2 = OR(x1, x2)\n",
    "    y = AND(s1, s2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5417224-5d89-44db-8dfe-1cfea057b957",
   "metadata": {
    "id": "a5417224-5d89-44db-8dfe-1cfea057b957",
    "outputId": "ac473bac-b6d9-44ce-94d2-aa62f8b1bbd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(XOR(0,0))\n",
    "print(XOR(1,0))\n",
    "print(XOR(0,1))\n",
    "print(XOR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb208bf3-f029-4b71-be7d-487cab5bb408",
   "metadata": {
    "id": "eb208bf3-f029-4b71-be7d-487cab5bb408"
   },
   "source": [
    "# XOR 문제를 MLP로 풀어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f91e4-53c5-4879-a1c2-c4a479ec347e",
   "metadata": {
    "id": "f10f91e4-53c5-4879-a1c2-c4a479ec347e"
   },
   "source": [
    "## 파이썬 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2b5b3cb-ef9b-4f36-a077-0b1bc35ca742",
   "metadata": {
    "id": "d2b5b3cb-ef9b-4f36-a077-0b1bc35ca742",
    "outputId": "e2df575f-c79c-489d-c6b6-c2942b53cf27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 에러 값 =  3.0364465419410878\n",
      "step =  0 에러 값 =  3.0111874305131083\n",
      "step =  400 에러 값 =  1.4779923982290848\n",
      "step =  800 에러 값 =  1.1111397893581216\n",
      "step =  1200 에러 값 =  0.8987485649433992\n",
      "step =  1600 에러 값 =  0.7569263139832219\n",
      "step =  2000 에러 값 =  0.6542587512812378\n",
      "step =  2400 에러 값 =  0.5760396746860992\n",
      "step =  2800 에러 값 =  0.5142996831644823\n",
      "step =  3200 에러 값 =  0.4642717243941455\n",
      "step =  3600 에러 값 =  0.4228981412902694\n",
      "step =  4000 에러 값 =  0.38811392685071755\n",
      "step =  4400 에러 값 =  0.3584681177420513\n",
      "step =  4800 에러 값 =  0.33290863033400797\n",
      "step =  5200 에러 값 =  0.3106527706289026\n",
      "step =  5600 에러 값 =  0.2911056432907345\n",
      "step =  6000 에러 값 =  0.27380677475877724\n",
      "step =  6400 에러 값 =  0.2583940846483088\n",
      "step =  6800 에러 값 =  0.24457891427851552\n",
      "step =  7200 에러 값 =  0.2321283205423006\n",
      "step =  7600 에러 값 =  0.22085227024683007\n",
      "step =  8000 에러 값 =  0.21059421591608218\n",
      "AND_GATE \n",
      "\n",
      "[0 0] = 0 \n",
      "\n",
      "[0 1] = 0 \n",
      "\n",
      "[1 0] = 0 \n",
      "\n",
      "[1 1] = 1 \n",
      "\n",
      "초기 에러 값 =  2.581544367356249\n",
      "step =  0 에러 값 =  2.5744128904084977\n",
      "step =  400 에러 값 =  1.5237899658565408\n",
      "step =  800 에러 값 =  1.135474579388936\n",
      "step =  1200 에러 값 =  0.9141931115949092\n",
      "step =  1600 에러 값 =  0.7677490307425721\n",
      "step =  2000 에러 값 =  0.6623218166216424\n",
      "step =  2400 에러 값 =  0.5823009033183143\n",
      "step =  2800 에러 값 =  0.5193100533346441\n",
      "step =  3200 에러 값 =  0.46837428973123063\n",
      "step =  3600 에러 값 =  0.4263192939738625\n",
      "step =  4000 에러 값 =  0.391009842900521\n",
      "step =  4400 에러 값 =  0.36095035782449436\n",
      "step =  4800 에러 값 =  0.33505916413964193\n",
      "step =  5200 에러 값 =  0.31253324803877036\n",
      "step =  5600 에러 값 =  0.29276335423362776\n",
      "step =  6000 에러 값 =  0.27527861260170244\n",
      "step =  6400 에러 값 =  0.2597092583127333\n",
      "step =  6800 에러 값 =  0.24576085001255638\n",
      "step =  7200 에러 값 =  0.23319602397713451\n",
      "step =  7600 에러 값 =  0.22182132014674588\n",
      "step =  8000 에러 값 =  0.21147750119288322\n",
      "NAND_GATE \n",
      "\n",
      "[0 0] = 1 \n",
      "\n",
      "[0 1] = 1 \n",
      "\n",
      "[1 0] = 1 \n",
      "\n",
      "[1 1] = 0 \n",
      "\n",
      "초기 에러 값 =  1.8535689805683393\n",
      "step =  0 에러 값 =  1.8502950329185386\n",
      "step =  400 에러 값 =  1.1534789929280258\n",
      "step =  800 에러 값 =  0.8270349761467888\n",
      "step =  1200 에러 값 =  0.638459500641439\n",
      "step =  1600 에러 값 =  0.5168042037682798\n",
      "step =  2000 에러 값 =  0.4323196056622474\n",
      "step =  2400 에러 값 =  0.3705162856929431\n",
      "step =  2800 에러 값 =  0.3235121365642707\n",
      "step =  3200 에러 값 =  0.286662954960677\n",
      "step =  3600 에러 값 =  0.2570625769858398\n",
      "step =  4000 에러 값 =  0.23280438502026587\n",
      "step =  4400 에러 값 =  0.2125886345912608\n",
      "step =  4800 에러 값 =  0.19550062536081755\n",
      "step =  5200 에러 값 =  0.1808790675157309\n",
      "step =  5600 에러 값 =  0.1682347059188159\n",
      "step =  6000 에러 값 =  0.15719823525623103\n",
      "step =  6400 에러 값 =  0.14748595018179414\n",
      "step =  6800 에러 값 =  0.13887649263661211\n",
      "step =  7200 에러 값 =  0.1311947452086079\n",
      "step =  7600 에러 값 =  0.12430044424378177\n",
      "step =  8000 에러 값 =  0.11807998125857583\n",
      "OR_GATE \n",
      "\n",
      "[0 0] = 0 \n",
      "\n",
      "[0 1] = 1 \n",
      "\n",
      "[1 0] = 1 \n",
      "\n",
      "[1 1] = 1 \n",
      "\n",
      "[0 0] = 0\n",
      "\n",
      "[0 1] = 1\n",
      "\n",
      "[1 0] = 1\n",
      "\n",
      "[1 1] = 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 패키지 로드\n",
    "import numpy as np\n",
    "\n",
    "# 2. MLP 구조 구성하는 게이트 클래스 구현\n",
    "def sigmoid(x):\n",
    "  return 1/(1+np.exp(-x))\n",
    "def numerical_derivative(f, x):\n",
    "  delta_x = 1e-4\n",
    "  grad = np.zeros_like(x)\n",
    "  it=np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "    idx=it.multi_index\n",
    "    tmp_val=x[idx]\n",
    "    x[idx]=float(tmp_val) + delta_x\n",
    "    fx1 = f(x)\n",
    "    x[idx]=float(tmp_val) - delta_x\n",
    "    fx2 = f(x)\n",
    "    grad[idx]=(fx1 - fx2) / (2*delta_x)\n",
    "    x[idx]=tmp_val\n",
    "    it.iternext()\n",
    "  return grad\n",
    "\n",
    "\n",
    "class LogicGate:\n",
    "  def __init__(self, gate_name, xdata, tdata):\n",
    "    self.name=gate_name\n",
    "    self.__xdata=xdata.reshape(4,2)\n",
    "    self.__tdata=tdata.reshape(4,1)\n",
    "    self.__w=np.random.rand(2,1)\n",
    "    self.__b=np.random.rand(1)\n",
    "    self.__learning_rate=1e-2\n",
    "  # 손실 함수\n",
    "  def __loss_function(self):\n",
    "    delta=1e-7\n",
    "    z=np.dot(self.__xdata, self.__w) + self.__b\n",
    "    y=sigmoid(z)\n",
    "    return -np.sum(self.__tdata*np.log(y+delta) + (1-self.__tdata)*np.log((1-y)+delta))\n",
    "  # 손실 값 계산 함수\n",
    "  def error_val(self):\n",
    "    return self.__loss_function()\n",
    "\n",
    "\n",
    "#학습\n",
    "  def train(self):\n",
    "    #손실함수 계산\n",
    "    f=lambda x: self.__loss_function()\n",
    "    print(\"초기 에러 값 = \", self.error_val())\n",
    "    #손실함수에 대해 경사하강 가중치, 바이어스 업데이트\n",
    "    for step in range(8001):\n",
    "      self.__w -= self.__learning_rate * numerical_derivative(f, self.__w)\n",
    "      self.__b -= self.__learning_rate * numerical_derivative(f, self.__b)\n",
    "      if(step %400==0):\n",
    "        print(\"step = \",step, \"에러 값 = \", self.error_val())\n",
    "  #예측 함수\n",
    "  def predict(self, input_data):\n",
    "    z=np.dot(input_data, self.__w) + self.__b\n",
    "    y=sigmoid(z)\n",
    "    if y > 0.5:\n",
    "      result = 1\n",
    "    else:\n",
    "      result = 0\n",
    "    return y, result\n",
    "\n",
    "\n",
    "# AND\n",
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([0,0,0,1])\n",
    "AND_g=LogicGate(\"AND_GATE\", xdata, tdata)\n",
    "AND_g.train()\n",
    "print(AND_g.name, '\\n')\n",
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "for input_data in test_data:\n",
    "  (sigmoid_val, logical_val)=AND_g.predict(input_data)\n",
    "  print(input_data, \"=\", logical_val, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# NAND\n",
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([1,1,1,0])\n",
    "NAND_g=LogicGate(\"NAND_GATE\", xdata, tdata)\n",
    "NAND_g.train()\n",
    "print(NAND_g.name, '\\n')\n",
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "for input_data in test_data:\n",
    "  (sigmoid_val, logical_val)=NAND_g.predict(input_data)\n",
    "  print(input_data, \"=\", logical_val, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# OR\n",
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([0,1,1,1])\n",
    "OR_g=LogicGate(\"OR_GATE\", xdata, tdata)\n",
    "OR_g.train()\n",
    "print(OR_g.name, '\\n')\n",
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "for input_data in test_data:\n",
    "  (sigmoid_val, logical_val)=OR_g.predict(input_data)\n",
    "  print(input_data, \"=\", logical_val, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# XOR 계산하기\n",
    "input_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "s1=[] #NAND\n",
    "s2=[] #OR\n",
    "new_input_data=[] #AND 입력\n",
    "final_output=[] #AND 출력\n",
    "for index in range(len(input_data)):\n",
    "  s1=NAND_g.predict(input_data[index])\n",
    "  s2=OR_g.predict(input_data[index])\n",
    "  new_input_data.append(s1[-1])\n",
    "  new_input_data.append(s2[-1])\n",
    "  (sigmoid_val, logical_val)=AND_g.predict(np.array(new_input_data))\n",
    "  final_output.append(logical_val)\n",
    "  new_input_data=[] #AND 입력 초기화\n",
    "for index in range(len(input_data)):\n",
    "  print(input_data[index], \"=\", final_output[index], end='')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903da86-cf14-41a4-8b2b-a343c9ef57a4",
   "metadata": {
    "id": "c903da86-cf14-41a4-8b2b-a343c9ef57a4"
   },
   "source": [
    "## 딥러닝 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddc56b95-f244-4b83-8612-f907896bf31f",
   "metadata": {
    "id": "ddc56b95-f244-4b83-8612-f907896bf31f",
    "outputId": "815335d1-3ab0-4f45-ab3b-b2262db0489e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial W2 =  [[0.30159257 0.73815212 0.93944769 0.70191101 0.55673302 0.72997733]\n",
      " [0.66632919 0.21386616 0.28700741 0.21237116 0.40779922 0.37792162]] b2 =  [0.27241897 0.58110865 0.28675977 0.4732978  0.97929777 0.60731609] W3 =  [[0.5981236 ]\n",
      " [0.40559573]\n",
      " [0.0980292 ]\n",
      " [0.59216488]\n",
      " [0.14699416]\n",
      " [0.46581696]] b3 =  [0.52085068] error_val =  4.777787895878026\n",
      "step =  0 W2 =  [[0.29337677 0.73337271 0.93828409 0.69425955 0.55546301 0.72519699]\n",
      " [0.6580575  0.20935038 0.28591575 0.20507855 0.40657009 0.3733718 ]] b2 =  [0.25255225 0.56905056 0.28373112 0.45470913 0.97571956 0.59416835] W3 =  [[ 0.49214866]\n",
      " [ 0.28959877]\n",
      " [-0.01258957]\n",
      " [ 0.4802196 ]\n",
      " [ 0.01986544]\n",
      " [ 0.34744515]] b3 =  [0.38446898] error_val =  3.8391622078324037\n",
      "step =  400 W2 =  [[ 0.64158354  0.73014716  0.90593296  0.82613148  0.41770606  0.89164499]\n",
      " [ 0.84665231  0.2111094  -0.0701875   0.48820758  0.18976303  0.6352828 ]] b2 =  [0.30911375 0.55622097 0.28415328 0.46089419 0.99632731 0.59271744] W3 =  [[ 0.40075003]\n",
      " [-0.01877246]\n",
      " [-0.39957084]\n",
      " [ 0.31908831]\n",
      " [-0.3344757 ]\n",
      " [ 0.3897698 ]] b3 =  [-0.28451392] error_val =  2.7501533096158166\n",
      "step =  800 W2 =  [[ 1.91736329  0.72157073  1.56397215  1.3661456   0.4029227   1.93800695]\n",
      " [ 1.79878465  0.14710744 -0.56595926  1.08610824  0.16252412  1.70345009]] b2 =  [0.28502848 0.53796917 0.63785269 0.5467242  1.00244292 0.37278928] W3 =  [[ 1.42532741]\n",
      " [-0.49214479]\n",
      " [-1.32097867]\n",
      " [ 0.63051421]\n",
      " [-0.84492309]\n",
      " [ 1.43260362]] b3 =  [-0.92879242] error_val =  2.4541079306350286\n",
      "step =  1200 W2 =  [[ 3.7963584   0.27233771  4.08205863  2.02450629 -0.73473522  3.71712178]\n",
      " [ 3.20876584  1.30117461 -2.27142454  1.51391325  2.36047835  3.10970564]] b2 =  [-0.3283978  -0.32806386  1.0796898   0.79313835  0.00923114 -0.25628941] W3 =  [[ 3.40196488]\n",
      " [-1.77481776]\n",
      " [-3.73219145]\n",
      " [ 0.87780964]\n",
      " [-2.76273351]\n",
      " [ 3.30118263]] b3 =  [-0.92591199] error_val =  1.0588163420804388\n",
      "step =  1600 W2 =  [[ 4.35893534  0.43101499  5.53377753  2.22873683 -1.7452672   4.2663198 ]\n",
      " [ 3.94823708  2.2852311  -3.48880464  1.78806924  4.38665647  3.84304683]] b2 =  [-0.66337815 -1.40982561  1.31249277  0.93680029  0.08729169 -0.5870994 ] W3 =  [[ 4.77173099]\n",
      " [-2.95336414]\n",
      " [-6.14669395]\n",
      " [ 1.49265385]\n",
      " [-4.85172112]\n",
      " [ 4.63363936]] b3 =  [-0.26260677] error_val =  0.261718848460304\n",
      "step =  2000 W2 =  [[ 4.56700782  0.81552072  6.06040474  2.31289033 -2.21799937  4.47140821]\n",
      " [ 4.22695556  2.53767393 -3.90832089  1.91339401  5.03910977  4.12089578]] b2 =  [-0.76914584 -2.06504831  1.45822691  0.99811884  0.24925085 -0.6937264 ] W3 =  [[ 5.34695714]\n",
      " [-3.58397299]\n",
      " [-7.23584833]\n",
      " [ 1.80309087]\n",
      " [-5.76866164]\n",
      " [ 5.19744336]] b3 =  [0.06014395] error_val =  0.12328689760457914\n",
      "step =  2400 W2 =  [[ 4.68505531  1.13767008  6.33128208  2.36365494 -2.49484691  4.58826183]\n",
      " [ 4.37058931  2.659207   -4.12114254  1.9818442   5.37807282  4.26444113]] b2 =  [-0.8252226  -2.52348598  1.54627848  1.03116737  0.35068954 -0.75055432] W3 =  [[ 5.67367889]\n",
      " [-4.04095062]\n",
      " [-7.82386922]\n",
      " [ 1.99036533]\n",
      " [-6.33652999]\n",
      " [ 5.51850174]] b3 =  [0.25176544] error_val =  0.07511991630798563\n",
      "step =  2800 W2 =  [[ 4.76470643  1.38980109  6.49995181  2.39920634 -2.68227931  4.66729783]\n",
      " [ 4.46070815  2.75059789 -4.25438804  2.02594148  5.59435249  4.35463213]] b2 =  [-0.86291064 -2.8555806   1.60639866  1.05171192  0.4226338  -0.78883164] W3 =  [[ 5.89532385]\n",
      " [-4.39621664]\n",
      " [-8.19530208]\n",
      " [ 2.12205898]\n",
      " [-6.73787346]\n",
      " [ 5.73662479]] b3 =  [0.38488414] error_val =  0.052154336602853055\n",
      "step =  3200 W2 =  [[ 4.82355518  1.5884061   6.61757055  2.42612015 -2.82027598  4.72578539]\n",
      " [ 4.52437567  2.82868904 -4.34873154  2.05760295  5.74815789  4.41841204]] b2 =  [-0.8911683  -3.10815502  1.65049372  1.06577709  0.47855306 -0.81756599] W3 =  [[ 6.06097497]\n",
      " [-4.68299036]\n",
      " [-8.45666553]\n",
      " [ 2.22287689]\n",
      " [-7.04203942]\n",
      " [ 5.89980301]] b3 =  [0.48593725] error_val =  0.039195110164376615\n",
      "step =  3600 W2 =  [[ 4.86959836  1.7487414   6.70586316  2.44753957 -2.92775397  4.77159908]\n",
      " [ 4.57287469  2.89725983 -4.42077737  2.08200145  5.86517789  4.46703181]] b2 =  [-0.91367157 -3.30896468  1.68454667  1.07607588  0.52434855 -0.84046746] W3 =  [[ 6.19222345]\n",
      " [-4.92159994]\n",
      " [-8.65462803]\n",
      " [ 2.3040937 ]\n",
      " [-7.28369803]\n",
      " [ 6.02918152]] b3 =  [0.56686678] error_val =  0.031045048262402446\n",
      "step =  4000 W2 =  [[ 4.90708484  1.88151006  6.77558445  2.46520054 -3.01489664  4.80893218]\n",
      " [ 4.61172492  2.95810311 -4.47858554  2.10172158  5.95841015  4.50600113]] b2 =  [-0.93229188 -3.47426303  1.71189077  1.08399007  0.56307733 -0.85942891] W3 =  [[ 6.30034676]\n",
      " [-5.12504992]\n",
      " [-8.8123917 ]\n",
      " [ 2.37180599]\n",
      " [-7.48244238]\n",
      " [ 6.13581923]] b3 =  [0.634052] error_val =  0.025517314552461756\n",
      "step =  4400 W2 =  [[ 4.93850908  1.99390625  6.83268245  2.48015187 -3.08769151  4.84025048]\n",
      " [ 4.64396694  3.01253373 -4.52659677  2.11820678  6.03520929  4.53835728]] b2 =  [-0.94811946 -3.61399749  1.73451773  1.09029363  0.59656251 -0.87555449] W3 =  [[ 6.39194487]\n",
      " [-5.30193984]\n",
      " [-8.94276929]\n",
      " [ 2.42968834]\n",
      " [-7.65023139]\n",
      " [ 6.22619504]] b3 =  [0.69129755] error_val =  0.021554958954916256\n",
      "step =  4800 W2 =  [[ 4.96544451  2.09082286  6.8807326   2.49306957 -3.14990538  4.86711124]\n",
      " [ 4.67143057  3.0616044  -4.5674933   2.13233458  6.10008424  4.56592914]] b2 =  [-0.96184729 -3.73458286  1.75368791  1.09545362  0.62599692 -0.88954661] W3 =  [[ 6.47119561]\n",
      " [-5.45816432]\n",
      " [-9.05345096]\n",
      " [ 2.4801218 ]\n",
      " [-7.79480603]\n",
      " [ 6.30441354]] b3 =  [0.74104819] error_val =  0.01859270061759624\n",
      "step =  5200 W2 =  [[ 4.98893957  2.17567395  6.92202665  2.50441172 -3.20403868  4.89055317]\n",
      " [ 4.69529348  3.10616359 -4.60300954  2.1446744   6.15597199  4.58989435]] b2 =  [-0.97394318 -3.84035541  1.77023803  1.09976904  0.65220981 -0.9018798 ] W3 =  [[ 6.54090024]\n",
      " [-5.59790416]\n",
      " [-9.14936415]\n",
      " [ 2.52473237]\n",
      " [-7.92141951]\n",
      " [ 6.37322874]] b3 =  [0.78496259] error_val =  0.016303930867872414\n",
      "step =  5600 W2 =  [[ 5.00972393  2.25090834  6.95810986  2.51450144 -3.25182487  4.91129975]\n",
      " [ 4.71635416  3.14689507 -4.63432709  2.15561494  6.20487867  4.61105172]] b2 =  [-0.98473735 -3.93436566  1.78474634  1.1034407   0.67580259 -0.91288918] W3 =  [[ 6.60302221]\n",
      " [-5.7242161 ]\n",
      " [-9.23383389]\n",
      " [ 2.5646764 ]\n",
      " [-8.03377766]\n",
      " [ 6.434572  ]] b3 =  [0.82421427] error_val =  0.014488117485592138\n",
      "step =  6000 W2 =  [[ 5.02832368  2.31832872  6.99006747  2.52357425 -3.29450922  4.92987294]\n",
      " [ 4.73517702  3.18435149 -4.66228467  2.16543254  6.24822798  4.62996607]] b2 =  [-0.99447109 -4.01883232  1.79762625  1.10660883  0.69722549 -0.92281974] W3 =  [[ 6.65898732]\n",
      " [-5.83939495]\n",
      " [-9.30919937]\n",
      " [ 2.60080354]\n",
      " [-8.13458031]\n",
      " [ 6.48984643]] b3 =  [0.85966187] error_val =  0.013015958157415061\n",
      "step =  6400 W2 =  [[ 5.04512936  2.37929349  7.01868755  2.53180661 -3.33301325  4.94666046]\n",
      " [ 4.75217461  3.21898232 -4.68749766  2.17433026  6.2870625   4.6470504 ]] b2 =  [-1.00332575 -4.09541494  1.80918188  1.10937454  0.71682417 -0.93185566] W3 =  [[ 6.70986138]\n",
      " [-5.94520332]\n",
      " [-9.37716321]\n",
      " [ 2.63375504]\n",
      " [-8.22584834]\n",
      " [ 6.54010127]] b3 =  [0.89195129] error_val =  0.011800655694018655\n",
      "step =  6800 W2 =  [[ 5.06043809  2.43484769  7.04455852  2.53933381 -3.36803607  4.96195744]\n",
      " [ 4.76765708  3.25115586 -4.71043007  2.1824614   6.32216605  4.66261526]] b2 =  [-1.01144081 -4.16538453  1.81964253  1.1118128   0.73486946 -0.94013869] W3 =  [[ 6.75646059]\n",
      " [-6.04302214]\n",
      " [-9.43900026]\n",
      " [ 2.66402602]\n",
      " [-8.30912966]\n",
      " [ 6.58614032]] b3 =  [0.92157979] error_val =  0.010781952829975246\n",
      "step =  7200 W2 =  [[ 5.07448071  2.48581013  7.06813019  2.5462617  -3.40011952  4.97599327]\n",
      " [ 4.78186305  3.28117641 -4.73143988  2.18994443  6.35414117  4.67689967]] b2 =  [-1.01892573 -4.22973444  1.82918479  1.11398048  0.75157738 -0.94778005] W3 =  [[ 6.79942302]\n",
      " [-6.13395213]\n",
      " [-9.49568807]\n",
      " [ 2.69200617]\n",
      " [-8.38563358]\n",
      " [ 6.6285921 ]] b3 =  [0.94893783] error_val =  0.009916807759099793\n",
      "step =  7600 W2 =  [[ 5.08743983  2.53283275  7.08975337  2.55267448 -3.42969128  4.98894948]\n",
      " [ 4.79497985  3.30929756 -4.75080867  2.19687267  6.38345999  4.6900913 ]] b2 =  [-1.02586782 -4.28925471  1.83794714  1.11592159  0.76712279 -0.95486855] W3 =  [[ 6.83925629]\n",
      " [-6.21888398]\n",
      " [-9.54799156]\n",
      " [ 2.71800741]\n",
      " [-8.45632123]\n",
      " [ 6.66795677]] b3 =  [0.97433725] error_val =  0.009173714763687392\n",
      "step =  8000 W2 =  [[ 5.09946216  2.57644218  7.10970648  2.55864017 -3.45709434  5.00097199]\n",
      " [ 4.80715706  3.33573233 -4.76876171  2.20332089  6.41049865  4.70234004]] b2 =  [-1.03233781 -4.34458325  1.84603998  1.11767077  0.78164908 -0.96147608] W3 =  [[ 6.87637051]\n",
      " [-6.29854807]\n",
      " [-9.59651985]\n",
      " [ 2.74228302]\n",
      " [-8.52196802]\n",
      " [ 6.7046385 ]] b3 =  [0.99803088] error_val =  0.008529118307917766\n",
      "(0, array([0.00055812]))\n",
      "(1, array([0.99742478]))\n",
      "(1, array([0.99802989]))\n",
      "(0, array([0.00341481]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 시그모이드 구하는 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 수치 미분 함수\n",
    "def numerical_derivative(f, input_data):\n",
    "    delta_x = 1e-4\n",
    "    ret = np.zeros_like(input_data)\n",
    "    it = np.nditer(input_data, flags=['multi_index'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp = input_data[idx]\n",
    "        input_data[idx] = float(tmp) + delta_x\n",
    "        fx1 = f(input_data)\n",
    "        input_data[idx] = float(tmp) - delta_x\n",
    "        fx2 = f(input_data)\n",
    "        ret[idx] = (fx1 - fx2) / (2 * delta_x)\n",
    "        input_data[idx] = tmp\n",
    "        it.iternext()\n",
    "    return ret\n",
    "\n",
    "class LogicGate:\n",
    "    def __init__(self, gate_name, x_data, t_data):\n",
    "        self.name = gate_name\n",
    "        # 입력 데이터\n",
    "        self.__x_data = x_data\n",
    "        self.__t_data = t_data\n",
    "        # 임의의 W2, b2, W3, b3 준비\n",
    "        self.__W2 = np.random.rand(2, 6)\n",
    "        self.__b2 = np.random.rand(6)\n",
    "        self.__W3 = np.random.rand(6, 1)\n",
    "        self.__b3 = np.random.rand(1)\n",
    "        self.__learning_rate = 1e-1\n",
    "        self.loss_func = self.__feed_forward\n",
    "    \n",
    "    # feed_forward 함수로, 에러를 찾아줌\n",
    "    def __feed_forward(self):\n",
    "        delta = 1e-7\n",
    "        # 입력층 -> 은닉층\n",
    "        z2 = np.dot(self.__x_data, self.__W2) + self.__b2\n",
    "        a2 = sigmoid(z2)\n",
    "        # 은닉층 -> 출력층\n",
    "        z3 = np.dot(a2, self.__W3) + self.__b3\n",
    "        y = sigmoid(z3)\n",
    "        # 크로스 엔트로피로 에러를 측정\n",
    "        return -np.sum(self.__t_data * np.log(y + delta) + (1 - self.__t_data) * np.log(1 - y + delta))\n",
    "    \n",
    "    def train(self):\n",
    "        f = lambda x: self.__feed_forward()\n",
    "        print(\"Initial W2 = \", self.__W2, \"b2 = \", self.__b2, \"W3 = \", self.__W3, \"b3 = \", self.__b3, \"error_val = \", self.loss_func())\n",
    "        for step in range(8001):\n",
    "            self.__W2 -= self.__learning_rate * numerical_derivative(f, self.__W2)\n",
    "            self.__b2 -= self.__learning_rate * numerical_derivative(f, self.__b2)\n",
    "            self.__W3 -= self.__learning_rate * numerical_derivative(f, self.__W3)\n",
    "            self.__b3 -= self.__learning_rate * numerical_derivative(f, self.__b3)\n",
    "            if step % 400 == 0:\n",
    "                print(\"step = \", step, \"W2 = \", self.__W2, \"b2 = \", self.__b2, \"W3 = \", self.__W3, \"b3 = \", self.__b3, \"error_val = \", self.loss_func())\n",
    "                \n",
    "    def predict(self, x_data):\n",
    "        z2 = np.dot(x_data, self.__W2) + self.__b2\n",
    "        a2 = sigmoid(z2)\n",
    "        z3 = np.dot(a2, self.__W3) + self.__b3\n",
    "        pro = sigmoid(z3)\n",
    "        if pro < 0.5:\n",
    "            return 0, pro\n",
    "        return 1, pro\n",
    "\n",
    "# XOR 데이터\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).reshape([4, 2])\n",
    "y_data = np.array([0, 1, 1, 0]).reshape([4, 1])\n",
    "\n",
    "# 학습\n",
    "XOR_Gate = LogicGate(\"XOR_Gate\", x_data, y_data)\n",
    "XOR_Gate.train()\n",
    "\n",
    "# 출력\n",
    "print(XOR_Gate.predict([0, 0]))\n",
    "print(XOR_Gate.predict([1, 0]))\n",
    "print(XOR_Gate.predict([0, 1]))\n",
    "print(XOR_Gate.predict([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50b10731-a6bb-4313-8544-1b42d34c3f0d",
   "metadata": {
    "id": "50b10731-a6bb-4313-8544-1b42d34c3f0d",
    "outputId": "5b5f2182-7626-45c2-e4a5-1996561de5c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W= [[0.25914185]] ,W.shape (1, 1) ,b= [0.53263988] ,b.shape (1,)\n",
      "초기 에러 값 = 8.33348949673567 초기 W 값 = [[0.25914185]] 초기 b 값 = [0.53263988]\n",
      "step =  0 에러 값 = 4.90819828216339 W 값 = [[0.45017225]] b 값 = [0.57497675]\n",
      "step =  500 에러 값 = 0.00048721858655564193 W 값 = [[1.01433371]] b 값 = [0.94826342]\n",
      "step =  1000 에러 값 = 1.5624279451355754e-05 W 값 = [[1.00256683]] b 값 = [0.99073521]\n",
      "step =  1500 에러 값 = 5.01044326119848e-07 W 값 = [[1.00045966]] b 값 = [0.9983409]\n",
      "step =  2000 에러 값 = 1.6067647632592755e-08 W 값 = [[1.00008231]] b 값 = [0.99970289]\n",
      "step =  2500 에러 값 = 5.152623969330119e-10 W 값 = [[1.00001474]] b 값 = [0.9999468]\n",
      "step =  3000 에러 값 = 1.6523597217978548e-11 W 값 = [[1.00000264]] b 값 = [0.99999047]\n",
      "step =  3500 에러 값 = 5.298839324585692e-13 W 값 = [[1.00000047]] b 값 = [0.99999829]\n",
      "step =  4000 에러 값 = 1.6992485290923388e-14 W 값 = [[1.00000008]] b 값 = [0.99999969]\n",
      "step =  4500 에러 값 = 5.449203919061008e-16 W 값 = [[1.00000002]] b 값 = [0.99999995]\n",
      "step =  5000 에러 값 = 1.7474678352868696e-17 W 값 = [[1.]] b 값 = [0.99999999]\n",
      "step =  5500 에러 값 = 5.603834001291476e-19 W 값 = [[1.]] b 값 = [1.]\n",
      "step =  6000 에러 값 = 1.7970606970326508e-20 W 값 = [[1.]] b 값 = [1.]\n",
      "step =  6500 에러 값 = 5.762902983822779e-22 W 값 = [[1.]] b 값 = [1.]\n",
      "step =  7000 에러 값 = 1.8482558992029615e-23 W 값 = [[1.]] b 값 = [1.]\n",
      "step =  7500 에러 값 = 5.941806834346866e-25 W 값 = [[1.]] b 값 = [1.]\n",
      "step =  8000 에러 값 = 1.9183085619666694e-26 W 값 = [[1.]] b 값 = [1.]\n",
      "step =  8500 에러 값 = 6.387012319121922e-28 W 값 = [[1.]] b 값 = [1.]\n",
      "step =  9000 에러 값 = 9.517606821491508e-29 W 값 = [[1.]] b 값 = [1.]\n",
      "step =  9500 에러 값 = 9.517606821491508e-29 W 값 = [[1.]] b 값 = [1.]\n",
      "step =  10000 에러 값 = 9.517606821491508e-29 W 값 = [[1.]] b 값 = [1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[48.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패키지 임포트\n",
    "import numpy as np\n",
    "\n",
    "# 학습 데이터 셋 생성\n",
    "X=np.array(\n",
    "    [1,2,3,4,5]\n",
    ").reshape(5,1)\n",
    "X\n",
    "\n",
    "\n",
    "y=np.array(\n",
    "    [2,3,4,5,6]\n",
    ").reshape(5,1)\n",
    "y\n",
    "\n",
    "# W,b 초기화\n",
    "W=np.random.rand(1,1)\n",
    "b=np.random.rand(1)\n",
    "print(\n",
    "    \"W=\",W,\n",
    "    \",W.shape\",W.shape,\n",
    "    \",b=\",b,\n",
    "    \",b.shape\",b.shape\n",
    ")\n",
    "\n",
    "# 손실함수 정의\n",
    "def loss_func(x, t):\n",
    "  # H(x)를 정의 - 행렬 계산\n",
    "  y=np.dot(x,W)+b\n",
    "  return np.sum((t - y)**2) /len(x)\n",
    "\n",
    "# 수치미분 함수 정의\n",
    "def numerical_derivative(fx, input_list):\n",
    "  delta_x=1e-4 # 1* 0.00001\n",
    "  ret=np.zeros_like(input_list)\n",
    "  it=np.nditer(input_list, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "    i=it.multi_index\n",
    "    tmp=input_list[i]\n",
    "    input_list[i]=float(tmp) - delta_x\n",
    "    f1=fx(input_list)\n",
    "    input_list[i]=float(tmp) + delta_x\n",
    "    f2=fx(input_list)\n",
    "    ret[i]=(f2-f1) / (delta_x*2)\n",
    "    input_list[i]=tmp\n",
    "    it.iternext()\n",
    "  return ret\n",
    "\n",
    "\n",
    "# 손실함수 계산- 에러 값 계산\n",
    "def error_val(x, t):\n",
    "  # H(x)를 정의 - 행렬 계산\n",
    "  y=np.dot(x,W)+b\n",
    "  return np.sum((t - y)**2) /len(x)\n",
    "\n",
    "\n",
    "# W,b 업데이트 수행 - 학습\n",
    "#학습율\n",
    "learning_rate=1e-2\n",
    "f = lambda x: loss_func(X, y)\n",
    "print(\n",
    "    \"초기 에러 값 =\", error_val(X,y),\n",
    "    \"초기 W 값 =\",W,\n",
    "    \"초기 b 값 =\",b\n",
    ")\n",
    "for step in range(10001):\n",
    "  # W,b 를 경사하강으로 업데이트\n",
    "  W -= numerical_derivative(f,W) * learning_rate\n",
    "  b -= numerical_derivative(f,b) * learning_rate\n",
    "  if step % 500 == 0:\n",
    "    print(\n",
    "        \"step = \", step,\n",
    "        \"에러 값 =\", error_val(X,y),\n",
    "        \"W 값 =\",W,\n",
    "        \"b 값 =\",b\n",
    "    )\n",
    "\n",
    "\n",
    "# 학습된 결과로 현장 적용 후 예측 함수 정의\n",
    "def predict(x):\n",
    "  y=np.dot(x,W)+b\n",
    "  return y\n",
    "\n",
    "predict(47)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c71767c-441b-4278-ab7d-b0e33725f419",
   "metadata": {
    "id": "0c71767c-441b-4278-ab7d-b0e33725f419"
   },
   "source": [
    "# deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8a4a7-6e07-4f9c-b362-67e42dfd14bc",
   "metadata": {
    "id": "bbd8a4a7-6e07-4f9c-b362-67e42dfd14bc"
   },
   "source": [
    "## 텐서플로  익히기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed24d6fc-9203-4a37-bdb3-15e9480b65f4",
   "metadata": {
    "id": "ed24d6fc-9203-4a37-bdb3-15e9480b65f4",
    "outputId": "b0c15d1e-12dc-41c0-df65-89efdb2ed9b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.compat.v1.Session() as sess:\n",
    "   helloworld = tf.constant(\"Hello World!\")\n",
    "   print(sess.run(helloworld))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s0vV3GZGH04Y",
   "metadata": {
    "id": "s0vV3GZGH04Y"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de0UjWExH1B4",
   "metadata": {
    "id": "de0UjWExH1B4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "j-ADpoOtRgO2",
   "metadata": {
    "id": "j-ADpoOtRgO2"
   },
   "source": [
    "# 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "Ux2a8GmC7sua",
   "metadata": {
    "id": "Ux2a8GmC7sua"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "T75CiRkI7sua",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "T75CiRkI7sua",
    "outputId": "1fec735a-a8d9-4a2d-dc5a-7ac1a6224cb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ncozDaUeP4iN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncozDaUeP4iN",
    "outputId": "bd48c615-27dc-46fc-bc5d-661fe40b13e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'PrintV2_1' type=PrintV2>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텐서 생성\n",
    "x=tf.constant(3)\n",
    "y=x**2\n",
    "\n",
    "# 세션 객체 생성\n",
    "# sess=tf.Session()\n",
    "\n",
    "# 세션 객체를 이용해서 텐서를 사용\n",
    "# print(sess.run(x))\n",
    "# print(sess.run(y))\n",
    "\n",
    "# sess.close()\n",
    "# sess.close()\n",
    "\n",
    "tf.print(x)\n",
    "tf.print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sFXocppVENs2",
   "metadata": {
    "id": "sFXocppVENs2"
   },
   "source": [
    "### 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "wi2ak581Sa54",
   "metadata": {
    "id": "wi2ak581Sa54",
    "outputId": "8b819421-5914-4a19-eb77-6ae25d83e48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n",
      "Tensor(\"random_uniform/RandomUniform:0\", shape=(2, 3), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "a=tf.random.uniform([2,3],0,1)\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "E--VBxtqScvA",
   "metadata": {
    "id": "E--VBxtqScvA",
    "outputId": "579521a1-18ee-42fa-f7f1-3def099a966a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow로 생성한 텐서:\n",
      " Tensor(\"random_uniform_1/RandomUniform:0\", shape=(2, 3), dtype=float32) \n",
      "\n",
      "numpy로 생성한 ndarray:\n",
      " [[0.2918795  0.39234135 0.28915484]\n",
      " [0.67476766 0.1486133  0.06581115]] \n",
      "\n",
      "덧셈 결과:\n",
      " Tensor(\"add:0\", shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "t=tf.random.uniform([2,3],0,1)\n",
    "n=np.random.uniform(0,1,[2,3])\n",
    "print(\"tensorflow로 생성한 텐서:\\n\",t,\"\\n\")\n",
    "print(\"numpy로 생성한 ndarray:\\n\",n,\"\\n\")\n",
    "\n",
    "res=t+n # 텐서 t와 ndarray n의 덧셈\n",
    "print(\"덧셈 결과:\\n\",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Beil24jxHkzB",
   "metadata": {
    "id": "Beil24jxHkzB"
   },
   "source": [
    "## 텐서 기본 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ODCtR_mHkzB",
   "metadata": {
    "id": "8ODCtR_mHkzB",
    "outputId": "dc7986f2-5f82-499b-9fca-f7217dbcdf17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 14.191356 [0.03673817] [1.4522395]\n",
      "20 0.053189345 [0.8475491] [1.6362792]\n",
      "40 0.04619885 [0.8609106] [1.602093]\n",
      "60 0.04034581 [0.87003493] [1.5692147]\n",
      "80 0.035234194 [0.8785466] [1.5384852]\n",
      "100 0.030770317 [0.88650066] [1.5097686]\n",
      "120 0.02687196 [0.8939338] [1.4829326]\n",
      "140 0.023467492 [0.90088016] [1.4578539]\n",
      "160 0.020494347 [0.90737164] [1.4344177]\n",
      "180 0.01789782 [0.91343796] [1.4125162]\n",
      "200 0.01563031 [0.919107] [1.3920493]\n",
      "220 0.013650069 [0.9244047] [1.3729229]\n",
      "240 0.01192073 [0.9293555] [1.3550491]\n",
      "260 0.010410472 [0.933982] [1.3383458]\n",
      "280 0.009091536 [0.9383056] [1.322736]\n",
      "300 0.007939668 [0.94234604] [1.3081489]\n",
      "320 0.00693378 [0.9461219] [1.294517]\n",
      "340 0.006055315 [0.9496504] [1.2817779]\n",
      "360 0.00528816 [0.9529478] [1.269873]\n",
      "380 0.004618183 [0.95602936] [1.2587479]\n",
      "400 0.004033106 [0.95890903] [1.2483515]\n",
      "420 0.003522118 [0.9616001] [1.2386357]\n",
      "440 0.0030758972 [0.96411496] [1.2295562]\n",
      "460 0.0026862146 [0.96646506] [1.2210717]\n",
      "480 0.0023458838 [0.9686613] [1.2131424]\n",
      "500 0.0020486745 [0.97071373] [1.2057326]\n",
      "520 0.0017891232 [0.97263175] [1.1988081]\n",
      "540 0.0015624496 [0.9744241] [1.1923368]\n",
      "560 0.0013644907 [0.9760992] [1.1862894]\n",
      "580 0.001191621 [0.9776645] [1.1806382]\n",
      "600 0.0010406486 [0.9791272] [1.1753572]\n",
      "620 0.0009088198 [0.98049414] [1.1704222]\n",
      "640 0.0007936802 [0.9817716] [1.1658102]\n",
      "660 0.00069313176 [0.98296535] [1.1615005]\n",
      "680 0.0006053117 [0.9840809] [1.1574727]\n",
      "700 0.00052862294 [0.9851235] [1.1537088]\n",
      "720 0.0004616469 [0.9860979] [1.1501911]\n",
      "740 0.00040316122 [0.9870083] [1.146904]\n",
      "760 0.0003520795 [0.98785913] [1.1438322]\n",
      "780 0.00030746602 [0.9886544] [1.1409612]\n",
      "800 0.00026851616 [0.98939735] [1.1382787]\n",
      "820 0.0002344982 [0.9900918] [1.1357719]\n",
      "840 0.00020478747 [0.99074066] [1.133429]\n",
      "860 0.00017884563 [0.991347] [1.1312399]\n",
      "880 0.0001561893 [0.9919137] [1.129194]\n",
      "900 0.00013639925 [0.99244326] [1.1272821]\n",
      "920 0.00011911788 [0.99293816] [1.1254953]\n",
      "940 0.00010402823 [0.9934007] [1.1238256]\n",
      "960 9.084861e-05 [0.9938328] [1.1222652]\n",
      "980 7.9339e-05 [0.99423665] [1.1208073]\n",
      "1000 6.928995e-05 [0.99461406] [1.1194448]\n",
      "1020 6.0511702e-05 [0.99496686] [1.1181713]\n",
      "1040 5.284326e-05 [0.99529654] [1.1169811]\n",
      "1060 4.6147503e-05 [0.9956045] [1.1158689]\n",
      "1080 4.030042e-05 [0.99589247] [1.1148295]\n",
      "1100 3.5195335e-05 [0.99616146] [1.1138583]\n",
      "1120 3.0735733e-05 [0.9964128] [1.1129507]\n",
      "1140 2.6841657e-05 [0.9966478] [1.1121025]\n",
      "1160 2.344074e-05 [0.9968673] [1.1113099]\n",
      "1180 2.0470834e-05 [0.9970725] [1.1105691]\n",
      "1200 1.7876155e-05 [0.9972643] [1.1098765]\n",
      "1220 1.5611533e-05 [0.99744344] [1.1092298]\n",
      "1240 1.3633542e-05 [0.9976109] [1.1086253]\n",
      "1260 1.1906665e-05 [0.9977674] [1.1080605]\n",
      "1280 1.0397492e-05 [0.99791366] [1.1075324]\n",
      "1300 9.080643e-06 [0.9980503] [1.1070391]\n",
      "1320 7.930076e-06 [0.99817795] [1.1065781]\n",
      "1340 6.924957e-06 [0.9982973] [1.1061472]\n",
      "1360 6.047797e-06 [0.9984088] [1.1057446]\n",
      "1380 5.2817495e-06 [0.998513] [1.1053684]\n",
      "1400 4.6122013e-06 [0.9986104] [1.1050167]\n",
      "1420 4.0276977e-06 [0.9987014] [1.1046883]\n",
      "1440 3.517926e-06 [0.9987865] [1.1043811]\n",
      "1460 3.0718081e-06 [0.998866] [1.104094]\n",
      "1480 2.6823002e-06 [0.9989403] [1.1038259]\n",
      "1500 2.3426169e-06 [0.99900967] [1.1035753]\n",
      "1520 2.0463267e-06 [0.9990745] [1.1033413]\n",
      "1540 1.7869572e-06 [0.9991351] [1.1031225]\n",
      "1560 1.5603321e-06 [0.9991917] [1.102918]\n",
      "1580 1.3628314e-06 [0.99924463] [1.1027269]\n",
      "1600 1.190297e-06 [0.9992941] [1.1025482]\n",
      "1620 1.0392107e-06 [0.9993404] [1.1023813]\n",
      "1640 9.073974e-07 [0.9993835] [1.1022253]\n",
      "1660 7.926372e-07 [0.9994239] [1.1020796]\n",
      "1680 6.9220033e-07 [0.99946165] [1.1019435]\n",
      "1700 6.0461764e-07 [0.99949694] [1.1018162]\n",
      "1720 5.28025e-07 [0.99952984] [1.1016973]\n",
      "1740 4.6119317e-07 [0.9995606] [1.1015862]\n",
      "1760 4.0278115e-07 [0.9995894] [1.1014824]\n",
      "1780 3.517452e-07 [0.99961627] [1.1013852]\n",
      "1800 3.0724928e-07 [0.99964136] [1.1012946]\n",
      "1820 2.6814587e-07 [0.99966484] [1.1012098]\n",
      "1840 2.3431599e-07 [0.9996868] [1.1011306]\n",
      "1860 2.0463645e-07 [0.99970734] [1.1010566]\n",
      "1880 1.7872536e-07 [0.99972653] [1.1009874]\n",
      "1900 1.560285e-07 [0.9997444] [1.1009227]\n",
      "1920 1.3629192e-07 [0.99976116] [1.1008621]\n",
      "1940 1.1903627e-07 [0.9997768] [1.1008059]\n",
      "1960 1.03932805e-07 [0.9997913] [1.1007531]\n",
      "1980 9.088599e-08 [0.999805] [1.100704]\n",
      "2000 7.9324e-08 [0.9998177] [1.1006578]\n",
      "[6.0997467]\n",
      "[3.600202]\n",
      "[2.6003842 4.60002  ]\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None])\n",
    "y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "hypothesis = x * W + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    _cost, _W, _b, _ = \\\n",
    "        sess.run([cost, W, b, train],\n",
    "            feed_dict={x: [1, 2, 3, 4, 5], y: [2.1, 3.1, 4.1, 5.1, 6.1]})\n",
    "    if step % 20 == 0:\n",
    "        print(step, _cost, _W, _b)\n",
    "\n",
    "print(sess.run(hypothesis, feed_dict={x: [5]}))\n",
    "print(sess.run(hypothesis, feed_dict={x: [2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={x: [1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "OM906N88HkzC",
   "metadata": {
    "id": "OM906N88HkzC",
    "outputId": "da4e01bf-4574-4947-d1a9-becec6d2531f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 24.492353 \t [0.5278827] \t [-0.98970443]\n",
      "20 \t 1.5172867 \t [2.150767] \t [-0.0980537]\n",
      "40 \t 1.1900975 \t [2.2491026] \t [0.11459204]\n",
      "60 \t 1.0791618 \t [2.2049124] \t [0.25657988]\n",
      "80 \t 0.9800968 \t [2.149668] \t [0.38611847]\n",
      "100 \t 0.89013934 \t [2.0957692] \t [0.50901914]\n",
      "120 \t 0.8084386 \t [2.0442848] \t [0.62609166]\n",
      "140 \t 0.73423696 \t [1.9952086] \t [0.73765695]\n",
      "160 \t 0.666846 \t [1.9484373] \t [0.8439788]\n",
      "180 \t 0.60563993 \t [1.9038643] \t [0.9453039]\n",
      "200 \t 0.55005187 \t [1.8613861] \t [1.0418671]\n",
      "220 \t 0.49956584 \t [1.8209041] \t [1.1338923]\n",
      "240 \t 0.45371366 \t [1.7823246] \t [1.2215925]\n",
      "260 \t 0.41206995 \t [1.7455581] \t [1.3051713]\n",
      "280 \t 0.3742486 \t [1.7105196] \t [1.3848218]\n",
      "300 \t 0.33989847 \t [1.6771277] \t [1.4607294]\n",
      "320 \t 0.30870134 \t [1.6453054] \t [1.5330693]\n",
      "340 \t 0.28036758 \t [1.6149786] \t [1.6020094]\n",
      "360 \t 0.25463444 \t [1.5860767] \t [1.6677097]\n",
      "380 \t 0.23126312 \t [1.5585333] \t [1.7303226]\n",
      "400 \t 0.21003668 \t [1.5322841] \t [1.7899928]\n",
      "420 \t 0.19075875 \t [1.5072689] \t [1.8468587]\n",
      "440 \t 0.17325027 \t [1.4834292] \t [1.9010519]\n",
      "460 \t 0.15734868 \t [1.4607097] \t [1.9526985]\n",
      "480 \t 0.1429065 \t [1.439058] \t [2.001918]\n",
      "500 \t 0.1297899 \t [1.418424] \t [2.048824]\n",
      "520 \t 0.117877364 \t [1.3987595] \t [2.093526]\n",
      "540 \t 0.107058145 \t [1.3800193] \t [2.1361268]\n",
      "560 \t 0.09723196 \t [1.36216] \t [2.1767254]\n",
      "580 \t 0.0883076 \t [1.3451397] \t [2.2154164]\n",
      "600 \t 0.08020232 \t [1.3289194] \t [2.252289]\n",
      "620 \t 0.07284104 \t [1.3134613] \t [2.2874286]\n",
      "640 \t 0.06615534 \t [1.2987297] \t [2.3209171]\n",
      "660 \t 0.060083415 \t [1.2846906] \t [2.3528311]\n",
      "680 \t 0.05456869 \t [1.2713113] \t [2.383246]\n",
      "700 \t 0.04956009 \t [1.2585604] \t [2.4122314]\n",
      "720 \t 0.045011345 \t [1.2464092] \t [2.439854]\n",
      "740 \t 0.040879995 \t [1.2348288] \t [2.4661787]\n",
      "760 \t 0.037127864 \t [1.2237926] \t [2.4912665]\n",
      "780 \t 0.033720095 \t [1.2132753] \t [2.515175]\n",
      "800 \t 0.030625163 \t [1.2032521] \t [2.53796]\n",
      "820 \t 0.02781429 \t [1.1937001] \t [2.559674]\n",
      "840 \t 0.025261348 \t [1.1845969] \t [2.5803678]\n",
      "860 \t 0.022942828 \t [1.1759216] \t [2.6000888]\n",
      "880 \t 0.02083703 \t [1.167654] \t [2.6188831]\n",
      "900 \t 0.018924559 \t [1.1597749] \t [2.636794]\n",
      "920 \t 0.017187612 \t [1.1522661] \t [2.6538632]\n",
      "940 \t 0.015610076 \t [1.1451102] \t [2.6701303]\n",
      "960 \t 0.014177299 \t [1.1382908] \t [2.6856327]\n",
      "980 \t 0.012876113 \t [1.1317917] \t [2.7004066]\n",
      "1000 \t 0.011694238 \t [1.125598] \t [2.7144868]\n",
      "1020 \t 0.010620876 \t [1.119695] \t [2.727905]\n",
      "1040 \t 0.009646026 \t [1.1140698] \t [2.7406926]\n",
      "1060 \t 0.0087606935 \t [1.108709] \t [2.7528794]\n",
      "1080 \t 0.007956549 \t [1.1035999] \t [2.7644935]\n",
      "1100 \t 0.0072262757 \t [1.0987309] \t [2.775561]\n",
      "1120 \t 0.00656303 \t [1.0940909] \t [2.786109]\n",
      "1140 \t 0.005960655 \t [1.0896691] \t [2.7961607]\n",
      "1160 \t 0.005413592 \t [1.0854551] \t [2.8057404]\n",
      "1180 \t 0.0049166917 \t [1.0814394] \t [2.8148699]\n",
      "1200 \t 0.004465426 \t [1.0776117] \t [2.8235703]\n",
      "1220 \t 0.004055586 \t [1.0739642] \t [2.8318617]\n",
      "1240 \t 0.0036833556 \t [1.0704882] \t [2.8397632]\n",
      "1260 \t 0.0033452788 \t [1.0671756] \t [2.8472939]\n",
      "1280 \t 0.0030382369 \t [1.0640187] \t [2.8544705]\n",
      "1300 \t 0.0027594015 \t [1.0610105] \t [2.8613095]\n",
      "1320 \t 0.002506121 \t [1.0581429] \t [2.8678274]\n",
      "1340 \t 0.0022761023 \t [1.0554104] \t [2.874039]\n",
      "1360 \t 0.002067187 \t [1.0528064] \t [2.8799589]\n",
      "1380 \t 0.0018774504 \t [1.0503244] \t [2.8856003]\n",
      "1400 \t 0.0017051334 \t [1.0479596] \t [2.8909767]\n",
      "1420 \t 0.0015486246 \t [1.0457056] \t [2.8961005]\n",
      "1440 \t 0.0014064745 \t [1.0435574] \t [2.9009838]\n",
      "1460 \t 0.0012773913 \t [1.0415107] \t [2.9056373]\n",
      "1480 \t 0.0011601517 \t [1.0395595] \t [2.9100716]\n",
      "1500 \t 0.0010536703 \t [1.0377003] \t [2.9142978]\n",
      "1520 \t 0.00095695816 \t [1.0359287] \t [2.9183254]\n",
      "1540 \t 0.0008691116 \t [1.03424] \t [2.9221644]\n",
      "1560 \t 0.00078933424 \t [1.0326308] \t [2.9258225]\n",
      "1580 \t 0.0007168885 \t [1.0310973] \t [2.9293084]\n",
      "1600 \t 0.00065108325 \t [1.0296358] \t [2.932631]\n",
      "1620 \t 0.00059132196 \t [1.028243] \t [2.9357972]\n",
      "1640 \t 0.0005370618 \t [1.0269157] \t [2.9388142]\n",
      "1660 \t 0.00048776343 \t [1.0256509] \t [2.9416895]\n",
      "1680 \t 0.00044299464 \t [1.0244454] \t [2.9444299]\n",
      "1700 \t 0.0004023379 \t [1.0232966] \t [2.9470415]\n",
      "1720 \t 0.00036540997 \t [1.0222018] \t [2.9495304]\n",
      "1740 \t 0.00033187552 \t [1.0211583] \t [2.9519022]\n",
      "1760 \t 0.00030141036 \t [1.0201638] \t [2.9541626]\n",
      "1780 \t 0.00027374472 \t [1.0192162] \t [2.9563172]\n",
      "1800 \t 0.00024862075 \t [1.018313] \t [2.95837]\n",
      "1820 \t 0.0002257856 \t [1.0174521] \t [2.9603274]\n",
      "1840 \t 0.00020506237 \t [1.016632] \t [2.9621918]\n",
      "1860 \t 0.00018624251 \t [1.0158504] \t [2.9639685]\n",
      "1880 \t 0.00016915145 \t [1.0151054] \t [2.965662]\n",
      "1900 \t 0.00015362313 \t [1.0143955] \t [2.9672759]\n",
      "1920 \t 0.00013952174 \t [1.013719] \t [2.9688137]\n",
      "1940 \t 0.00012671827 \t [1.0130742] \t [2.9702792]\n",
      "1960 \t 0.00011508679 \t [1.0124598] \t [2.971676]\n",
      "1980 \t 0.00010452542 \t [1.0118742] \t [2.973007]\n",
      "2000 \t 9.492813e-05 \t [1.0113162] \t [2.9742758]\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "#데이터 셋\n",
    "x_train=[1,2,3]\n",
    "y_train=[4,5,6]\n",
    "W=tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b=tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "#(H(x))\n",
    "hypothesis=x_train*W+b\n",
    "#손실함수\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-y_train))\n",
    "#경사하강\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "#개선\n",
    "train=optimizer.minimize(cost)\n",
    "#훈련\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if (step % 20 == 0):\n",
    "        print(step, '\\t', sess.run(cost), '\\t', sess.run(W), '\\t', sess.run(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ZpUHdJpbHkzC",
   "metadata": {
    "id": "ZpUHdJpbHkzC",
    "outputId": "0c320509-8bd7-4c64-9037-aa1d12089a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.971836 [0.8928878] [0.38379902]\n",
      "20 0.69095355 [1.8020409] [0.9044743]\n",
      "40 0.5516862 [1.8504376] [1.0408272]\n",
      "60 0.50036263 [1.8186675] [1.1365072]\n",
      "80 0.4544311 [1.7809738] [1.2244277]\n",
      "100 0.41272163 [1.7443454] [1.3079057]\n",
      "120 0.37484017 [1.709371] [1.387431]\n",
      "140 0.34043598 [1.6760339] [1.4632162]\n",
      "160 0.3091894 [1.6442628] [1.5354393]\n",
      "180 0.28081092 [1.613985] [1.6042683]\n",
      "200 0.2550368 [1.5851297] [1.6698625]\n",
      "220 0.23162861 [1.5576309] [1.7323741]\n",
      "240 0.21036868 [1.5314242] [1.7919482]\n",
      "260 0.19106014 [1.506449] [1.8487223]\n",
      "280 0.17352392 [1.4826479] [1.9028281]\n",
      "300 0.15759717 [1.4599652] [1.954391]\n",
      "320 0.14313227 [1.4383485] [2.0035307]\n",
      "340 0.12999514 [1.4177477] [2.050361]\n",
      "360 0.11806366 [1.3981153] [2.0949903]\n",
      "380 0.107227415 [1.3794055] [2.1375222]\n",
      "400 0.09738573 [1.3615749] [2.1780553]\n",
      "420 0.0884473 [1.3445821] [2.2166836]\n",
      "440 0.08032922 [1.3283882] [2.2534964]\n",
      "460 0.07295627 [1.3129551] [2.2885795]\n",
      "480 0.06626013 [1.2982475] [2.3220134]\n",
      "500 0.060178548 [1.2842311] [2.353876]\n",
      "520 0.054655105 [1.2708732] [2.3842418]\n",
      "540 0.04963861 [1.2581432] [2.41318]\n",
      "560 0.045082588 [1.2460114] [2.4407585]\n",
      "580 0.04094469 [1.2344496] [2.4670413]\n",
      "600 0.03718659 [1.2234312] [2.4920886]\n",
      "620 0.033773426 [1.2129307] [2.5159585]\n",
      "640 0.030673595 [1.2029235] [2.5387065]\n",
      "660 0.027858192 [1.1933869] [2.560386]\n",
      "680 0.025301253 [1.1842984] [2.5810463]\n",
      "700 0.022978956 [1.175637] [2.600736]\n",
      "720 0.020869851 [1.1673827] [2.6195]\n",
      "740 0.018954394 [1.1595165] [2.6373816]\n",
      "760 0.017214714 [1.1520199] [2.6544232]\n",
      "780 0.015634658 [1.1448753] [2.6706643]\n",
      "800 0.014199562 [1.1380664] [2.6861424]\n",
      "820 0.012896306 [1.1315778] [2.7008924]\n",
      "840 0.011712606 [1.1253943] [2.7149494]\n",
      "860 0.01063758 [1.1195011] [2.7283459]\n",
      "880 0.009661217 [1.113885] [2.7411125]\n",
      "900 0.008774468 [1.1085328] [2.7532797]\n",
      "920 0.0079690805 [1.103432] [2.7648747]\n",
      "940 0.0072376556 [1.098571] [2.775925]\n",
      "960 0.006573358 [1.0939386] [2.7864556]\n",
      "980 0.005970011 [1.0895238] [2.7964916]\n",
      "1000 0.0054220636 [1.0853164] [2.8060558]\n",
      "1020 0.004924403 [1.0813068] [2.8151705]\n",
      "1040 0.004472408 [1.0774857] [2.823857]\n",
      "1060 0.004061909 [1.0738441] [2.8321352]\n",
      "1080 0.0036891035 [1.0703738] [2.8400238]\n",
      "1100 0.0033504919 [1.0670666] [2.847542]\n",
      "1120 0.0030429747 [1.0639148] [2.854707]\n",
      "1140 0.0027636997 [1.0609113] [2.8615348]\n",
      "1160 0.0025100333 [1.0580485] [2.8680422]\n",
      "1180 0.0022796635 [1.0553205] [2.8742435]\n",
      "1200 0.002070422 [1.0527205] [2.880154]\n",
      "1220 0.0018803892 [1.0502429] [2.885786]\n",
      "1240 0.0017077968 [1.0478816] [2.8911538]\n",
      "1260 0.001551047 [1.0456314] [2.896269]\n",
      "1280 0.0014086887 [1.043487] [2.901144]\n",
      "1300 0.0012794049 [1.0414432] [2.9057896]\n",
      "1320 0.0011619659 [1.0394955] [2.9102173]\n",
      "1340 0.0010553175 [1.0376394] [2.9144366]\n",
      "1360 0.0009584729 [1.0358707] [2.918457]\n",
      "1380 0.0008705067 [1.0341852] [2.922289]\n",
      "1400 0.0007906137 [1.0325786] [2.9259412]\n",
      "1420 0.0007180444 [1.0310473] [2.9294217]\n",
      "1440 0.0006521403 [1.0295883] [2.9327385]\n",
      "1460 0.00059229095 [1.028198] [2.9358995]\n",
      "1480 0.00053792837 [1.0268728] [2.9389117]\n",
      "1500 0.00048855593 [1.0256099] [2.9417827]\n",
      "1520 0.00044371316 [1.0244063] [2.9445188]\n",
      "1540 0.00040299003 [1.0232594] [2.947126]\n",
      "1560 0.00036600025 [1.0221664] [2.9496107]\n",
      "1580 0.00033241636 [1.0211248] [2.9519787]\n",
      "1600 0.0003019048 [1.0201318] [2.9542358]\n",
      "1620 0.00027419202 [1.0191855] [2.9563863]\n",
      "1640 0.000249028 [1.0182841] [2.9584355]\n",
      "1660 0.00022617674 [1.017425] [2.9603882]\n",
      "1680 0.0002054231 [1.0166063] [2.96225]\n",
      "1700 0.00018656703 [1.0158257] [2.964024]\n",
      "1720 0.00016944302 [1.015082] [2.965715]\n",
      "1740 0.000153891 [1.0143732] [2.9673262]\n",
      "1760 0.00013976611 [1.0136976] [2.9688618]\n",
      "1780 0.00012693678 [1.0130539] [2.9703252]\n",
      "1800 0.00011528514 [1.0124406] [2.97172]\n",
      "1820 0.00010470619 [1.0118558] [2.9730487]\n",
      "1840 9.509568e-05 [1.0112989] [2.9743152]\n",
      "1860 8.6368666e-05 [1.0107679] [2.975522]\n",
      "1880 7.8442026e-05 [1.0102619] [2.9766724]\n",
      "1900 7.124211e-05 [1.0097795] [2.977769]\n",
      "1920 6.470428e-05 [1.0093199] [2.9788136]\n",
      "1940 5.87589e-05 [1.0088817] [2.97981]\n",
      "1960 5.336647e-05 [1.0084642] [2.980759]\n",
      "1980 4.846884e-05 [1.0080663] [2.9816632]\n",
      "2000 4.4019118e-05 [1.0076872] [2.982525]\n",
      "[7.013274]\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "#데이터 셋\n",
    "x_train=tf.placeholder(tf.float32, shape=[None]) #x_train=[1,2,3]\n",
    "y_train=tf.placeholder(tf.float32, shape=[None]) #y_train=[1,2,3]\n",
    "W=tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b=tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "#(H(x))\n",
    "hypothesis=x_train*W+b\n",
    "#손실함수\n",
    "cost=tf.reduce_mean(tf.square(hypothesis-y_train))\n",
    "#경사하강\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "#개선\n",
    "train=optimizer.minimize(cost)\n",
    "#훈련\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    #sess.run(train)\n",
    "    _cost, _W, _b, _= \\\n",
    "      sess.run([cost, W, b, train],\n",
    "               feed_dict={\n",
    "                   x_train: [1,2,3],\n",
    "                   y_train: [4,5,6]\n",
    "               })\n",
    "    if step % 20 == 0:\n",
    "      print(step, _cost, _W, _b)\n",
    "print(sess.run(hypothesis, feed_dict={x_train: [4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5BxaLF64HkzC",
   "metadata": {
    "id": "5BxaLF64HkzC"
   },
   "source": [
    "## XOR 문제 해결을"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "Y7GdpkQTHkzC",
   "metadata": {
    "id": "Y7GdpkQTHkzC",
    "outputId": "8eaa293a-e2bc-42c9-d5c7-44d657209617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('./datasets/train.txt')\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ign2qkN-HkzD",
   "metadata": {
    "id": "ign2qkN-HkzD",
    "outputId": "f63c92bb-2d3a-4d78-bf9c-f4c2d2ba3149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]]\n",
      "*****\n",
      "[1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_data = xy[0:-1]\n",
    "y_data = xy[-1]\n",
    "\n",
    "print(x_data)\n",
    "print(\"*****\")\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "u8GR165UHkzD",
   "metadata": {
    "id": "u8GR165UHkzD"
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "mUOqbzPRHkzD",
   "metadata": {
    "id": "mUOqbzPRHkzD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\DEV\\miniconda3\\envs\\tf38_cpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# Our hypothesis\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1.+tf.exp(-h))\n",
    "\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "learning_rate = tf.Variable(0.01)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1LBKYggNHkzD",
   "metadata": {
    "id": "1LBKYggNHkzD",
    "outputId": "8f8ebe4f-472a-4fe2-9576-1b1d60c6704c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6942447 [[-0.5709181  -0.13003848  0.0427733 ]]\n",
      "500 0.6934566 [[-0.5709181  -0.06945731  0.04447583]]\n",
      "1000 0.6932679 [[-0.5709181  -0.04112526  0.03397733]]\n",
      "1500 0.69319844 [[-0.5709181  -0.02577433  0.0237294 ]]\n",
      "2000 0.69316936 [[-0.5709181  -0.01660734  0.01602232]]\n",
      "2500 0.6931569 [[-0.5709181  -0.01083725  0.01066994]]\n",
      "3000 0.6931514 [[-0.5709181  -0.00711191  0.00706407]]\n",
      "3500 0.6931489 [[-0.5709181  -0.00467874  0.00466505]]\n",
      "4000 0.69314796 [[-0.5709181  -0.00308133  0.00307741]]\n",
      "4500 0.6931476 [[-0.5709181  -0.00203026  0.00202913]]\n",
      "5000 0.69314736 [[-0.5709181  -0.001338    0.00133765]]\n",
      "[array([[0.50033444, 0.4996655 , 0.49999988]], dtype=float32), array([[1., 0., 0.]], dtype=float32), array([[ True, False,  True]]), 0.6666667]\n",
      "Accuracy: 0.6666667\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(5001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data })\n",
    "        if step % 500 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "\n",
    "# Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "# Calculate Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(sess.run([hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy],feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sQFWQCQSHkzD",
   "metadata": {
    "id": "sQFWQCQSHkzD"
   },
   "source": [
    "# 신경망 구현: MLP: Neural Network with 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "VWsuBzTmHkzE",
   "metadata": {
    "id": "VWsuBzTmHkzE",
    "outputId": "63a74615-ced2-4d0f-c6f2-7ef275cd89b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7241926\n",
      "500 0.6932\n",
      "1000 0.6922654\n",
      "1500 0.69071686\n",
      "2000 0.68672127\n",
      "2500 0.67375267\n",
      "3000 0.6368515\n",
      "3500 0.5792512\n",
      "4000 0.52406365\n",
      "4500 0.472346\n",
      "5000 0.3350476\n",
      "5500 0.17062394\n",
      "6000 0.10167086\n",
      "6500 0.07014176\n",
      "7000 0.05291698\n",
      "[array([[0.05050014],\n",
      "       [0.96185565],\n",
      "       [0.9244921 ],\n",
      "       [0.04155813]], dtype=float32)]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "xy = np.loadtxt('./datasets/train.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1],(4,1))\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "# define 2 layer Neural Network\n",
    "W1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "b1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "# Our hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2)+b2)\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# For this time, learning rate is very important.\n",
    "learning_rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(7001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "    # Test model\n",
    "    correct = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    # Calculate Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    print(sess.run([hypothesis],feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WVYV81n0HkzE",
   "metadata": {
    "id": "WVYV81n0HkzE"
   },
   "source": [
    "# 신경망 구현: MLP: Wide Neural Network with 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "zX8URWWVHkzE",
   "metadata": {
    "id": "zX8URWWVHkzE",
    "outputId": "51fcd4ac-f74d-4b20-b78b-fce4a9f59a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8568788\n",
      "2000 0.42546427\n",
      "4000 0.08535412\n",
      "6000 0.03135662\n",
      "8000 0.017576568\n",
      "10000 0.011851653\n",
      "12000 0.008816486\n",
      "14000 0.006964482\n",
      "16000 0.005727258\n",
      "18000 0.004846968\n",
      "20000 0.0041910387\n",
      "22000 0.0036846849\n",
      "24000 0.0032827484\n",
      "26000 0.0029564803\n",
      "28000 0.0026866049\n",
      "30000 0.0024599447\n",
      "[array([[0.00260405],\n",
      "       [0.9970063 ],\n",
      "       [0.9980808 ],\n",
      "       [0.0023104 ]], dtype=float32)]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "xy = np.loadtxt('./datasets/train.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1],(4,1))\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "# define 2 layer Neural Network - we will use 10 units\n",
    "W1 = tf.Variable(tf.random_uniform([2, 10], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([10, 1], -1.0, 1.0))\n",
    "b1 = tf.Variable(tf.zeros([10]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "# Our hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2)+b2)\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# For this time, learning rate is very important.\n",
    "learning_rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(30001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "    # Test model\n",
    "    correct = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    # Calculate Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    print(sess.run([hypothesis],feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FaBV3mPNHkzE",
   "metadata": {
    "id": "FaBV3mPNHkzE"
   },
   "source": [
    "# 신경망 구현: MLP: Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "MMCvtGmtHkzE",
   "metadata": {
    "id": "MMCvtGmtHkzE",
    "outputId": "a53e9a8c-885f-4d74-8c9f-68a8b2df45f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7454236\n",
      "2000 0.6451336\n",
      "4000 0.09627043\n",
      "6000 0.019333255\n",
      "8000 0.0103016095\n",
      "10000 0.006980075\n",
      "12000 0.0052685747\n",
      "14000 0.0042279162\n",
      "16000 0.0035292404\n",
      "18000 0.0030281208\n",
      "20000 0.0026513445\n",
      "22000 0.0023577842\n",
      "24000 0.002122697\n",
      "26000 0.0019301398\n",
      "28000 0.0017696097\n",
      "30000 0.0016336876\n",
      "[array([[0.0017147 ],\n",
      "       [0.99845356],\n",
      "       [0.99849796],\n",
      "       [0.00176623]], dtype=float32)]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "xy = np.loadtxt('./datasets/train.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1],(4,1))\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "# define 2 layer Neural Network - we will use 3 layers.\n",
    "W1 = tf.Variable(tf.random_uniform([2,10], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([10,5], -1.0, 1.0))\n",
    "W3 = tf.Variable(tf.random_uniform([5,1], -1.0, 1.0))\n",
    "b1 = tf.Variable(tf.zeros([10]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([5]), name=\"Bias2\")\n",
    "b3 = tf.Variable(tf.zeros([1]), name=\"Bias3\")\n",
    "# Our hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "L3 = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L3, W3) + b3)\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# For this time, learning rate is very important.\n",
    "learning_rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(30001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step,\n",
    "                sess.run(cost, feed_dict={X:x_data, Y:y_data})\n",
    "                #sess.run(W1),\n",
    "                #sess.run(W2)\n",
    "            )\n",
    "    # Test model\n",
    "    correct = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    print(sess.run([hypothesis],feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "OAWsvW7lHkzF",
   "metadata": {
    "id": "OAWsvW7lHkzF"
   },
   "outputs": [],
   "source": [
    "# 신경망 구현: MLP: Deep Neural Network 좀 더 깊이를 더하면…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6TJNW0uYHkzF",
   "metadata": {
    "id": "6TJNW0uYHkzF",
    "outputId": "ac0c628f-a059-47af-a0d3-e89c9c3c1f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7342313\n",
      "2000 0.006312538\n",
      "4000 0.0009804426\n",
      "6000 0.00050063396\n",
      "8000 0.00033093878\n",
      "10000 0.0002454245\n",
      "12000 0.00019430152\n",
      "14000 0.00016042477\n",
      "16000 0.00013638537\n",
      "18000 0.00011850154\n",
      "20000 0.00010465671\n",
      "22000 9.365849e-05\n",
      "24000 8.470204e-05\n",
      "26000 7.728062e-05\n",
      "28000 7.1036535e-05\n",
      "30000 6.570153e-05\n",
      "[array([[3.8131897e-05],\n",
      "       [9.9992716e-01],\n",
      "       [9.9993408e-01],\n",
      "       [8.5884436e-05]], dtype=float32)]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "xy = np.loadtxt('./datasets/train.txt', unpack=True)\n",
    "x_data = np.transpose(xy[0:-1])\n",
    "y_data = np.reshape(xy[-1],(4,1))\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "# define 2 layer Neural Network - we will use 3 layers.\n",
    "W1 = tf.Variable(tf.random_uniform([2,10], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([10,10], -1.0, 1.0))\n",
    "W3 = tf.Variable(tf.random_uniform([10,10], -1.0, 1.0))\n",
    "W4 = tf.Variable(tf.random_uniform([10,1], -1.0, 1.0))\n",
    "b1 = tf.Variable(tf.zeros([10]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([10]), name=\"Bias2\")\n",
    "b3 = tf.Variable(tf.zeros([10]), name=\"Bias3\")\n",
    "b4 = tf.Variable(tf.zeros([1]), name=\"Bias4\")\n",
    "# Our hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "L3 = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "L4 = tf.sigmoid(tf.matmul(L3, W3) + b2)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L4, W4) + b4)\n",
    "# Cross entropy cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "# For this time, learning rate is very important.\n",
    "learning_rate = tf.Variable(0.3)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(30001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step,\n",
    "                sess.run(cost, feed_dict={X:x_data, Y:y_data})\n",
    "                #sess.run(W1),\n",
    "                #sess.run(W2)\n",
    "            )\n",
    "    # Test model\n",
    "    correct = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    print(sess.run([hypothesis],feed_dict={X:x_data, Y:y_data}))\n",
    "    print(\"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x7ttSidcHkzF",
   "metadata": {
    "id": "x7ttSidcHkzF"
   },
   "source": [
    "# 텐서플로와 케라스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41dbb49e-5fea-475a-af17-42ad73e81c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "598a7f17-7705-4583-b9e2-9f71270ecbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22597224-783b-41e7-943a-6e62795db9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24555d54-c936-4ded-809f-7f141bce356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029afb6f-d97d-45c8-a96f-40c867ccc5de",
   "metadata": {},
   "source": [
    "# 재시작 후 다시 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fe4c1-86f8-4088-a77a-74590e1054fe",
   "metadata": {},
   "source": [
    "학습 시간이 많이 걸립니다. 시간이 많을 때 실행 해 보세요 ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CfbNjNicHkzF",
   "metadata": {
    "id": "CfbNjNicHkzF",
    "outputId": "cc7a32bb-681d-408b-a6d7-c2119bbc7638",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9 (36.00 Byte)\n",
      "Trainable params: 9 (36.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "4/4 [==============================] - 1s 6ms/step - loss: 0.2673\n",
      "Epoch 2/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2671\n",
      "Epoch 3/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2669\n",
      "Epoch 4/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2667\n",
      "Epoch 5/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2665\n",
      "Epoch 6/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2664\n",
      "Epoch 7/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2662\n",
      "Epoch 8/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2660\n",
      "Epoch 9/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2658\n",
      "Epoch 10/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2656\n",
      "Epoch 11/2000\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2654\n",
      "Epoch 12/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2653\n",
      "Epoch 13/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2651\n",
      "Epoch 14/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2649\n",
      "Epoch 15/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2648\n",
      "Epoch 16/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2646\n",
      "Epoch 17/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2644\n",
      "Epoch 18/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2643\n",
      "Epoch 19/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2641\n",
      "Epoch 20/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2639\n",
      "Epoch 21/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2638\n",
      "Epoch 22/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2636\n",
      "Epoch 23/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2635\n",
      "Epoch 24/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2633\n",
      "Epoch 25/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2632\n",
      "Epoch 26/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2630\n",
      "Epoch 27/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2629\n",
      "Epoch 28/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2627\n",
      "Epoch 29/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2626\n",
      "Epoch 30/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2624\n",
      "Epoch 31/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2623\n",
      "Epoch 32/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2622\n",
      "Epoch 33/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2620\n",
      "Epoch 34/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2619\n",
      "Epoch 35/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2618\n",
      "Epoch 36/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2616\n",
      "Epoch 37/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2615\n",
      "Epoch 38/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2614\n",
      "Epoch 39/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2612\n",
      "Epoch 40/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2611\n",
      "Epoch 41/2000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2610\n",
      "Epoch 42/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2609\n",
      "Epoch 43/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2607\n",
      "Epoch 44/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2606\n",
      "Epoch 45/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2605\n",
      "Epoch 46/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2604\n",
      "Epoch 47/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2603\n",
      "Epoch 48/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2602\n",
      "Epoch 49/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2600\n",
      "Epoch 50/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2599\n",
      "Epoch 51/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2598\n",
      "Epoch 52/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2597\n",
      "Epoch 53/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2596\n",
      "Epoch 54/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2595\n",
      "Epoch 55/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2594\n",
      "Epoch 56/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2593\n",
      "Epoch 57/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2592\n",
      "Epoch 58/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2591\n",
      "Epoch 59/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2590\n",
      "Epoch 60/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2589\n",
      "Epoch 61/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2588\n",
      "Epoch 62/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2587\n",
      "Epoch 63/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2586\n",
      "Epoch 64/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2585\n",
      "Epoch 65/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2584\n",
      "Epoch 66/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2583\n",
      "Epoch 67/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2582\n",
      "Epoch 68/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2581\n",
      "Epoch 69/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2580\n",
      "Epoch 70/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2580\n",
      "Epoch 71/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2579\n",
      "Epoch 72/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2578\n",
      "Epoch 73/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2577\n",
      "Epoch 74/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2576\n",
      "Epoch 75/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2575\n",
      "Epoch 76/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2574\n",
      "Epoch 77/2000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2574\n",
      "Epoch 78/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2573\n",
      "Epoch 79/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2572\n",
      "Epoch 80/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2571\n",
      "Epoch 81/2000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2571\n",
      "Epoch 82/2000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2570\n",
      "Epoch 83/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2569\n",
      "Epoch 84/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2568\n",
      "Epoch 85/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2568\n",
      "Epoch 86/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2567\n",
      "Epoch 87/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2566\n",
      "Epoch 88/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2566\n",
      "Epoch 89/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2565\n",
      "Epoch 90/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2564\n",
      "Epoch 91/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2563\n",
      "Epoch 92/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2563\n",
      "Epoch 93/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2562\n",
      "Epoch 94/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2562\n",
      "Epoch 95/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2561\n",
      "Epoch 96/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2560\n",
      "Epoch 97/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2560\n",
      "Epoch 98/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2559\n",
      "Epoch 99/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2558\n",
      "Epoch 100/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2558\n",
      "Epoch 101/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2557\n",
      "Epoch 102/2000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2557\n",
      "Epoch 103/2000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2556\n",
      "Epoch 104/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2555\n",
      "Epoch 105/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2555\n",
      "Epoch 106/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2554\n",
      "Epoch 107/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2554\n",
      "Epoch 108/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2553\n",
      "Epoch 109/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2553\n",
      "Epoch 110/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2552\n",
      "Epoch 111/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2552\n",
      "Epoch 112/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2551\n",
      "Epoch 113/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2551\n",
      "Epoch 114/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2550\n",
      "Epoch 115/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2550\n",
      "Epoch 116/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2549\n",
      "Epoch 117/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2549\n",
      "Epoch 118/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2548\n",
      "Epoch 119/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2548\n",
      "Epoch 120/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2547\n",
      "Epoch 121/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2547\n",
      "Epoch 122/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2546\n",
      "Epoch 123/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2546\n",
      "Epoch 124/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2546\n",
      "Epoch 125/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2545\n",
      "Epoch 126/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2545\n",
      "Epoch 127/2000\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2544\n",
      "Epoch 128/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2544\n",
      "Epoch 129/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2543\n",
      "Epoch 130/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2543\n",
      "Epoch 131/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2543\n",
      "Epoch 132/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2542\n",
      "Epoch 133/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2542\n",
      "Epoch 134/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2542\n",
      "Epoch 135/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2541\n",
      "Epoch 136/2000\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.2541\n",
      "Epoch 137/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2540\n",
      "Epoch 138/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2540\n",
      "Epoch 139/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2540\n",
      "Epoch 140/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2539\n",
      "Epoch 141/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2539\n",
      "Epoch 142/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2539\n",
      "Epoch 143/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2538\n",
      "Epoch 144/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2538\n",
      "Epoch 145/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2538\n",
      "Epoch 146/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2537\n",
      "Epoch 147/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2537\n",
      "Epoch 148/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2537\n",
      "Epoch 149/2000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2536\n",
      "Epoch 150/2000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2536\n",
      "Epoch 151/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2536\n",
      "Epoch 152/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2535\n",
      "Epoch 153/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2535\n",
      "Epoch 154/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2535\n",
      "Epoch 155/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2534\n",
      "Epoch 156/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2534\n",
      "Epoch 157/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2534\n",
      "Epoch 158/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2534\n",
      "Epoch 159/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2533\n",
      "Epoch 160/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2533\n",
      "Epoch 161/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2533\n",
      "Epoch 162/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2533\n",
      "Epoch 163/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2532\n",
      "Epoch 164/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2532\n",
      "Epoch 165/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2532\n",
      "Epoch 166/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2532\n",
      "Epoch 167/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2531\n",
      "Epoch 168/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2531\n",
      "Epoch 169/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2531\n",
      "Epoch 170/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2531\n",
      "Epoch 171/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2530\n",
      "Epoch 172/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2530\n",
      "Epoch 173/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2530\n",
      "Epoch 174/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2530\n",
      "Epoch 175/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2529\n",
      "Epoch 176/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2529\n",
      "Epoch 177/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2529\n",
      "Epoch 178/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2529\n",
      "Epoch 179/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2529\n",
      "Epoch 180/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2528\n",
      "Epoch 181/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2528\n",
      "Epoch 182/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2528\n",
      "Epoch 183/2000\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.2528\n",
      "Epoch 184/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2528\n",
      "Epoch 185/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2527\n",
      "Epoch 186/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2527\n",
      "Epoch 187/2000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2527\n",
      "Epoch 188/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2527\n",
      "Epoch 189/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2527\n",
      "Epoch 190/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2526\n",
      "Epoch 191/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2526\n",
      "Epoch 192/2000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2526\n",
      "Epoch 193/2000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2526\n",
      "Epoch 194/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2526\n",
      "Epoch 195/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2525\n",
      "Epoch 196/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2525\n",
      "Epoch 197/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2525\n",
      "Epoch 198/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2525\n",
      "Epoch 199/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2525\n",
      "Epoch 200/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2525\n",
      "Epoch 201/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2524\n",
      "Epoch 202/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2524\n",
      "Epoch 203/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2524\n",
      "Epoch 204/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2524\n",
      "Epoch 205/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2524\n",
      "Epoch 206/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2524\n",
      "Epoch 207/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2524\n",
      "Epoch 208/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2523\n",
      "Epoch 209/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2523\n",
      "Epoch 210/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2523\n",
      "Epoch 211/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2523\n",
      "Epoch 212/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2523\n",
      "Epoch 213/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2523\n",
      "Epoch 214/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2523\n",
      "Epoch 215/2000\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2522\n",
      "Epoch 216/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2522\n",
      "Epoch 217/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2522\n",
      "Epoch 218/2000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2522\n",
      "Epoch 219/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2522\n",
      "Epoch 220/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2522\n",
      "Epoch 221/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2522\n",
      "Epoch 222/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2522\n",
      "Epoch 223/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2521\n",
      "Epoch 224/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2521\n",
      "Epoch 225/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2521\n",
      "Epoch 226/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2521\n",
      "Epoch 227/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2521\n",
      "Epoch 228/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2521\n",
      "Epoch 229/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2521\n",
      "Epoch 230/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2521\n",
      "Epoch 231/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2521\n",
      "Epoch 232/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2520\n",
      "Epoch 233/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2520\n",
      "Epoch 234/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2520\n",
      "Epoch 235/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2520\n",
      "Epoch 236/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2520\n",
      "Epoch 237/2000\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2520\n",
      "Epoch 238/2000\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.2520\n",
      "Epoch 239/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2520\n",
      "Epoch 240/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2520\n",
      "Epoch 241/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2520\n",
      "Epoch 242/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2519\n",
      "Epoch 243/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2519\n",
      "Epoch 244/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2519\n",
      "Epoch 245/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2519\n",
      "Epoch 246/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2519\n",
      "Epoch 247/2000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2519\n",
      "Epoch 248/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2519\n",
      "Epoch 249/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2519\n",
      "Epoch 250/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2519\n",
      "Epoch 251/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2519\n",
      "Epoch 252/2000\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2519\n",
      "Epoch 253/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2518\n",
      "Epoch 254/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2518\n",
      "Epoch 255/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2518\n",
      "Epoch 256/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2518\n",
      "Epoch 257/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2518\n",
      "Epoch 258/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2518\n",
      "Epoch 259/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2518\n",
      "Epoch 260/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2518\n",
      "Epoch 261/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2518\n",
      "Epoch 262/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2518\n",
      "Epoch 263/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2518\n",
      "Epoch 264/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2518\n",
      "Epoch 265/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2518\n",
      "Epoch 266/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2517\n",
      "Epoch 267/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2517\n",
      "Epoch 268/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2517\n",
      "Epoch 269/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2517\n",
      "Epoch 270/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2517\n",
      "Epoch 271/2000\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.2517\n",
      "Epoch 272/2000\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.2517\n",
      "Epoch 273/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2517\n",
      "Epoch 274/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2517\n",
      "Epoch 275/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2517\n",
      "Epoch 276/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2517\n",
      "Epoch 277/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2517\n",
      "Epoch 278/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2517\n",
      "Epoch 279/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2517\n",
      "Epoch 280/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2517\n",
      "Epoch 281/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2517\n",
      "Epoch 282/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 283/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2516\n",
      "Epoch 284/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 285/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 286/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 287/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 288/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 289/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 290/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 291/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 292/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2516\n",
      "Epoch 293/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 294/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 295/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2516\n",
      "Epoch 296/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2516\n",
      "Epoch 297/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
      "Epoch 298/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2516\n",
      "Epoch 299/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2516\n",
      "Epoch 300/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2516\n",
      "Epoch 301/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2515\n",
      "Epoch 302/2000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2515\n",
      "Epoch 303/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 304/2000\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 0.2515\n",
      "Epoch 305/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 306/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 307/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 308/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 309/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2515\n",
      "Epoch 310/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 311/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 312/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 313/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2515\n",
      "Epoch 314/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 315/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 316/2000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.2515\n",
      "Epoch 317/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 318/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 319/2000\n",
      "4/4 [==============================] - 0s 111ms/step - loss: 0.2515\n",
      "Epoch 320/2000\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.2515\n",
      "Epoch 321/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 322/2000\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2515\n",
      "Epoch 323/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 324/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2515\n",
      "Epoch 325/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2515\n",
      "Epoch 326/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 327/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 328/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2514\n",
      "Epoch 329/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 330/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 331/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 332/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 333/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 334/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2514\n",
      "Epoch 335/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2514\n",
      "Epoch 336/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2514\n",
      "Epoch 337/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2514\n",
      "Epoch 338/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 339/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 340/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 341/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 342/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 343/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 344/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 345/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 346/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 347/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 348/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 349/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 350/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 351/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2514\n",
      "Epoch 352/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 353/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 354/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 355/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 356/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 357/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2514\n",
      "Epoch 358/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 359/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 360/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 361/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 362/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 363/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 364/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 365/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 366/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2513\n",
      "Epoch 367/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 368/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 369/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 370/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 371/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 372/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 373/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2513\n",
      "Epoch 374/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 375/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 376/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 377/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 378/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 379/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 380/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2513\n",
      "Epoch 381/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 382/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 383/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2513\n",
      "Epoch 384/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 385/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 386/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 387/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 388/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 389/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 390/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 391/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 392/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 393/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 394/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 395/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 396/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 397/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 398/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2513\n",
      "Epoch 399/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 400/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 401/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 402/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 403/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 404/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 405/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2513\n",
      "Epoch 406/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2513\n",
      "Epoch 407/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 408/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2513\n",
      "Epoch 409/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 410/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 411/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 412/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2512\n",
      "Epoch 413/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 414/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 415/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 416/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 417/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 418/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 419/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2512\n",
      "Epoch 420/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 421/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 422/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 423/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 424/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 425/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 426/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2512\n",
      "Epoch 427/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 428/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 429/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 430/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 431/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 432/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 433/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2512\n",
      "Epoch 434/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 435/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 436/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 437/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 438/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 439/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 440/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2512\n",
      "Epoch 441/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 442/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2512\n",
      "Epoch 443/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 444/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 445/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2512\n",
      "Epoch 446/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 447/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 448/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 449/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 450/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 451/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 452/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 453/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 454/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 455/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 456/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 457/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 458/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 459/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 460/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 461/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2512\n",
      "Epoch 462/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 463/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 464/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 465/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 466/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 467/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 468/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 469/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 470/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 471/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 472/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 473/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 474/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 475/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2512\n",
      "Epoch 476/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 477/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 478/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 479/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 480/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 481/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 482/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 483/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2512\n",
      "Epoch 484/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2512\n",
      "Epoch 485/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 486/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 487/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 488/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 489/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 490/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2511\n",
      "Epoch 491/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 492/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 493/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 494/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 495/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 496/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 497/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 498/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 499/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 500/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 501/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 502/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 503/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 504/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 505/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 506/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 507/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 508/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 509/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2511\n",
      "Epoch 510/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 511/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 512/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 513/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2511\n",
      "Epoch 514/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 515/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 516/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 517/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 518/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2511\n",
      "Epoch 519/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 520/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 521/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 522/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 523/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 524/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 525/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 526/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 527/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 528/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 529/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 530/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 531/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 532/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 533/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 534/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 535/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 536/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 537/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 538/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 539/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 540/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 541/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 542/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 543/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 544/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 545/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2511\n",
      "Epoch 546/2000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.2511\n",
      "Epoch 547/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 548/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 549/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 550/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 551/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2511\n",
      "Epoch 552/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 553/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 554/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 555/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 556/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 557/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 558/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 559/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 560/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 561/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 562/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 563/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 564/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 565/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 566/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 567/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 568/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 569/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 570/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 571/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 572/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 573/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2511\n",
      "Epoch 574/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 575/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 576/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 577/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 578/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 579/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 580/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 581/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 582/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 583/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 584/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 585/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 586/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 587/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 588/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 589/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 590/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 591/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 592/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 593/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 594/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 595/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2511\n",
      "Epoch 596/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2511\n",
      "Epoch 597/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 598/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 599/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 600/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 601/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 602/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 603/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 604/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 605/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 606/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 607/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 608/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 609/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 610/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 611/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 612/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 613/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2510\n",
      "Epoch 614/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 615/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 616/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 617/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 618/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 619/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 620/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 621/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 622/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 623/2000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2510\n",
      "Epoch 624/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 625/2000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2510\n",
      "Epoch 626/2000\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2510\n",
      "Epoch 627/2000\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2510\n",
      "Epoch 628/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2510\n",
      "Epoch 629/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 630/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2510\n",
      "Epoch 631/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 632/2000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2510\n",
      "Epoch 633/2000\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2510\n",
      "Epoch 634/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 635/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 636/2000\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.2510\n",
      "Epoch 637/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 638/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 639/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 640/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 641/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 642/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 643/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 644/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 645/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 646/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 647/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 648/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 649/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 650/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 651/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 652/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 653/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 654/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2510\n",
      "Epoch 655/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 656/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 657/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 658/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2510\n",
      "Epoch 659/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2510\n",
      "Epoch 660/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 661/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 662/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 663/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 664/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 665/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 666/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 667/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 668/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 669/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 670/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2510\n",
      "Epoch 671/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 672/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 673/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 674/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 675/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 676/2000\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2510\n",
      "Epoch 677/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 678/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 679/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 680/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 681/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 682/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2510\n",
      "Epoch 683/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 684/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 685/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 686/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 687/2000\n",
      "4/4 [==============================] - 0s 115ms/step - loss: 0.2510\n",
      "Epoch 688/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2510\n",
      "Epoch 689/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 690/2000\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.2510\n",
      "Epoch 691/2000\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.2510\n",
      "Epoch 692/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 693/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 694/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 695/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 696/2000\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2510\n",
      "Epoch 697/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 698/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 699/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 700/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 701/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 702/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 703/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 704/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 705/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 706/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 707/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 708/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 709/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 710/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 711/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 712/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 713/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 714/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 715/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 716/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 717/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 718/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 719/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 720/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 721/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 722/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 723/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 724/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 725/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 726/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 727/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 728/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2510\n",
      "Epoch 729/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2510\n",
      "Epoch 730/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 731/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 732/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 733/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 734/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 735/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 736/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 737/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 738/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 739/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 740/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 741/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 742/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 743/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 744/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 745/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 746/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 747/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 748/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 749/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 750/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 751/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 752/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 753/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 754/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 755/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 756/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 757/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 758/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 759/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 760/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2509\n",
      "Epoch 761/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 762/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 763/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 764/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 765/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 766/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 767/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 768/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 769/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 770/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 771/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 772/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 773/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 774/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 775/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 776/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 777/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 778/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 779/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 780/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 781/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 782/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 783/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 784/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 785/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 786/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 787/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 788/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 789/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 790/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 791/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 792/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 793/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 794/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 795/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 796/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 797/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 798/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 799/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 800/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 801/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 802/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 803/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 804/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 805/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 806/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 807/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 808/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 809/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 810/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 811/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 812/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 813/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 814/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 815/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 816/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 817/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 818/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 819/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 820/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 821/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 822/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 823/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 824/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 825/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 826/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 827/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 828/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 829/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 830/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 831/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2509\n",
      "Epoch 832/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 833/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 834/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 835/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 836/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 837/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 838/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 839/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 840/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 841/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 842/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 843/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 844/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 845/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 846/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 847/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2509\n",
      "Epoch 848/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 849/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 850/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 851/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 852/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 853/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 854/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 855/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 856/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 857/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 858/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 859/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 860/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 861/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 862/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 863/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 864/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2509\n",
      "Epoch 865/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 866/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 867/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509\n",
      "Epoch 868/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 869/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 870/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 871/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 872/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 873/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 874/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 875/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 876/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 877/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 878/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 879/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 880/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 881/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 882/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 883/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 884/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 885/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 886/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 887/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 888/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 889/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 890/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 891/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 892/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 893/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 894/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 895/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 896/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 897/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 898/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 899/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 900/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 901/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 902/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 903/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 904/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 905/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 906/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 907/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 908/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 909/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 910/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 911/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 912/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 913/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 914/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 915/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 916/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 917/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 918/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 919/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 920/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 921/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 922/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 923/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 924/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 925/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 926/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 927/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 928/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 929/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 930/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 931/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 932/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 933/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 934/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 935/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 936/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 937/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 938/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 939/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 940/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 941/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 942/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 943/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 944/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 945/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 946/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 947/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 948/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 949/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 950/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 951/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 952/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 953/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 954/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 955/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 956/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 957/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 958/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 959/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 960/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 961/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 962/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 963/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 964/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 965/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 966/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 967/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 968/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 969/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 970/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 971/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 972/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2508\n",
      "Epoch 973/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 974/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 975/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 976/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 977/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 978/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 979/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 980/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 981/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 982/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 983/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 984/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 985/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 986/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 987/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 988/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 989/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 990/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 991/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 992/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 993/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 994/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 995/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 996/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 997/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 998/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 999/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 1000/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 1001/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 1002/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 1003/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 1004/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
      "Epoch 1005/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2508\n",
      "Epoch 1006/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1007/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1008/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1009/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1010/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1011/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1012/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1013/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1014/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1015/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1016/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1017/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1018/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1019/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1020/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1021/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1022/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1023/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1024/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2507\n",
      "Epoch 1025/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1026/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1027/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1028/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1029/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1030/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1031/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1032/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1033/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1034/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1035/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1036/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1037/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1038/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1039/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2507\n",
      "Epoch 1040/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1041/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1042/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1043/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1044/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1045/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1046/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1047/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1048/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1049/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1050/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1051/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1052/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1053/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1054/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1055/2000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2507\n",
      "Epoch 1056/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1057/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1058/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1059/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1060/2000\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.2507\n",
      "Epoch 1061/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1062/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1063/2000\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2507\n",
      "Epoch 1064/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1065/2000\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.2507\n",
      "Epoch 1066/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1067/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1068/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1069/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1070/2000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2507\n",
      "Epoch 1071/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1072/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1073/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1074/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1075/2000\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2507\n",
      "Epoch 1076/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1077/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1078/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1079/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1080/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1081/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1082/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1083/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1084/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1085/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1086/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1087/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1088/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1089/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1090/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1091/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1092/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1093/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1094/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1095/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1096/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1097/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1098/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1099/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1100/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1101/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1102/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1103/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1104/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1105/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1106/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1107/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1108/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1109/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1110/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1111/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1112/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1113/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1114/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1115/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1116/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1117/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1118/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1119/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1120/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1121/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1122/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1123/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1124/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1125/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1126/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1127/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1128/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1129/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1130/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1131/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1132/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1133/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1134/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1135/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1136/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1137/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1138/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1139/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1140/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2507\n",
      "Epoch 1141/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1142/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1143/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2507\n",
      "Epoch 1144/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1145/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1146/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1147/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1148/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1149/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1150/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1151/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1152/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1153/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1154/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1155/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1156/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1157/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1158/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1159/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1160/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1161/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1162/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1163/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1164/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1165/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1166/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1167/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1168/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1169/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1170/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1171/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1172/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1173/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1174/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1175/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1176/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1177/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1178/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1179/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1180/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1181/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1182/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1183/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1184/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1185/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1186/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1187/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1188/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1189/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1190/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1191/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1192/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1193/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1194/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1195/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1196/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1197/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1198/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1199/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1200/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1201/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1202/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1203/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1204/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1205/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1206/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1207/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1208/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1209/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1210/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1211/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1212/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1213/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1214/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1215/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1216/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1217/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1218/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1219/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1220/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1221/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1222/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1223/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1224/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1225/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1226/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1227/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1228/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1229/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1230/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1231/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1232/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1233/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1234/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1235/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1236/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1237/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1238/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1239/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1240/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1241/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1242/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1243/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1244/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1245/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1246/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1247/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1248/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1249/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1250/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1251/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1252/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1253/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1254/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1255/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1256/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1257/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1258/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1259/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1260/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1261/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1262/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1263/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1264/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1265/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1266/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1267/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1268/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1269/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1270/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1271/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1272/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1273/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1274/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1275/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2506\n",
      "Epoch 1276/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1277/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1278/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2506\n",
      "Epoch 1279/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1280/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1281/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1282/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1283/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1284/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1285/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1286/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1287/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1288/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1289/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1290/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1291/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1292/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1293/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1294/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1295/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1296/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1297/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1298/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1299/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1300/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1301/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1302/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1303/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1304/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1305/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1306/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1307/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1308/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1309/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1310/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1311/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1312/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1313/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1314/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1315/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1316/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1317/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2505\n",
      "Epoch 1318/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1319/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1320/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1321/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1322/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1323/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1324/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1325/2000\n",
      "4/4 [==============================] - 0s 0s/step - loss: 0.2505\n",
      "Epoch 1326/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1327/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1328/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1329/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1330/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1331/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1332/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1333/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1334/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1335/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1336/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1337/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1338/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2505\n",
      "Epoch 1339/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1340/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1341/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1342/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1343/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1344/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1345/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1346/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1347/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1348/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1349/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1350/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1351/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1352/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1353/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1354/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1355/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1356/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1357/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1358/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1359/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1360/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1361/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1362/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1363/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1364/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1365/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1366/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1367/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1368/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1369/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1370/2000\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.2505\n",
      "Epoch 1371/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1372/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1373/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1374/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1375/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1376/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1377/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1378/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1379/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1380/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1381/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1382/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1383/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1384/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1385/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1386/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1387/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1388/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1389/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1390/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1391/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1392/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1393/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1394/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1395/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1396/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1397/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1398/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1399/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1400/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1401/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1402/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1403/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1404/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1405/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1406/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1407/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1408/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1409/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2505\n",
      "Epoch 1410/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2505\n",
      "Epoch 1411/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1412/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1413/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1414/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1415/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1416/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1417/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1418/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1419/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1420/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1421/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1422/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1423/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1424/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1425/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1426/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1427/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1428/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1429/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1430/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1431/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1432/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1433/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1434/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1435/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1436/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1437/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1438/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1439/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1440/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1441/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1442/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1443/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1444/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1445/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1446/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1447/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1448/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1449/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1450/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1451/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1452/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1453/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1454/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1455/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1456/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1457/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1458/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1459/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1460/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1461/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1462/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1463/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1464/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1465/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1466/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1467/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1468/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1469/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1470/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1471/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1472/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1473/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1474/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1475/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1476/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1477/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1478/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1479/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1480/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1481/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1482/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1483/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1484/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1485/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1486/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1487/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1488/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1489/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1490/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1491/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1492/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1493/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1494/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1495/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1496/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1497/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1498/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1499/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1500/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1501/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1502/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1503/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1504/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1505/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1506/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1507/2000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2504\n",
      "Epoch 1508/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1509/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1510/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1511/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1512/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1513/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1514/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1515/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1516/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1517/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1518/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1519/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1520/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1521/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1522/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1523/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1524/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1525/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1526/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1527/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1528/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1529/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1530/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1531/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1532/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1533/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1534/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1535/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2504\n",
      "Epoch 1536/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2504\n",
      "Epoch 1537/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2504\n",
      "Epoch 1538/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1539/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1540/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1541/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1542/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1543/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1544/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1545/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1546/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1547/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1548/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1549/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1550/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1551/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1552/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1553/2000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2503\n",
      "Epoch 1554/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1555/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1556/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1557/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1558/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1559/2000\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.2503\n",
      "Epoch 1560/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1561/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1562/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1563/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2503\n",
      "Epoch 1564/2000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2503\n",
      "Epoch 1565/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1566/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1567/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1568/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1569/2000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2503\n",
      "Epoch 1570/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1571/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1572/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1573/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1574/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1575/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1576/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1577/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1578/2000\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.2503\n",
      "Epoch 1579/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1580/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1581/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1582/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1583/2000\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2503\n",
      "Epoch 1584/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1585/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1586/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1587/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1588/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1589/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1590/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1591/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1592/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1593/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1594/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1595/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1596/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1597/2000\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2503\n",
      "Epoch 1598/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1599/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1600/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1601/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1602/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1603/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1604/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1605/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1606/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1607/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1608/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1609/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1610/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1611/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1612/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1613/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1614/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1615/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1616/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1617/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1618/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1619/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1620/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1621/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1622/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1623/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1624/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1625/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1626/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1627/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1628/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1629/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1630/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1631/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1632/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1633/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1634/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1635/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1636/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1637/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1638/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1639/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1640/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1641/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1642/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1643/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1644/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1645/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1646/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1647/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1648/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1649/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1650/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1651/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1652/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1653/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1654/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1655/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1656/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1657/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1658/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1659/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2503\n",
      "Epoch 1660/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1661/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 1662/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1663/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1664/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1665/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1666/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1667/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1668/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1669/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1670/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1671/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1672/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2502\n",
      "Epoch 1673/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1674/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1675/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1676/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1677/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1678/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1679/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1680/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1681/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1682/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1683/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1684/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1685/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1686/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1687/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1688/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1689/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1690/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1691/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1692/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1693/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1694/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1695/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1696/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1697/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1698/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1699/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1700/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1701/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1702/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1703/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1704/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1705/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1706/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1707/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1708/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1709/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1710/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1711/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1712/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1713/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1714/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1715/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1716/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1717/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1718/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1719/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1720/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1721/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1722/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1723/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1724/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1725/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1726/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1727/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1728/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1729/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1730/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1731/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1732/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1733/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1734/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1735/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1736/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1737/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1738/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1739/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1740/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1741/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1742/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1743/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1744/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1745/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1746/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1747/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1748/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1749/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1750/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1751/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1752/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1753/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1754/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1755/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1756/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1757/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1758/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1759/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1760/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1761/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1762/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1763/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1764/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1765/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1766/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1767/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1768/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1769/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1770/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1771/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1772/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1773/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1774/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1775/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1776/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1777/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2502\n",
      "Epoch 1778/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2502\n",
      "Epoch 1779/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1780/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1781/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1782/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1783/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1784/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1785/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1786/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1787/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1788/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1789/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1790/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1791/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1792/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1793/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1794/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1795/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1796/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1797/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1798/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1799/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1800/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1801/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1802/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1803/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1804/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1805/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1806/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1807/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1808/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1809/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1810/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1811/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1812/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1813/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1814/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1815/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1816/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1817/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1818/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1819/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1820/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1821/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1822/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1823/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1824/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1825/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1826/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1827/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1828/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1829/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1830/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1831/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1832/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1833/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1834/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1835/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1836/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1837/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1838/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1839/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1840/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1841/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1842/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1843/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1844/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1845/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1846/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1847/2000\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2501\n",
      "Epoch 1848/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1849/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1850/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1851/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1852/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1853/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1854/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1855/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1856/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1857/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1858/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1859/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1860/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1861/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1862/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1863/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1864/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1865/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1866/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1867/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1868/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1869/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1870/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1871/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1872/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1873/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1874/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1875/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1876/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1877/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1878/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1879/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1880/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1881/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1882/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1883/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1884/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1885/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1886/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1887/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1888/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1889/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1890/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1891/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1892/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 1893/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2501\n",
      "Epoch 1894/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1895/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1896/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1897/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1898/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1899/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1900/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1901/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1902/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1903/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1904/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1905/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1906/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1907/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1908/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1909/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1910/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1911/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1912/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1913/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1914/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1915/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1916/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1917/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1918/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1919/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1920/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1921/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1922/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1923/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1924/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1925/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1926/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1927/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1928/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1929/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1930/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1931/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1932/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1933/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1934/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1935/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1936/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1937/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1938/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1939/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1940/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1941/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1942/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1943/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1944/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1945/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1946/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1947/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1948/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1949/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1950/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1951/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1952/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1953/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1954/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1955/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1956/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1957/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1958/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1959/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1960/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1961/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1962/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1963/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1964/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1965/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1966/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1967/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1968/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1969/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1970/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1971/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1972/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1973/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1974/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1975/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1976/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1977/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1978/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1979/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1980/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1981/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1982/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1983/2000\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2500\n",
      "Epoch 1984/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1985/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1986/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1987/2000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2500\n",
      "Epoch 1988/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1989/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1990/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1991/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1992/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1993/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1994/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1995/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1996/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1997/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1998/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 1999/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "Epoch 2000/2000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2500\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 3.27 tf.keras 를 이용한 XOR 네트워크 계산\n",
    "import numpy as np\n",
    "x = np.array([[1,1], [1,0], [0,1], [0,0]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=2, activation='sigmoid', input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.3), loss='mse')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# 3.28 tf.keras 를 이용한 XOR 네트워크 학습\n",
    "history = model.fit(x, y, epochs=2000, batch_size=1)\n",
    "\n",
    "# 3.29 tf.keras 를 이용한 XOR 네트워크 평가\n",
    "model.predict(x)\n",
    "\n",
    "# 3.30 XOR 네트워크의 가중치와 편향 확인\n",
    "for weight in model.weights:\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Z0s8wTI3HkzF",
   "metadata": {
    "id": "Z0s8wTI3HkzF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 - 1s - loss: 1.0391 - mse: 1.0391 - 523ms/epoch - 523ms/step\n",
      "Epoch 2/500\n",
      "1/1 - 0s - loss: 0.7717 - mse: 0.7717 - 10ms/epoch - 10ms/step\n",
      "Epoch 3/500\n",
      "1/1 - 0s - loss: 0.6532 - mse: 0.6532 - 24ms/epoch - 24ms/step\n",
      "Epoch 4/500\n",
      "1/1 - 0s - loss: 0.5962 - mse: 0.5962 - 51ms/epoch - 51ms/step\n",
      "Epoch 5/500\n",
      "1/1 - 0s - loss: 0.5623 - mse: 0.5623 - 28ms/epoch - 28ms/step\n",
      "Epoch 6/500\n",
      "1/1 - 0s - loss: 0.5384 - mse: 0.5384 - 11ms/epoch - 11ms/step\n",
      "Epoch 7/500\n",
      "1/1 - 0s - loss: 0.5192 - mse: 0.5192 - 16ms/epoch - 16ms/step\n",
      "Epoch 8/500\n",
      "1/1 - 0s - loss: 0.5027 - mse: 0.5027 - 0s/epoch - 0s/step\n",
      "Epoch 9/500\n",
      "1/1 - 0s - loss: 0.4877 - mse: 0.4877 - 16ms/epoch - 16ms/step\n",
      "Epoch 10/500\n",
      "1/1 - 0s - loss: 0.4737 - mse: 0.4737 - 0s/epoch - 0s/step\n",
      "Epoch 11/500\n",
      "1/1 - 0s - loss: 0.4604 - mse: 0.4604 - 10ms/epoch - 10ms/step\n",
      "Epoch 12/500\n",
      "1/1 - 0s - loss: 0.4478 - mse: 0.4478 - 27ms/epoch - 27ms/step\n",
      "Epoch 13/500\n",
      "1/1 - 0s - loss: 0.4356 - mse: 0.4356 - 26ms/epoch - 26ms/step\n",
      "Epoch 14/500\n",
      "1/1 - 0s - loss: 0.4238 - mse: 0.4238 - 7ms/epoch - 7ms/step\n",
      "Epoch 15/500\n",
      "1/1 - 0s - loss: 0.4125 - mse: 0.4125 - 0s/epoch - 0s/step\n",
      "Epoch 16/500\n",
      "1/1 - 0s - loss: 0.4016 - mse: 0.4016 - 16ms/epoch - 16ms/step\n",
      "Epoch 17/500\n",
      "1/1 - 0s - loss: 0.3910 - mse: 0.3910 - 0s/epoch - 0s/step\n",
      "Epoch 18/500\n",
      "1/1 - 0s - loss: 0.3808 - mse: 0.3808 - 0s/epoch - 0s/step\n",
      "Epoch 19/500\n",
      "1/1 - 0s - loss: 0.3710 - mse: 0.3710 - 0s/epoch - 0s/step\n",
      "Epoch 20/500\n",
      "1/1 - 0s - loss: 0.3615 - mse: 0.3615 - 25ms/epoch - 25ms/step\n",
      "Epoch 21/500\n",
      "1/1 - 0s - loss: 0.3523 - mse: 0.3523 - 2ms/epoch - 2ms/step\n",
      "Epoch 22/500\n",
      "1/1 - 0s - loss: 0.3435 - mse: 0.3435 - 0s/epoch - 0s/step\n",
      "Epoch 23/500\n",
      "1/1 - 0s - loss: 0.3350 - mse: 0.3350 - 0s/epoch - 0s/step\n",
      "Epoch 24/500\n",
      "1/1 - 0s - loss: 0.3268 - mse: 0.3268 - 0s/epoch - 0s/step\n",
      "Epoch 25/500\n",
      "1/1 - 0s - loss: 0.3189 - mse: 0.3189 - 14ms/epoch - 14ms/step\n",
      "Epoch 26/500\n",
      "1/1 - 0s - loss: 0.3113 - mse: 0.3113 - 30ms/epoch - 30ms/step\n",
      "Epoch 27/500\n",
      "1/1 - 0s - loss: 0.3039 - mse: 0.3039 - 1ms/epoch - 1ms/step\n",
      "Epoch 28/500\n",
      "1/1 - 0s - loss: 0.2968 - mse: 0.2968 - 8ms/epoch - 8ms/step\n",
      "Epoch 29/500\n",
      "1/1 - 0s - loss: 0.2900 - mse: 0.2900 - 27ms/epoch - 27ms/step\n",
      "Epoch 30/500\n",
      "1/1 - 0s - loss: 0.2834 - mse: 0.2834 - 9ms/epoch - 9ms/step\n",
      "Epoch 31/500\n",
      "1/1 - 0s - loss: 0.2770 - mse: 0.2770 - 0s/epoch - 0s/step\n",
      "Epoch 32/500\n",
      "1/1 - 0s - loss: 0.2709 - mse: 0.2709 - 16ms/epoch - 16ms/step\n",
      "Epoch 33/500\n",
      "1/1 - 0s - loss: 0.2649 - mse: 0.2649 - 7ms/epoch - 7ms/step\n",
      "Epoch 34/500\n",
      "1/1 - 0s - loss: 0.2592 - mse: 0.2592 - 6ms/epoch - 6ms/step\n",
      "Epoch 35/500\n",
      "1/1 - 0s - loss: 0.2537 - mse: 0.2537 - 7ms/epoch - 7ms/step\n",
      "Epoch 36/500\n",
      "1/1 - 0s - loss: 0.2483 - mse: 0.2483 - 7ms/epoch - 7ms/step\n",
      "Epoch 37/500\n",
      "1/1 - 0s - loss: 0.2431 - mse: 0.2431 - 6ms/epoch - 6ms/step\n",
      "Epoch 38/500\n",
      "1/1 - 0s - loss: 0.2381 - mse: 0.2381 - 11ms/epoch - 11ms/step\n",
      "Epoch 39/500\n",
      "1/1 - 0s - loss: 0.2333 - mse: 0.2333 - 20ms/epoch - 20ms/step\n",
      "Epoch 40/500\n",
      "1/1 - 0s - loss: 0.2286 - mse: 0.2286 - 7ms/epoch - 7ms/step\n",
      "Epoch 41/500\n",
      "1/1 - 0s - loss: 0.2241 - mse: 0.2241 - 0s/epoch - 0s/step\n",
      "Epoch 42/500\n",
      "1/1 - 0s - loss: 0.2197 - mse: 0.2197 - 14ms/epoch - 14ms/step\n",
      "Epoch 43/500\n",
      "1/1 - 0s - loss: 0.2154 - mse: 0.2154 - 15ms/epoch - 15ms/step\n",
      "Epoch 44/500\n",
      "1/1 - 0s - loss: 0.2113 - mse: 0.2113 - 15ms/epoch - 15ms/step\n",
      "Epoch 45/500\n",
      "1/1 - 0s - loss: 0.2073 - mse: 0.2073 - 15ms/epoch - 15ms/step\n",
      "Epoch 46/500\n",
      "1/1 - 0s - loss: 0.2034 - mse: 0.2034 - 6ms/epoch - 6ms/step\n",
      "Epoch 47/500\n",
      "1/1 - 0s - loss: 0.1997 - mse: 0.1997 - 6ms/epoch - 6ms/step\n",
      "Epoch 48/500\n",
      "1/1 - 0s - loss: 0.1960 - mse: 0.1960 - 25ms/epoch - 25ms/step\n",
      "Epoch 49/500\n",
      "1/1 - 0s - loss: 0.1925 - mse: 0.1925 - 27ms/epoch - 27ms/step\n",
      "Epoch 50/500\n",
      "1/1 - 0s - loss: 0.1891 - mse: 0.1891 - 7ms/epoch - 7ms/step\n",
      "Epoch 51/500\n",
      "1/1 - 0s - loss: 0.1857 - mse: 0.1857 - 6ms/epoch - 6ms/step\n",
      "Epoch 52/500\n",
      "1/1 - 0s - loss: 0.1825 - mse: 0.1825 - 6ms/epoch - 6ms/step\n",
      "Epoch 53/500\n",
      "1/1 - 0s - loss: 0.1794 - mse: 0.1794 - 6ms/epoch - 6ms/step\n",
      "Epoch 54/500\n",
      "1/1 - 0s - loss: 0.1763 - mse: 0.1763 - 7ms/epoch - 7ms/step\n",
      "Epoch 55/500\n",
      "1/1 - 0s - loss: 0.1733 - mse: 0.1733 - 5ms/epoch - 5ms/step\n",
      "Epoch 56/500\n",
      "1/1 - 0s - loss: 0.1705 - mse: 0.1705 - 8ms/epoch - 8ms/step\n",
      "Epoch 57/500\n",
      "1/1 - 0s - loss: 0.1676 - mse: 0.1676 - 16ms/epoch - 16ms/step\n",
      "Epoch 58/500\n",
      "1/1 - 0s - loss: 0.1649 - mse: 0.1649 - 0s/epoch - 0s/step\n",
      "Epoch 59/500\n",
      "1/1 - 0s - loss: 0.1623 - mse: 0.1623 - 16ms/epoch - 16ms/step\n",
      "Epoch 60/500\n",
      "1/1 - 0s - loss: 0.1597 - mse: 0.1597 - 16ms/epoch - 16ms/step\n",
      "Epoch 61/500\n",
      "1/1 - 0s - loss: 0.1572 - mse: 0.1572 - 16ms/epoch - 16ms/step\n",
      "Epoch 62/500\n",
      "1/1 - 0s - loss: 0.1547 - mse: 0.1547 - 7ms/epoch - 7ms/step\n",
      "Epoch 63/500\n",
      "1/1 - 0s - loss: 0.1523 - mse: 0.1523 - 7ms/epoch - 7ms/step\n",
      "Epoch 64/500\n",
      "1/1 - 0s - loss: 0.1500 - mse: 0.1500 - 6ms/epoch - 6ms/step\n",
      "Epoch 65/500\n",
      "1/1 - 0s - loss: 0.1477 - mse: 0.1477 - 19ms/epoch - 19ms/step\n",
      "Epoch 66/500\n",
      "1/1 - 0s - loss: 0.1455 - mse: 0.1455 - 14ms/epoch - 14ms/step\n",
      "Epoch 67/500\n",
      "1/1 - 0s - loss: 0.1433 - mse: 0.1433 - 8ms/epoch - 8ms/step\n",
      "Epoch 68/500\n",
      "1/1 - 0s - loss: 0.1412 - mse: 0.1412 - 7ms/epoch - 7ms/step\n",
      "Epoch 69/500\n",
      "1/1 - 0s - loss: 0.1392 - mse: 0.1392 - 16ms/epoch - 16ms/step\n",
      "Epoch 70/500\n",
      "1/1 - 0s - loss: 0.1372 - mse: 0.1372 - 15ms/epoch - 15ms/step\n",
      "Epoch 71/500\n",
      "1/1 - 0s - loss: 0.1352 - mse: 0.1352 - 16ms/epoch - 16ms/step\n",
      "Epoch 72/500\n",
      "1/1 - 0s - loss: 0.1333 - mse: 0.1333 - 8ms/epoch - 8ms/step\n",
      "Epoch 73/500\n",
      "1/1 - 0s - loss: 0.1315 - mse: 0.1315 - 169ms/epoch - 169ms/step\n",
      "Epoch 74/500\n",
      "1/1 - 0s - loss: 0.1297 - mse: 0.1297 - 8ms/epoch - 8ms/step\n",
      "Epoch 75/500\n",
      "1/1 - 0s - loss: 0.1279 - mse: 0.1279 - 8ms/epoch - 8ms/step\n",
      "Epoch 76/500\n",
      "1/1 - 0s - loss: 0.1262 - mse: 0.1262 - 8ms/epoch - 8ms/step\n",
      "Epoch 77/500\n",
      "1/1 - 0s - loss: 0.1245 - mse: 0.1245 - 6ms/epoch - 6ms/step\n",
      "Epoch 78/500\n",
      "1/1 - 0s - loss: 0.1228 - mse: 0.1228 - 19ms/epoch - 19ms/step\n",
      "Epoch 79/500\n",
      "1/1 - 0s - loss: 0.1212 - mse: 0.1212 - 8ms/epoch - 8ms/step\n",
      "Epoch 80/500\n",
      "1/1 - 0s - loss: 0.1196 - mse: 0.1196 - 17ms/epoch - 17ms/step\n",
      "Epoch 81/500\n",
      "1/1 - 0s - loss: 0.1181 - mse: 0.1181 - 22ms/epoch - 22ms/step\n",
      "Epoch 82/500\n",
      "1/1 - 0s - loss: 0.1166 - mse: 0.1166 - 6ms/epoch - 6ms/step\n",
      "Epoch 83/500\n",
      "1/1 - 0s - loss: 0.1151 - mse: 0.1151 - 26ms/epoch - 26ms/step\n",
      "Epoch 84/500\n",
      "1/1 - 0s - loss: 0.1136 - mse: 0.1136 - 0s/epoch - 0s/step\n",
      "Epoch 85/500\n",
      "1/1 - 0s - loss: 0.1122 - mse: 0.1122 - 45ms/epoch - 45ms/step\n",
      "Epoch 86/500\n",
      "1/1 - 0s - loss: 0.1109 - mse: 0.1109 - 0s/epoch - 0s/step\n",
      "Epoch 87/500\n",
      "1/1 - 0s - loss: 0.1095 - mse: 0.1095 - 8ms/epoch - 8ms/step\n",
      "Epoch 88/500\n",
      "1/1 - 0s - loss: 0.1082 - mse: 0.1082 - 18ms/epoch - 18ms/step\n",
      "Epoch 89/500\n",
      "1/1 - 0s - loss: 0.1069 - mse: 0.1069 - 16ms/epoch - 16ms/step\n",
      "Epoch 90/500\n",
      "1/1 - 0s - loss: 0.1056 - mse: 0.1056 - 30ms/epoch - 30ms/step\n",
      "Epoch 91/500\n",
      "1/1 - 0s - loss: 0.1044 - mse: 0.1044 - 6ms/epoch - 6ms/step\n",
      "Epoch 92/500\n",
      "1/1 - 0s - loss: 0.1031 - mse: 0.1031 - 8ms/epoch - 8ms/step\n",
      "Epoch 93/500\n",
      "1/1 - 0s - loss: 0.1019 - mse: 0.1019 - 8ms/epoch - 8ms/step\n",
      "Epoch 94/500\n",
      "1/1 - 0s - loss: 0.1008 - mse: 0.1008 - 12ms/epoch - 12ms/step\n",
      "Epoch 95/500\n",
      "1/1 - 0s - loss: 0.0996 - mse: 0.0996 - 13ms/epoch - 13ms/step\n",
      "Epoch 96/500\n",
      "1/1 - 0s - loss: 0.0985 - mse: 0.0985 - 0s/epoch - 0s/step\n",
      "Epoch 97/500\n",
      "1/1 - 0s - loss: 0.0974 - mse: 0.0974 - 24ms/epoch - 24ms/step\n",
      "Epoch 98/500\n",
      "1/1 - 0s - loss: 0.0963 - mse: 0.0963 - 0s/epoch - 0s/step\n",
      "Epoch 99/500\n",
      "1/1 - 0s - loss: 0.0953 - mse: 0.0953 - 24ms/epoch - 24ms/step\n",
      "Epoch 100/500\n",
      "1/1 - 0s - loss: 0.0942 - mse: 0.0942 - 6ms/epoch - 6ms/step\n",
      "Epoch 101/500\n",
      "1/1 - 0s - loss: 0.0932 - mse: 0.0932 - 6ms/epoch - 6ms/step\n",
      "Epoch 102/500\n",
      "1/1 - 0s - loss: 0.0922 - mse: 0.0922 - 6ms/epoch - 6ms/step\n",
      "Epoch 103/500\n",
      "1/1 - 0s - loss: 0.0912 - mse: 0.0912 - 6ms/epoch - 6ms/step\n",
      "Epoch 104/500\n",
      "1/1 - 0s - loss: 0.0903 - mse: 0.0903 - 7ms/epoch - 7ms/step\n",
      "Epoch 105/500\n",
      "1/1 - 0s - loss: 0.0893 - mse: 0.0893 - 7ms/epoch - 7ms/step\n",
      "Epoch 106/500\n",
      "1/1 - 0s - loss: 0.0884 - mse: 0.0884 - 6ms/epoch - 6ms/step\n",
      "Epoch 107/500\n",
      "1/1 - 0s - loss: 0.0875 - mse: 0.0875 - 7ms/epoch - 7ms/step\n",
      "Epoch 108/500\n",
      "1/1 - 0s - loss: 0.0866 - mse: 0.0866 - 8ms/epoch - 8ms/step\n",
      "Epoch 109/500\n",
      "1/1 - 0s - loss: 0.0857 - mse: 0.0857 - 13ms/epoch - 13ms/step\n",
      "Epoch 110/500\n",
      "1/1 - 0s - loss: 0.0848 - mse: 0.0848 - 14ms/epoch - 14ms/step\n",
      "Epoch 111/500\n",
      "1/1 - 0s - loss: 0.0840 - mse: 0.0840 - 6ms/epoch - 6ms/step\n",
      "Epoch 112/500\n",
      "1/1 - 0s - loss: 0.0832 - mse: 0.0832 - 5ms/epoch - 5ms/step\n",
      "Epoch 113/500\n",
      "1/1 - 0s - loss: 0.0823 - mse: 0.0823 - 19ms/epoch - 19ms/step\n",
      "Epoch 114/500\n",
      "1/1 - 0s - loss: 0.0815 - mse: 0.0815 - 7ms/epoch - 7ms/step\n",
      "Epoch 115/500\n",
      "1/1 - 0s - loss: 0.0807 - mse: 0.0807 - 6ms/epoch - 6ms/step\n",
      "Epoch 116/500\n",
      "1/1 - 0s - loss: 0.0800 - mse: 0.0800 - 7ms/epoch - 7ms/step\n",
      "Epoch 117/500\n",
      "1/1 - 0s - loss: 0.0792 - mse: 0.0792 - 12ms/epoch - 12ms/step\n",
      "Epoch 118/500\n",
      "1/1 - 0s - loss: 0.0784 - mse: 0.0784 - 15ms/epoch - 15ms/step\n",
      "Epoch 119/500\n",
      "1/1 - 0s - loss: 0.0777 - mse: 0.0777 - 14ms/epoch - 14ms/step\n",
      "Epoch 120/500\n",
      "1/1 - 0s - loss: 0.0770 - mse: 0.0770 - 7ms/epoch - 7ms/step\n",
      "Epoch 121/500\n",
      "1/1 - 0s - loss: 0.0763 - mse: 0.0763 - 5ms/epoch - 5ms/step\n",
      "Epoch 122/500\n",
      "1/1 - 0s - loss: 0.0756 - mse: 0.0756 - 6ms/epoch - 6ms/step\n",
      "Epoch 123/500\n",
      "1/1 - 0s - loss: 0.0749 - mse: 0.0749 - 6ms/epoch - 6ms/step\n",
      "Epoch 124/500\n",
      "1/1 - 0s - loss: 0.0742 - mse: 0.0742 - 14ms/epoch - 14ms/step\n",
      "Epoch 125/500\n",
      "1/1 - 0s - loss: 0.0735 - mse: 0.0735 - 10ms/epoch - 10ms/step\n",
      "Epoch 126/500\n",
      "1/1 - 0s - loss: 0.0729 - mse: 0.0729 - 8ms/epoch - 8ms/step\n",
      "Epoch 127/500\n",
      "1/1 - 0s - loss: 0.0722 - mse: 0.0722 - 5ms/epoch - 5ms/step\n",
      "Epoch 128/500\n",
      "1/1 - 0s - loss: 0.0716 - mse: 0.0716 - 6ms/epoch - 6ms/step\n",
      "Epoch 129/500\n",
      "1/1 - 0s - loss: 0.0710 - mse: 0.0710 - 5ms/epoch - 5ms/step\n",
      "Epoch 130/500\n",
      "1/1 - 0s - loss: 0.0703 - mse: 0.0703 - 7ms/epoch - 7ms/step\n",
      "Epoch 131/500\n",
      "1/1 - 0s - loss: 0.0697 - mse: 0.0697 - 6ms/epoch - 6ms/step\n",
      "Epoch 132/500\n",
      "1/1 - 0s - loss: 0.0691 - mse: 0.0691 - 33ms/epoch - 33ms/step\n",
      "Epoch 133/500\n",
      "1/1 - 0s - loss: 0.0686 - mse: 0.0686 - 13ms/epoch - 13ms/step\n",
      "Epoch 134/500\n",
      "1/1 - 0s - loss: 0.0680 - mse: 0.0680 - 5ms/epoch - 5ms/step\n",
      "Epoch 135/500\n",
      "1/1 - 0s - loss: 0.0674 - mse: 0.0674 - 6ms/epoch - 6ms/step\n",
      "Epoch 136/500\n",
      "1/1 - 0s - loss: 0.0668 - mse: 0.0668 - 10ms/epoch - 10ms/step\n",
      "Epoch 137/500\n",
      "1/1 - 0s - loss: 0.0663 - mse: 0.0663 - 13ms/epoch - 13ms/step\n",
      "Epoch 138/500\n",
      "1/1 - 0s - loss: 0.0657 - mse: 0.0657 - 7ms/epoch - 7ms/step\n",
      "Epoch 139/500\n",
      "1/1 - 0s - loss: 0.0652 - mse: 0.0652 - 5ms/epoch - 5ms/step\n",
      "Epoch 140/500\n",
      "1/1 - 0s - loss: 0.0647 - mse: 0.0647 - 6ms/epoch - 6ms/step\n",
      "Epoch 141/500\n",
      "1/1 - 0s - loss: 0.0642 - mse: 0.0642 - 12ms/epoch - 12ms/step\n",
      "Epoch 142/500\n",
      "1/1 - 0s - loss: 0.0636 - mse: 0.0636 - 5ms/epoch - 5ms/step\n",
      "Epoch 143/500\n",
      "1/1 - 0s - loss: 0.0631 - mse: 0.0631 - 7ms/epoch - 7ms/step\n",
      "Epoch 144/500\n",
      "1/1 - 0s - loss: 0.0626 - mse: 0.0626 - 11ms/epoch - 11ms/step\n",
      "Epoch 145/500\n",
      "1/1 - 0s - loss: 0.0621 - mse: 0.0621 - 12ms/epoch - 12ms/step\n",
      "Epoch 146/500\n",
      "1/1 - 0s - loss: 0.0617 - mse: 0.0617 - 6ms/epoch - 6ms/step\n",
      "Epoch 147/500\n",
      "1/1 - 0s - loss: 0.0612 - mse: 0.0612 - 7ms/epoch - 7ms/step\n",
      "Epoch 148/500\n",
      "1/1 - 0s - loss: 0.0607 - mse: 0.0607 - 6ms/epoch - 6ms/step\n",
      "Epoch 149/500\n",
      "1/1 - 0s - loss: 0.0602 - mse: 0.0602 - 6ms/epoch - 6ms/step\n",
      "Epoch 150/500\n",
      "1/1 - 0s - loss: 0.0598 - mse: 0.0598 - 6ms/epoch - 6ms/step\n",
      "Epoch 151/500\n",
      "1/1 - 0s - loss: 0.0593 - mse: 0.0593 - 6ms/epoch - 6ms/step\n",
      "Epoch 152/500\n",
      "1/1 - 0s - loss: 0.0589 - mse: 0.0589 - 5ms/epoch - 5ms/step\n",
      "Epoch 153/500\n",
      "1/1 - 0s - loss: 0.0584 - mse: 0.0584 - 6ms/epoch - 6ms/step\n",
      "Epoch 154/500\n",
      "1/1 - 0s - loss: 0.0580 - mse: 0.0580 - 8ms/epoch - 8ms/step\n",
      "Epoch 155/500\n",
      "1/1 - 0s - loss: 0.0576 - mse: 0.0576 - 6ms/epoch - 6ms/step\n",
      "Epoch 156/500\n",
      "1/1 - 0s - loss: 0.0572 - mse: 0.0572 - 6ms/epoch - 6ms/step\n",
      "Epoch 157/500\n",
      "1/1 - 0s - loss: 0.0567 - mse: 0.0567 - 8ms/epoch - 8ms/step\n",
      "Epoch 158/500\n",
      "1/1 - 0s - loss: 0.0563 - mse: 0.0563 - 6ms/epoch - 6ms/step\n",
      "Epoch 159/500\n",
      "1/1 - 0s - loss: 0.0559 - mse: 0.0559 - 6ms/epoch - 6ms/step\n",
      "Epoch 160/500\n",
      "1/1 - 0s - loss: 0.0555 - mse: 0.0555 - 7ms/epoch - 7ms/step\n",
      "Epoch 161/500\n",
      "1/1 - 0s - loss: 0.0551 - mse: 0.0551 - 6ms/epoch - 6ms/step\n",
      "Epoch 162/500\n",
      "1/1 - 0s - loss: 0.0547 - mse: 0.0547 - 6ms/epoch - 6ms/step\n",
      "Epoch 163/500\n",
      "1/1 - 0s - loss: 0.0543 - mse: 0.0543 - 6ms/epoch - 6ms/step\n",
      "Epoch 164/500\n",
      "1/1 - 0s - loss: 0.0540 - mse: 0.0540 - 15ms/epoch - 15ms/step\n",
      "Epoch 165/500\n",
      "1/1 - 0s - loss: 0.0536 - mse: 0.0536 - 6ms/epoch - 6ms/step\n",
      "Epoch 166/500\n",
      "1/1 - 0s - loss: 0.0532 - mse: 0.0532 - 5ms/epoch - 5ms/step\n",
      "Epoch 167/500\n",
      "1/1 - 0s - loss: 0.0529 - mse: 0.0529 - 7ms/epoch - 7ms/step\n",
      "Epoch 168/500\n",
      "1/1 - 0s - loss: 0.0525 - mse: 0.0525 - 6ms/epoch - 6ms/step\n",
      "Epoch 169/500\n",
      "1/1 - 0s - loss: 0.0521 - mse: 0.0521 - 8ms/epoch - 8ms/step\n",
      "Epoch 170/500\n",
      "1/1 - 0s - loss: 0.0518 - mse: 0.0518 - 7ms/epoch - 7ms/step\n",
      "Epoch 171/500\n",
      "1/1 - 0s - loss: 0.0514 - mse: 0.0514 - 22ms/epoch - 22ms/step\n",
      "Epoch 172/500\n",
      "1/1 - 0s - loss: 0.0511 - mse: 0.0511 - 11ms/epoch - 11ms/step\n",
      "Epoch 173/500\n",
      "1/1 - 0s - loss: 0.0507 - mse: 0.0507 - 6ms/epoch - 6ms/step\n",
      "Epoch 174/500\n",
      "1/1 - 0s - loss: 0.0504 - mse: 0.0504 - 34ms/epoch - 34ms/step\n",
      "Epoch 175/500\n",
      "1/1 - 0s - loss: 0.0501 - mse: 0.0501 - 6ms/epoch - 6ms/step\n",
      "Epoch 176/500\n",
      "1/1 - 0s - loss: 0.0498 - mse: 0.0498 - 19ms/epoch - 19ms/step\n",
      "Epoch 177/500\n",
      "1/1 - 0s - loss: 0.0494 - mse: 0.0494 - 6ms/epoch - 6ms/step\n",
      "Epoch 178/500\n",
      "1/1 - 0s - loss: 0.0491 - mse: 0.0491 - 6ms/epoch - 6ms/step\n",
      "Epoch 179/500\n",
      "1/1 - 0s - loss: 0.0488 - mse: 0.0488 - 16ms/epoch - 16ms/step\n",
      "Epoch 180/500\n",
      "1/1 - 0s - loss: 0.0485 - mse: 0.0485 - 11ms/epoch - 11ms/step\n",
      "Epoch 181/500\n",
      "1/1 - 0s - loss: 0.0482 - mse: 0.0482 - 35ms/epoch - 35ms/step\n",
      "Epoch 182/500\n",
      "1/1 - 0s - loss: 0.0479 - mse: 0.0479 - 6ms/epoch - 6ms/step\n",
      "Epoch 183/500\n",
      "1/1 - 0s - loss: 0.0476 - mse: 0.0476 - 6ms/epoch - 6ms/step\n",
      "Epoch 184/500\n",
      "1/1 - 0s - loss: 0.0473 - mse: 0.0473 - 5ms/epoch - 5ms/step\n",
      "Epoch 185/500\n",
      "1/1 - 0s - loss: 0.0470 - mse: 0.0470 - 23ms/epoch - 23ms/step\n",
      "Epoch 186/500\n",
      "1/1 - 0s - loss: 0.0467 - mse: 0.0467 - 28ms/epoch - 28ms/step\n",
      "Epoch 187/500\n",
      "1/1 - 0s - loss: 0.0464 - mse: 0.0464 - 15ms/epoch - 15ms/step\n",
      "Epoch 188/500\n",
      "1/1 - 0s - loss: 0.0461 - mse: 0.0461 - 30ms/epoch - 30ms/step\n",
      "Epoch 189/500\n",
      "1/1 - 0s - loss: 0.0458 - mse: 0.0458 - 26ms/epoch - 26ms/step\n",
      "Epoch 190/500\n",
      "1/1 - 0s - loss: 0.0455 - mse: 0.0455 - 6ms/epoch - 6ms/step\n",
      "Epoch 191/500\n",
      "1/1 - 0s - loss: 0.0453 - mse: 0.0453 - 20ms/epoch - 20ms/step\n",
      "Epoch 192/500\n",
      "1/1 - 0s - loss: 0.0450 - mse: 0.0450 - 30ms/epoch - 30ms/step\n",
      "Epoch 193/500\n",
      "1/1 - 0s - loss: 0.0447 - mse: 0.0447 - 20ms/epoch - 20ms/step\n",
      "Epoch 194/500\n",
      "1/1 - 0s - loss: 0.0445 - mse: 0.0445 - 23ms/epoch - 23ms/step\n",
      "Epoch 195/500\n",
      "1/1 - 0s - loss: 0.0442 - mse: 0.0442 - 5ms/epoch - 5ms/step\n",
      "Epoch 196/500\n",
      "1/1 - 0s - loss: 0.0439 - mse: 0.0439 - 5ms/epoch - 5ms/step\n",
      "Epoch 197/500\n",
      "1/1 - 0s - loss: 0.0437 - mse: 0.0437 - 0s/epoch - 0s/step\n",
      "Epoch 198/500\n",
      "1/1 - 0s - loss: 0.0434 - mse: 0.0434 - 25ms/epoch - 25ms/step\n",
      "Epoch 199/500\n",
      "1/1 - 0s - loss: 0.0432 - mse: 0.0432 - 26ms/epoch - 26ms/step\n",
      "Epoch 200/500\n",
      "1/1 - 0s - loss: 0.0429 - mse: 0.0429 - 29ms/epoch - 29ms/step\n",
      "Epoch 201/500\n",
      "1/1 - 0s - loss: 0.0427 - mse: 0.0427 - 5ms/epoch - 5ms/step\n",
      "Epoch 202/500\n",
      "1/1 - 0s - loss: 0.0424 - mse: 0.0424 - 12ms/epoch - 12ms/step\n",
      "Epoch 203/500\n",
      "1/1 - 0s - loss: 0.0422 - mse: 0.0422 - 32ms/epoch - 32ms/step\n",
      "Epoch 204/500\n",
      "1/1 - 0s - loss: 0.0420 - mse: 0.0420 - 32ms/epoch - 32ms/step\n",
      "Epoch 205/500\n",
      "1/1 - 0s - loss: 0.0417 - mse: 0.0417 - 15ms/epoch - 15ms/step\n",
      "Epoch 206/500\n",
      "1/1 - 0s - loss: 0.0415 - mse: 0.0415 - 17ms/epoch - 17ms/step\n",
      "Epoch 207/500\n",
      "1/1 - 0s - loss: 0.0412 - mse: 0.0412 - 32ms/epoch - 32ms/step\n",
      "Epoch 208/500\n",
      "1/1 - 0s - loss: 0.0410 - mse: 0.0410 - 36ms/epoch - 36ms/step\n",
      "Epoch 209/500\n",
      "1/1 - 0s - loss: 0.0408 - mse: 0.0408 - 23ms/epoch - 23ms/step\n",
      "Epoch 210/500\n",
      "1/1 - 0s - loss: 0.0406 - mse: 0.0406 - 6ms/epoch - 6ms/step\n",
      "Epoch 211/500\n",
      "1/1 - 0s - loss: 0.0403 - mse: 0.0403 - 29ms/epoch - 29ms/step\n",
      "Epoch 212/500\n",
      "1/1 - 0s - loss: 0.0401 - mse: 0.0401 - 15ms/epoch - 15ms/step\n",
      "Epoch 213/500\n",
      "1/1 - 0s - loss: 0.0399 - mse: 0.0399 - 18ms/epoch - 18ms/step\n",
      "Epoch 214/500\n",
      "1/1 - 0s - loss: 0.0397 - mse: 0.0397 - 19ms/epoch - 19ms/step\n",
      "Epoch 215/500\n",
      "1/1 - 0s - loss: 0.0395 - mse: 0.0395 - 28ms/epoch - 28ms/step\n",
      "Epoch 216/500\n",
      "1/1 - 0s - loss: 0.0393 - mse: 0.0393 - 29ms/epoch - 29ms/step\n",
      "Epoch 217/500\n",
      "1/1 - 0s - loss: 0.0391 - mse: 0.0391 - 29ms/epoch - 29ms/step\n",
      "Epoch 218/500\n",
      "1/1 - 0s - loss: 0.0388 - mse: 0.0388 - 30ms/epoch - 30ms/step\n",
      "Epoch 219/500\n",
      "1/1 - 0s - loss: 0.0386 - mse: 0.0386 - 6ms/epoch - 6ms/step\n",
      "Epoch 220/500\n",
      "1/1 - 0s - loss: 0.0384 - mse: 0.0384 - 6ms/epoch - 6ms/step\n",
      "Epoch 221/500\n",
      "1/1 - 0s - loss: 0.0382 - mse: 0.0382 - 17ms/epoch - 17ms/step\n",
      "Epoch 222/500\n",
      "1/1 - 0s - loss: 0.0380 - mse: 0.0380 - 20ms/epoch - 20ms/step\n",
      "Epoch 223/500\n",
      "1/1 - 0s - loss: 0.0378 - mse: 0.0378 - 21ms/epoch - 21ms/step\n",
      "Epoch 224/500\n",
      "1/1 - 0s - loss: 0.0376 - mse: 0.0376 - 13ms/epoch - 13ms/step\n",
      "Epoch 225/500\n",
      "1/1 - 0s - loss: 0.0375 - mse: 0.0375 - 13ms/epoch - 13ms/step\n",
      "Epoch 226/500\n",
      "1/1 - 0s - loss: 0.0373 - mse: 0.0373 - 21ms/epoch - 21ms/step\n",
      "Epoch 227/500\n",
      "1/1 - 0s - loss: 0.0371 - mse: 0.0371 - 20ms/epoch - 20ms/step\n",
      "Epoch 228/500\n",
      "1/1 - 0s - loss: 0.0369 - mse: 0.0369 - 22ms/epoch - 22ms/step\n",
      "Epoch 229/500\n",
      "1/1 - 0s - loss: 0.0367 - mse: 0.0367 - 11ms/epoch - 11ms/step\n",
      "Epoch 230/500\n",
      "1/1 - 0s - loss: 0.0365 - mse: 0.0365 - 19ms/epoch - 19ms/step\n",
      "Epoch 231/500\n",
      "1/1 - 0s - loss: 0.0363 - mse: 0.0363 - 7ms/epoch - 7ms/step\n",
      "Epoch 232/500\n",
      "1/1 - 0s - loss: 0.0361 - mse: 0.0361 - 20ms/epoch - 20ms/step\n",
      "Epoch 233/500\n",
      "1/1 - 0s - loss: 0.0360 - mse: 0.0360 - 24ms/epoch - 24ms/step\n",
      "Epoch 234/500\n",
      "1/1 - 0s - loss: 0.0358 - mse: 0.0358 - 21ms/epoch - 21ms/step\n",
      "Epoch 235/500\n",
      "1/1 - 0s - loss: 0.0356 - mse: 0.0356 - 22ms/epoch - 22ms/step\n",
      "Epoch 236/500\n",
      "1/1 - 0s - loss: 0.0354 - mse: 0.0354 - 12ms/epoch - 12ms/step\n",
      "Epoch 237/500\n",
      "1/1 - 0s - loss: 0.0353 - mse: 0.0353 - 21ms/epoch - 21ms/step\n",
      "Epoch 238/500\n",
      "1/1 - 0s - loss: 0.0351 - mse: 0.0351 - 8ms/epoch - 8ms/step\n",
      "Epoch 239/500\n",
      "1/1 - 0s - loss: 0.0349 - mse: 0.0349 - 8ms/epoch - 8ms/step\n",
      "Epoch 240/500\n",
      "1/1 - 0s - loss: 0.0348 - mse: 0.0348 - 32ms/epoch - 32ms/step\n",
      "Epoch 241/500\n",
      "1/1 - 0s - loss: 0.0346 - mse: 0.0346 - 32ms/epoch - 32ms/step\n",
      "Epoch 242/500\n",
      "1/1 - 0s - loss: 0.0344 - mse: 0.0344 - 15ms/epoch - 15ms/step\n",
      "Epoch 243/500\n",
      "1/1 - 0s - loss: 0.0343 - mse: 0.0343 - 14ms/epoch - 14ms/step\n",
      "Epoch 244/500\n",
      "1/1 - 0s - loss: 0.0341 - mse: 0.0341 - 22ms/epoch - 22ms/step\n",
      "Epoch 245/500\n",
      "1/1 - 0s - loss: 0.0339 - mse: 0.0339 - 290ms/epoch - 290ms/step\n",
      "Epoch 246/500\n",
      "1/1 - 0s - loss: 0.0338 - mse: 0.0338 - 7ms/epoch - 7ms/step\n",
      "Epoch 247/500\n",
      "1/1 - 0s - loss: 0.0336 - mse: 0.0336 - 20ms/epoch - 20ms/step\n",
      "Epoch 248/500\n",
      "1/1 - 0s - loss: 0.0335 - mse: 0.0335 - 19ms/epoch - 19ms/step\n",
      "Epoch 249/500\n",
      "1/1 - 0s - loss: 0.0333 - mse: 0.0333 - 26ms/epoch - 26ms/step\n",
      "Epoch 250/500\n",
      "1/1 - 0s - loss: 0.0332 - mse: 0.0332 - 16ms/epoch - 16ms/step\n",
      "Epoch 251/500\n",
      "1/1 - 0s - loss: 0.0330 - mse: 0.0330 - 17ms/epoch - 17ms/step\n",
      "Epoch 252/500\n",
      "1/1 - 0s - loss: 0.0328 - mse: 0.0328 - 6ms/epoch - 6ms/step\n",
      "Epoch 253/500\n",
      "1/1 - 0s - loss: 0.0327 - mse: 0.0327 - 25ms/epoch - 25ms/step\n",
      "Epoch 254/500\n",
      "1/1 - 0s - loss: 0.0325 - mse: 0.0325 - 34ms/epoch - 34ms/step\n",
      "Epoch 255/500\n",
      "1/1 - 0s - loss: 0.0324 - mse: 0.0324 - 32ms/epoch - 32ms/step\n",
      "Epoch 256/500\n",
      "1/1 - 0s - loss: 0.0323 - mse: 0.0323 - 21ms/epoch - 21ms/step\n",
      "Epoch 257/500\n",
      "1/1 - 0s - loss: 0.0321 - mse: 0.0321 - 32ms/epoch - 32ms/step\n",
      "Epoch 258/500\n",
      "1/1 - 0s - loss: 0.0320 - mse: 0.0320 - 17ms/epoch - 17ms/step\n",
      "Epoch 259/500\n",
      "1/1 - 0s - loss: 0.0318 - mse: 0.0318 - 8ms/epoch - 8ms/step\n",
      "Epoch 260/500\n",
      "1/1 - 0s - loss: 0.0317 - mse: 0.0317 - 10ms/epoch - 10ms/step\n",
      "Epoch 261/500\n",
      "1/1 - 0s - loss: 0.0315 - mse: 0.0315 - 24ms/epoch - 24ms/step\n",
      "Epoch 262/500\n",
      "1/1 - 0s - loss: 0.0314 - mse: 0.0314 - 4ms/epoch - 4ms/step\n",
      "Epoch 263/500\n",
      "1/1 - 0s - loss: 0.0313 - mse: 0.0313 - 7ms/epoch - 7ms/step\n",
      "Epoch 264/500\n",
      "1/1 - 0s - loss: 0.0311 - mse: 0.0311 - 21ms/epoch - 21ms/step\n",
      "Epoch 265/500\n",
      "1/1 - 0s - loss: 0.0310 - mse: 0.0310 - 11ms/epoch - 11ms/step\n",
      "Epoch 266/500\n",
      "1/1 - 0s - loss: 0.0309 - mse: 0.0309 - 7ms/epoch - 7ms/step\n",
      "Epoch 267/500\n",
      "1/1 - 0s - loss: 0.0307 - mse: 0.0307 - 18ms/epoch - 18ms/step\n",
      "Epoch 268/500\n",
      "1/1 - 0s - loss: 0.0306 - mse: 0.0306 - 7ms/epoch - 7ms/step\n",
      "Epoch 269/500\n",
      "1/1 - 0s - loss: 0.0305 - mse: 0.0305 - 25ms/epoch - 25ms/step\n",
      "Epoch 270/500\n",
      "1/1 - 0s - loss: 0.0303 - mse: 0.0303 - 16ms/epoch - 16ms/step\n",
      "Epoch 271/500\n",
      "1/1 - 0s - loss: 0.0302 - mse: 0.0302 - 24ms/epoch - 24ms/step\n",
      "Epoch 272/500\n",
      "1/1 - 0s - loss: 0.0301 - mse: 0.0301 - 27ms/epoch - 27ms/step\n",
      "Epoch 273/500\n",
      "1/1 - 0s - loss: 0.0299 - mse: 0.0299 - 9ms/epoch - 9ms/step\n",
      "Epoch 274/500\n",
      "1/1 - 0s - loss: 0.0298 - mse: 0.0298 - 12ms/epoch - 12ms/step\n",
      "Epoch 275/500\n",
      "1/1 - 0s - loss: 0.0297 - mse: 0.0297 - 7ms/epoch - 7ms/step\n",
      "Epoch 276/500\n",
      "1/1 - 0s - loss: 0.0296 - mse: 0.0296 - 10ms/epoch - 10ms/step\n",
      "Epoch 277/500\n",
      "1/1 - 0s - loss: 0.0294 - mse: 0.0294 - 8ms/epoch - 8ms/step\n",
      "Epoch 278/500\n",
      "1/1 - 0s - loss: 0.0293 - mse: 0.0293 - 6ms/epoch - 6ms/step\n",
      "Epoch 279/500\n",
      "1/1 - 0s - loss: 0.0292 - mse: 0.0292 - 11ms/epoch - 11ms/step\n",
      "Epoch 280/500\n",
      "1/1 - 0s - loss: 0.0291 - mse: 0.0291 - 7ms/epoch - 7ms/step\n",
      "Epoch 281/500\n",
      "1/1 - 0s - loss: 0.0290 - mse: 0.0290 - 32ms/epoch - 32ms/step\n",
      "Epoch 282/500\n",
      "1/1 - 0s - loss: 0.0288 - mse: 0.0288 - 30ms/epoch - 30ms/step\n",
      "Epoch 283/500\n",
      "1/1 - 0s - loss: 0.0287 - mse: 0.0287 - 15ms/epoch - 15ms/step\n",
      "Epoch 284/500\n",
      "1/1 - 0s - loss: 0.0286 - mse: 0.0286 - 6ms/epoch - 6ms/step\n",
      "Epoch 285/500\n",
      "1/1 - 0s - loss: 0.0285 - mse: 0.0285 - 6ms/epoch - 6ms/step\n",
      "Epoch 286/500\n",
      "1/1 - 0s - loss: 0.0284 - mse: 0.0284 - 8ms/epoch - 8ms/step\n",
      "Epoch 287/500\n",
      "1/1 - 0s - loss: 0.0283 - mse: 0.0283 - 9ms/epoch - 9ms/step\n",
      "Epoch 288/500\n",
      "1/1 - 0s - loss: 0.0281 - mse: 0.0281 - 6ms/epoch - 6ms/step\n",
      "Epoch 289/500\n",
      "1/1 - 0s - loss: 0.0280 - mse: 0.0280 - 7ms/epoch - 7ms/step\n",
      "Epoch 290/500\n",
      "1/1 - 0s - loss: 0.0279 - mse: 0.0279 - 6ms/epoch - 6ms/step\n",
      "Epoch 291/500\n",
      "1/1 - 0s - loss: 0.0278 - mse: 0.0278 - 7ms/epoch - 7ms/step\n",
      "Epoch 292/500\n",
      "1/1 - 0s - loss: 0.0277 - mse: 0.0277 - 5ms/epoch - 5ms/step\n",
      "Epoch 293/500\n",
      "1/1 - 0s - loss: 0.0276 - mse: 0.0276 - 23ms/epoch - 23ms/step\n",
      "Epoch 294/500\n",
      "1/1 - 0s - loss: 0.0275 - mse: 0.0275 - 26ms/epoch - 26ms/step\n",
      "Epoch 295/500\n",
      "1/1 - 0s - loss: 0.0274 - mse: 0.0274 - 27ms/epoch - 27ms/step\n",
      "Epoch 296/500\n",
      "1/1 - 0s - loss: 0.0273 - mse: 0.0273 - 20ms/epoch - 20ms/step\n",
      "Epoch 297/500\n",
      "1/1 - 0s - loss: 0.0272 - mse: 0.0272 - 6ms/epoch - 6ms/step\n",
      "Epoch 298/500\n",
      "1/1 - 0s - loss: 0.0271 - mse: 0.0271 - 18ms/epoch - 18ms/step\n",
      "Epoch 299/500\n",
      "1/1 - 0s - loss: 0.0270 - mse: 0.0270 - 70ms/epoch - 70ms/step\n",
      "Epoch 300/500\n",
      "1/1 - 0s - loss: 0.0269 - mse: 0.0269 - 8ms/epoch - 8ms/step\n",
      "Epoch 301/500\n",
      "1/1 - 0s - loss: 0.0267 - mse: 0.0267 - 9ms/epoch - 9ms/step\n",
      "Epoch 302/500\n",
      "1/1 - 0s - loss: 0.0266 - mse: 0.0266 - 6ms/epoch - 6ms/step\n",
      "Epoch 303/500\n",
      "1/1 - 0s - loss: 0.0265 - mse: 0.0265 - 13ms/epoch - 13ms/step\n",
      "Epoch 304/500\n",
      "1/1 - 0s - loss: 0.0264 - mse: 0.0264 - 7ms/epoch - 7ms/step\n",
      "Epoch 305/500\n",
      "1/1 - 0s - loss: 0.0263 - mse: 0.0263 - 5ms/epoch - 5ms/step\n",
      "Epoch 306/500\n",
      "1/1 - 0s - loss: 0.0262 - mse: 0.0262 - 18ms/epoch - 18ms/step\n",
      "Epoch 307/500\n",
      "1/1 - 0s - loss: 0.0261 - mse: 0.0261 - 6ms/epoch - 6ms/step\n",
      "Epoch 308/500\n",
      "1/1 - 0s - loss: 0.0260 - mse: 0.0260 - 8ms/epoch - 8ms/step\n",
      "Epoch 309/500\n",
      "1/1 - 0s - loss: 0.0260 - mse: 0.0260 - 8ms/epoch - 8ms/step\n",
      "Epoch 310/500\n",
      "1/1 - 0s - loss: 0.0259 - mse: 0.0259 - 30ms/epoch - 30ms/step\n",
      "Epoch 311/500\n",
      "1/1 - 0s - loss: 0.0258 - mse: 0.0258 - 8ms/epoch - 8ms/step\n",
      "Epoch 312/500\n",
      "1/1 - 0s - loss: 0.0257 - mse: 0.0257 - 8ms/epoch - 8ms/step\n",
      "Epoch 313/500\n",
      "1/1 - 0s - loss: 0.0256 - mse: 0.0256 - 6ms/epoch - 6ms/step\n",
      "Epoch 314/500\n",
      "1/1 - 0s - loss: 0.0255 - mse: 0.0255 - 7ms/epoch - 7ms/step\n",
      "Epoch 315/500\n",
      "1/1 - 0s - loss: 0.0254 - mse: 0.0254 - 27ms/epoch - 27ms/step\n",
      "Epoch 316/500\n",
      "1/1 - 0s - loss: 0.0253 - mse: 0.0253 - 26ms/epoch - 26ms/step\n",
      "Epoch 317/500\n",
      "1/1 - 0s - loss: 0.0252 - mse: 0.0252 - 21ms/epoch - 21ms/step\n",
      "Epoch 318/500\n",
      "1/1 - 0s - loss: 0.0251 - mse: 0.0251 - 11ms/epoch - 11ms/step\n",
      "Epoch 319/500\n",
      "1/1 - 0s - loss: 0.0250 - mse: 0.0250 - 7ms/epoch - 7ms/step\n",
      "Epoch 320/500\n",
      "1/1 - 0s - loss: 0.0249 - mse: 0.0249 - 6ms/epoch - 6ms/step\n",
      "Epoch 321/500\n",
      "1/1 - 0s - loss: 0.0248 - mse: 0.0248 - 31ms/epoch - 31ms/step\n",
      "Epoch 322/500\n",
      "1/1 - 0s - loss: 0.0247 - mse: 0.0247 - 9ms/epoch - 9ms/step\n",
      "Epoch 323/500\n",
      "1/1 - 0s - loss: 0.0247 - mse: 0.0247 - 6ms/epoch - 6ms/step\n",
      "Epoch 324/500\n",
      "1/1 - 0s - loss: 0.0246 - mse: 0.0246 - 6ms/epoch - 6ms/step\n",
      "Epoch 325/500\n",
      "1/1 - 0s - loss: 0.0245 - mse: 0.0245 - 20ms/epoch - 20ms/step\n",
      "Epoch 326/500\n",
      "1/1 - 0s - loss: 0.0244 - mse: 0.0244 - 6ms/epoch - 6ms/step\n",
      "Epoch 327/500\n",
      "1/1 - 0s - loss: 0.0243 - mse: 0.0243 - 7ms/epoch - 7ms/step\n",
      "Epoch 328/500\n",
      "1/1 - 0s - loss: 0.0242 - mse: 0.0242 - 6ms/epoch - 6ms/step\n",
      "Epoch 329/500\n",
      "1/1 - 0s - loss: 0.0241 - mse: 0.0241 - 9ms/epoch - 9ms/step\n",
      "Epoch 330/500\n",
      "1/1 - 0s - loss: 0.0241 - mse: 0.0241 - 18ms/epoch - 18ms/step\n",
      "Epoch 331/500\n",
      "1/1 - 0s - loss: 0.0240 - mse: 0.0240 - 6ms/epoch - 6ms/step\n",
      "Epoch 332/500\n",
      "1/1 - 0s - loss: 0.0239 - mse: 0.0239 - 28ms/epoch - 28ms/step\n",
      "Epoch 333/500\n",
      "1/1 - 0s - loss: 0.0238 - mse: 0.0238 - 29ms/epoch - 29ms/step\n",
      "Epoch 334/500\n",
      "1/1 - 0s - loss: 0.0237 - mse: 0.0237 - 23ms/epoch - 23ms/step\n",
      "Epoch 335/500\n",
      "1/1 - 0s - loss: 0.0236 - mse: 0.0236 - 24ms/epoch - 24ms/step\n",
      "Epoch 336/500\n",
      "1/1 - 0s - loss: 0.0236 - mse: 0.0236 - 7ms/epoch - 7ms/step\n",
      "Epoch 337/500\n",
      "1/1 - 0s - loss: 0.0235 - mse: 0.0235 - 22ms/epoch - 22ms/step\n",
      "Epoch 338/500\n",
      "1/1 - 0s - loss: 0.0234 - mse: 0.0234 - 6ms/epoch - 6ms/step\n",
      "Epoch 339/500\n",
      "1/1 - 0s - loss: 0.0233 - mse: 0.0233 - 6ms/epoch - 6ms/step\n",
      "Epoch 340/500\n",
      "1/1 - 0s - loss: 0.0232 - mse: 0.0232 - 6ms/epoch - 6ms/step\n",
      "Epoch 341/500\n",
      "1/1 - 0s - loss: 0.0232 - mse: 0.0232 - 23ms/epoch - 23ms/step\n",
      "Epoch 342/500\n",
      "1/1 - 0s - loss: 0.0231 - mse: 0.0231 - 8ms/epoch - 8ms/step\n",
      "Epoch 343/500\n",
      "1/1 - 0s - loss: 0.0230 - mse: 0.0230 - 6ms/epoch - 6ms/step\n",
      "Epoch 344/500\n",
      "1/1 - 0s - loss: 0.0229 - mse: 0.0229 - 6ms/epoch - 6ms/step\n",
      "Epoch 345/500\n",
      "1/1 - 0s - loss: 0.0229 - mse: 0.0229 - 9ms/epoch - 9ms/step\n",
      "Epoch 346/500\n",
      "1/1 - 0s - loss: 0.0228 - mse: 0.0228 - 25ms/epoch - 25ms/step\n",
      "Epoch 347/500\n",
      "1/1 - 0s - loss: 0.0227 - mse: 0.0227 - 12ms/epoch - 12ms/step\n",
      "Epoch 348/500\n",
      "1/1 - 0s - loss: 0.0226 - mse: 0.0226 - 7ms/epoch - 7ms/step\n",
      "Epoch 349/500\n",
      "1/1 - 0s - loss: 0.0226 - mse: 0.0226 - 12ms/epoch - 12ms/step\n",
      "Epoch 350/500\n",
      "1/1 - 0s - loss: 0.0225 - mse: 0.0225 - 14ms/epoch - 14ms/step\n",
      "Epoch 351/500\n",
      "1/1 - 0s - loss: 0.0224 - mse: 0.0224 - 22ms/epoch - 22ms/step\n",
      "Epoch 352/500\n",
      "1/1 - 0s - loss: 0.0223 - mse: 0.0223 - 15ms/epoch - 15ms/step\n",
      "Epoch 353/500\n",
      "1/1 - 0s - loss: 0.0223 - mse: 0.0223 - 10ms/epoch - 10ms/step\n",
      "Epoch 354/500\n",
      "1/1 - 0s - loss: 0.0222 - mse: 0.0222 - 10ms/epoch - 10ms/step\n",
      "Epoch 355/500\n",
      "1/1 - 0s - loss: 0.0221 - mse: 0.0221 - 10ms/epoch - 10ms/step\n",
      "Epoch 356/500\n",
      "1/1 - 0s - loss: 0.0220 - mse: 0.0220 - 10ms/epoch - 10ms/step\n",
      "Epoch 357/500\n",
      "1/1 - 0s - loss: 0.0220 - mse: 0.0220 - 7ms/epoch - 7ms/step\n",
      "Epoch 358/500\n",
      "1/1 - 0s - loss: 0.0219 - mse: 0.0219 - 6ms/epoch - 6ms/step\n",
      "Epoch 359/500\n",
      "1/1 - 0s - loss: 0.0218 - mse: 0.0218 - 6ms/epoch - 6ms/step\n",
      "Epoch 360/500\n",
      "1/1 - 0s - loss: 0.0218 - mse: 0.0218 - 8ms/epoch - 8ms/step\n",
      "Epoch 361/500\n",
      "1/1 - 0s - loss: 0.0217 - mse: 0.0217 - 6ms/epoch - 6ms/step\n",
      "Epoch 362/500\n",
      "1/1 - 0s - loss: 0.0216 - mse: 0.0216 - 10ms/epoch - 10ms/step\n",
      "Epoch 363/500\n",
      "1/1 - 0s - loss: 0.0216 - mse: 0.0216 - 11ms/epoch - 11ms/step\n",
      "Epoch 364/500\n",
      "1/1 - 0s - loss: 0.0215 - mse: 0.0215 - 10ms/epoch - 10ms/step\n",
      "Epoch 365/500\n",
      "1/1 - 0s - loss: 0.0214 - mse: 0.0214 - 0s/epoch - 0s/step\n",
      "Epoch 366/500\n",
      "1/1 - 0s - loss: 0.0214 - mse: 0.0214 - 11ms/epoch - 11ms/step\n",
      "Epoch 367/500\n",
      "1/1 - 0s - loss: 0.0213 - mse: 0.0213 - 9ms/epoch - 9ms/step\n",
      "Epoch 368/500\n",
      "1/1 - 0s - loss: 0.0212 - mse: 0.0212 - 8ms/epoch - 8ms/step\n",
      "Epoch 369/500\n",
      "1/1 - 0s - loss: 0.0212 - mse: 0.0212 - 8ms/epoch - 8ms/step\n",
      "Epoch 370/500\n",
      "1/1 - 0s - loss: 0.0211 - mse: 0.0211 - 7ms/epoch - 7ms/step\n",
      "Epoch 371/500\n",
      "1/1 - 0s - loss: 0.0210 - mse: 0.0210 - 9ms/epoch - 9ms/step\n",
      "Epoch 372/500\n",
      "1/1 - 0s - loss: 0.0210 - mse: 0.0210 - 22ms/epoch - 22ms/step\n",
      "Epoch 373/500\n",
      "1/1 - 0s - loss: 0.0209 - mse: 0.0209 - 15ms/epoch - 15ms/step\n",
      "Epoch 374/500\n",
      "1/1 - 0s - loss: 0.0208 - mse: 0.0208 - 25ms/epoch - 25ms/step\n",
      "Epoch 375/500\n",
      "1/1 - 0s - loss: 0.0208 - mse: 0.0208 - 7ms/epoch - 7ms/step\n",
      "Epoch 376/500\n",
      "1/1 - 0s - loss: 0.0207 - mse: 0.0207 - 7ms/epoch - 7ms/step\n",
      "Epoch 377/500\n",
      "1/1 - 0s - loss: 0.0206 - mse: 0.0206 - 8ms/epoch - 8ms/step\n",
      "Epoch 378/500\n",
      "1/1 - 0s - loss: 0.0206 - mse: 0.0206 - 7ms/epoch - 7ms/step\n",
      "Epoch 379/500\n",
      "1/1 - 0s - loss: 0.0205 - mse: 0.0205 - 7ms/epoch - 7ms/step\n",
      "Epoch 380/500\n",
      "1/1 - 0s - loss: 0.0205 - mse: 0.0205 - 10ms/epoch - 10ms/step\n",
      "Epoch 381/500\n",
      "1/1 - 0s - loss: 0.0204 - mse: 0.0204 - 7ms/epoch - 7ms/step\n",
      "Epoch 382/500\n",
      "1/1 - 0s - loss: 0.0203 - mse: 0.0203 - 6ms/epoch - 6ms/step\n",
      "Epoch 383/500\n",
      "1/1 - 0s - loss: 0.0203 - mse: 0.0203 - 7ms/epoch - 7ms/step\n",
      "Epoch 384/500\n",
      "1/1 - 0s - loss: 0.0202 - mse: 0.0202 - 23ms/epoch - 23ms/step\n",
      "Epoch 385/500\n",
      "1/1 - 0s - loss: 0.0202 - mse: 0.0202 - 78ms/epoch - 78ms/step\n",
      "Epoch 386/500\n",
      "1/1 - 0s - loss: 0.0201 - mse: 0.0201 - 8ms/epoch - 8ms/step\n",
      "Epoch 387/500\n",
      "1/1 - 0s - loss: 0.0200 - mse: 0.0200 - 7ms/epoch - 7ms/step\n",
      "Epoch 388/500\n",
      "1/1 - 0s - loss: 0.0200 - mse: 0.0200 - 21ms/epoch - 21ms/step\n",
      "Epoch 389/500\n",
      "1/1 - 0s - loss: 0.0199 - mse: 0.0199 - 24ms/epoch - 24ms/step\n",
      "Epoch 390/500\n",
      "1/1 - 0s - loss: 0.0199 - mse: 0.0199 - 5ms/epoch - 5ms/step\n",
      "Epoch 391/500\n",
      "1/1 - 0s - loss: 0.0198 - mse: 0.0198 - 6ms/epoch - 6ms/step\n",
      "Epoch 392/500\n",
      "1/1 - 0s - loss: 0.0197 - mse: 0.0197 - 5ms/epoch - 5ms/step\n",
      "Epoch 393/500\n",
      "1/1 - 0s - loss: 0.0197 - mse: 0.0197 - 7ms/epoch - 7ms/step\n",
      "Epoch 394/500\n",
      "1/1 - 0s - loss: 0.0196 - mse: 0.0196 - 5ms/epoch - 5ms/step\n",
      "Epoch 395/500\n",
      "1/1 - 0s - loss: 0.0196 - mse: 0.0196 - 11ms/epoch - 11ms/step\n",
      "Epoch 396/500\n",
      "1/1 - 0s - loss: 0.0195 - mse: 0.0195 - 8ms/epoch - 8ms/step\n",
      "Epoch 397/500\n",
      "1/1 - 0s - loss: 0.0195 - mse: 0.0195 - 26ms/epoch - 26ms/step\n",
      "Epoch 398/500\n",
      "1/1 - 0s - loss: 0.0194 - mse: 0.0194 - 9ms/epoch - 9ms/step\n",
      "Epoch 399/500\n",
      "1/1 - 0s - loss: 0.0193 - mse: 0.0193 - 7ms/epoch - 7ms/step\n",
      "Epoch 400/500\n",
      "1/1 - 0s - loss: 0.0193 - mse: 0.0193 - 10ms/epoch - 10ms/step\n",
      "Epoch 401/500\n",
      "1/1 - 0s - loss: 0.0192 - mse: 0.0192 - 6ms/epoch - 6ms/step\n",
      "Epoch 402/500\n",
      "1/1 - 0s - loss: 0.0192 - mse: 0.0192 - 6ms/epoch - 6ms/step\n",
      "Epoch 403/500\n",
      "1/1 - 0s - loss: 0.0191 - mse: 0.0191 - 6ms/epoch - 6ms/step\n",
      "Epoch 404/500\n",
      "1/1 - 0s - loss: 0.0191 - mse: 0.0191 - 0s/epoch - 0s/step\n",
      "Epoch 405/500\n",
      "1/1 - 0s - loss: 0.0190 - mse: 0.0190 - 9ms/epoch - 9ms/step\n",
      "Epoch 406/500\n",
      "1/1 - 0s - loss: 0.0190 - mse: 0.0190 - 14ms/epoch - 14ms/step\n",
      "Epoch 407/500\n",
      "1/1 - 0s - loss: 0.0189 - mse: 0.0189 - 8ms/epoch - 8ms/step\n",
      "Epoch 408/500\n",
      "1/1 - 0s - loss: 0.0189 - mse: 0.0189 - 9ms/epoch - 9ms/step\n",
      "Epoch 409/500\n",
      "1/1 - 0s - loss: 0.0188 - mse: 0.0188 - 10ms/epoch - 10ms/step\n",
      "Epoch 410/500\n",
      "1/1 - 0s - loss: 0.0188 - mse: 0.0188 - 16ms/epoch - 16ms/step\n",
      "Epoch 411/500\n",
      "1/1 - 0s - loss: 0.0187 - mse: 0.0187 - 9ms/epoch - 9ms/step\n",
      "Epoch 412/500\n",
      "1/1 - 0s - loss: 0.0186 - mse: 0.0186 - 23ms/epoch - 23ms/step\n",
      "Epoch 413/500\n",
      "1/1 - 0s - loss: 0.0186 - mse: 0.0186 - 7ms/epoch - 7ms/step\n",
      "Epoch 414/500\n",
      "1/1 - 0s - loss: 0.0185 - mse: 0.0185 - 24ms/epoch - 24ms/step\n",
      "Epoch 415/500\n",
      "1/1 - 0s - loss: 0.0185 - mse: 0.0185 - 8ms/epoch - 8ms/step\n",
      "Epoch 416/500\n",
      "1/1 - 0s - loss: 0.0184 - mse: 0.0184 - 9ms/epoch - 9ms/step\n",
      "Epoch 417/500\n",
      "1/1 - 0s - loss: 0.0184 - mse: 0.0184 - 7ms/epoch - 7ms/step\n",
      "Epoch 418/500\n",
      "1/1 - 0s - loss: 0.0183 - mse: 0.0183 - 7ms/epoch - 7ms/step\n",
      "Epoch 419/500\n",
      "1/1 - 0s - loss: 0.0183 - mse: 0.0183 - 11ms/epoch - 11ms/step\n",
      "Epoch 420/500\n",
      "1/1 - 0s - loss: 0.0182 - mse: 0.0182 - 8ms/epoch - 8ms/step\n",
      "Epoch 421/500\n",
      "1/1 - 0s - loss: 0.0182 - mse: 0.0182 - 9ms/epoch - 9ms/step\n",
      "Epoch 422/500\n",
      "1/1 - 0s - loss: 0.0181 - mse: 0.0181 - 8ms/epoch - 8ms/step\n",
      "Epoch 423/500\n",
      "1/1 - 0s - loss: 0.0181 - mse: 0.0181 - 94ms/epoch - 94ms/step\n",
      "Epoch 424/500\n",
      "1/1 - 0s - loss: 0.0180 - mse: 0.0180 - 8ms/epoch - 8ms/step\n",
      "Epoch 425/500\n",
      "1/1 - 0s - loss: 0.0180 - mse: 0.0180 - 9ms/epoch - 9ms/step\n",
      "Epoch 426/500\n",
      "1/1 - 0s - loss: 0.0180 - mse: 0.0180 - 7ms/epoch - 7ms/step\n",
      "Epoch 427/500\n",
      "1/1 - 0s - loss: 0.0179 - mse: 0.0179 - 7ms/epoch - 7ms/step\n",
      "Epoch 428/500\n",
      "1/1 - 0s - loss: 0.0179 - mse: 0.0179 - 8ms/epoch - 8ms/step\n",
      "Epoch 429/500\n",
      "1/1 - 0s - loss: 0.0178 - mse: 0.0178 - 8ms/epoch - 8ms/step\n",
      "Epoch 430/500\n",
      "1/1 - 0s - loss: 0.0178 - mse: 0.0178 - 8ms/epoch - 8ms/step\n",
      "Epoch 431/500\n",
      "1/1 - 0s - loss: 0.0177 - mse: 0.0177 - 17ms/epoch - 17ms/step\n",
      "Epoch 432/500\n",
      "1/1 - 0s - loss: 0.0177 - mse: 0.0177 - 12ms/epoch - 12ms/step\n",
      "Epoch 433/500\n",
      "1/1 - 0s - loss: 0.0176 - mse: 0.0176 - 10ms/epoch - 10ms/step\n",
      "Epoch 434/500\n",
      "1/1 - 0s - loss: 0.0176 - mse: 0.0176 - 7ms/epoch - 7ms/step\n",
      "Epoch 435/500\n",
      "1/1 - 0s - loss: 0.0175 - mse: 0.0175 - 12ms/epoch - 12ms/step\n",
      "Epoch 436/500\n",
      "1/1 - 0s - loss: 0.0175 - mse: 0.0175 - 7ms/epoch - 7ms/step\n",
      "Epoch 437/500\n",
      "1/1 - 0s - loss: 0.0174 - mse: 0.0174 - 6ms/epoch - 6ms/step\n",
      "Epoch 438/500\n",
      "1/1 - 0s - loss: 0.0174 - mse: 0.0174 - 8ms/epoch - 8ms/step\n",
      "Epoch 439/500\n",
      "1/1 - 0s - loss: 0.0173 - mse: 0.0173 - 7ms/epoch - 7ms/step\n",
      "Epoch 440/500\n",
      "1/1 - 0s - loss: 0.0173 - mse: 0.0173 - 24ms/epoch - 24ms/step\n",
      "Epoch 441/500\n",
      "1/1 - 0s - loss: 0.0173 - mse: 0.0173 - 7ms/epoch - 7ms/step\n",
      "Epoch 442/500\n",
      "1/1 - 0s - loss: 0.0172 - mse: 0.0172 - 6ms/epoch - 6ms/step\n",
      "Epoch 443/500\n",
      "1/1 - 0s - loss: 0.0172 - mse: 0.0172 - 15ms/epoch - 15ms/step\n",
      "Epoch 444/500\n",
      "1/1 - 0s - loss: 0.0171 - mse: 0.0171 - 13ms/epoch - 13ms/step\n",
      "Epoch 445/500\n",
      "1/1 - 0s - loss: 0.0171 - mse: 0.0171 - 6ms/epoch - 6ms/step\n",
      "Epoch 446/500\n",
      "1/1 - 0s - loss: 0.0170 - mse: 0.0170 - 7ms/epoch - 7ms/step\n",
      "Epoch 447/500\n",
      "1/1 - 0s - loss: 0.0170 - mse: 0.0170 - 18ms/epoch - 18ms/step\n",
      "Epoch 448/500\n",
      "1/1 - 0s - loss: 0.0169 - mse: 0.0169 - 5ms/epoch - 5ms/step\n",
      "Epoch 449/500\n",
      "1/1 - 0s - loss: 0.0169 - mse: 0.0169 - 9ms/epoch - 9ms/step\n",
      "Epoch 450/500\n",
      "1/1 - 0s - loss: 0.0169 - mse: 0.0169 - 4ms/epoch - 4ms/step\n",
      "Epoch 451/500\n",
      "1/1 - 0s - loss: 0.0168 - mse: 0.0168 - 6ms/epoch - 6ms/step\n",
      "Epoch 452/500\n",
      "1/1 - 0s - loss: 0.0168 - mse: 0.0168 - 6ms/epoch - 6ms/step\n",
      "Epoch 453/500\n",
      "1/1 - 0s - loss: 0.0167 - mse: 0.0167 - 6ms/epoch - 6ms/step\n",
      "Epoch 454/500\n",
      "1/1 - 0s - loss: 0.0167 - mse: 0.0167 - 6ms/epoch - 6ms/step\n",
      "Epoch 455/500\n",
      "1/1 - 0s - loss: 0.0167 - mse: 0.0167 - 6ms/epoch - 6ms/step\n",
      "Epoch 456/500\n",
      "1/1 - 0s - loss: 0.0166 - mse: 0.0166 - 6ms/epoch - 6ms/step\n",
      "Epoch 457/500\n",
      "1/1 - 0s - loss: 0.0166 - mse: 0.0166 - 62ms/epoch - 62ms/step\n",
      "Epoch 458/500\n",
      "1/1 - 0s - loss: 0.0165 - mse: 0.0165 - 8ms/epoch - 8ms/step\n",
      "Epoch 459/500\n",
      "1/1 - 0s - loss: 0.0165 - mse: 0.0165 - 28ms/epoch - 28ms/step\n",
      "Epoch 460/500\n",
      "1/1 - 0s - loss: 0.0164 - mse: 0.0164 - 15ms/epoch - 15ms/step\n",
      "Epoch 461/500\n",
      "1/1 - 0s - loss: 0.0164 - mse: 0.0164 - 7ms/epoch - 7ms/step\n",
      "Epoch 462/500\n",
      "1/1 - 0s - loss: 0.0164 - mse: 0.0164 - 6ms/epoch - 6ms/step\n",
      "Epoch 463/500\n",
      "1/1 - 0s - loss: 0.0163 - mse: 0.0163 - 7ms/epoch - 7ms/step\n",
      "Epoch 464/500\n",
      "1/1 - 0s - loss: 0.0163 - mse: 0.0163 - 8ms/epoch - 8ms/step\n",
      "Epoch 465/500\n",
      "1/1 - 0s - loss: 0.0162 - mse: 0.0162 - 21ms/epoch - 21ms/step\n",
      "Epoch 466/500\n",
      "1/1 - 0s - loss: 0.0162 - mse: 0.0162 - 20ms/epoch - 20ms/step\n",
      "Epoch 467/500\n",
      "1/1 - 0s - loss: 0.0162 - mse: 0.0162 - 6ms/epoch - 6ms/step\n",
      "Epoch 468/500\n",
      "1/1 - 0s - loss: 0.0161 - mse: 0.0161 - 8ms/epoch - 8ms/step\n",
      "Epoch 469/500\n",
      "1/1 - 0s - loss: 0.0161 - mse: 0.0161 - 5ms/epoch - 5ms/step\n",
      "Epoch 470/500\n",
      "1/1 - 0s - loss: 0.0160 - mse: 0.0160 - 7ms/epoch - 7ms/step\n",
      "Epoch 471/500\n",
      "1/1 - 0s - loss: 0.0160 - mse: 0.0160 - 21ms/epoch - 21ms/step\n",
      "Epoch 472/500\n",
      "1/1 - 0s - loss: 0.0160 - mse: 0.0160 - 11ms/epoch - 11ms/step\n",
      "Epoch 473/500\n",
      "1/1 - 0s - loss: 0.0159 - mse: 0.0159 - 8ms/epoch - 8ms/step\n",
      "Epoch 474/500\n",
      "1/1 - 0s - loss: 0.0159 - mse: 0.0159 - 29ms/epoch - 29ms/step\n",
      "Epoch 475/500\n",
      "1/1 - 0s - loss: 0.0159 - mse: 0.0159 - 16ms/epoch - 16ms/step\n",
      "Epoch 476/500\n",
      "1/1 - 0s - loss: 0.0158 - mse: 0.0158 - 6ms/epoch - 6ms/step\n",
      "Epoch 477/500\n",
      "1/1 - 0s - loss: 0.0158 - mse: 0.0158 - 13ms/epoch - 13ms/step\n",
      "Epoch 478/500\n",
      "1/1 - 0s - loss: 0.0157 - mse: 0.0157 - 6ms/epoch - 6ms/step\n",
      "Epoch 479/500\n",
      "1/1 - 0s - loss: 0.0157 - mse: 0.0157 - 8ms/epoch - 8ms/step\n",
      "Epoch 480/500\n",
      "1/1 - 0s - loss: 0.0157 - mse: 0.0157 - 17ms/epoch - 17ms/step\n",
      "Epoch 481/500\n",
      "1/1 - 0s - loss: 0.0156 - mse: 0.0156 - 6ms/epoch - 6ms/step\n",
      "Epoch 482/500\n",
      "1/1 - 0s - loss: 0.0156 - mse: 0.0156 - 8ms/epoch - 8ms/step\n",
      "Epoch 483/500\n",
      "1/1 - 0s - loss: 0.0156 - mse: 0.0156 - 8ms/epoch - 8ms/step\n",
      "Epoch 484/500\n",
      "1/1 - 0s - loss: 0.0155 - mse: 0.0155 - 6ms/epoch - 6ms/step\n",
      "Epoch 485/500\n",
      "1/1 - 0s - loss: 0.0155 - mse: 0.0155 - 23ms/epoch - 23ms/step\n",
      "Epoch 486/500\n",
      "1/1 - 0s - loss: 0.0154 - mse: 0.0154 - 17ms/epoch - 17ms/step\n",
      "Epoch 487/500\n",
      "1/1 - 0s - loss: 0.0154 - mse: 0.0154 - 12ms/epoch - 12ms/step\n",
      "Epoch 488/500\n",
      "1/1 - 0s - loss: 0.0154 - mse: 0.0154 - 8ms/epoch - 8ms/step\n",
      "Epoch 489/500\n",
      "1/1 - 0s - loss: 0.0153 - mse: 0.0153 - 274ms/epoch - 274ms/step\n",
      "Epoch 490/500\n",
      "1/1 - 0s - loss: 0.0153 - mse: 0.0153 - 8ms/epoch - 8ms/step\n",
      "Epoch 491/500\n",
      "1/1 - 0s - loss: 0.0153 - mse: 0.0153 - 8ms/epoch - 8ms/step\n",
      "Epoch 492/500\n",
      "1/1 - 0s - loss: 0.0152 - mse: 0.0152 - 6ms/epoch - 6ms/step\n",
      "Epoch 493/500\n",
      "1/1 - 0s - loss: 0.0152 - mse: 0.0152 - 7ms/epoch - 7ms/step\n",
      "Epoch 494/500\n",
      "1/1 - 0s - loss: 0.0152 - mse: 0.0152 - 15ms/epoch - 15ms/step\n",
      "Epoch 495/500\n",
      "1/1 - 0s - loss: 0.0151 - mse: 0.0151 - 9ms/epoch - 9ms/step\n",
      "Epoch 496/500\n",
      "1/1 - 0s - loss: 0.0151 - mse: 0.0151 - 7ms/epoch - 7ms/step\n",
      "Epoch 497/500\n",
      "1/1 - 0s - loss: 0.0151 - mse: 0.0151 - 6ms/epoch - 6ms/step\n",
      "Epoch 498/500\n",
      "1/1 - 0s - loss: 0.0150 - mse: 0.0150 - 21ms/epoch - 21ms/step\n",
      "Epoch 499/500\n",
      "1/1 - 0s - loss: 0.0150 - mse: 0.0150 - 20ms/epoch - 20ms/step\n",
      "Epoch 500/500\n",
      "1/1 - 0s - loss: 0.0150 - mse: 0.0150 - 23ms/epoch - 23ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "[[-0.8161051]\n",
      " [ 0.8862374]\n",
      " [ 0.8862374]\n",
      " [ 0.9992635]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# OR 데이터 구축\n",
    "x=[[0.0,0.0],[0.0,1.0],[1.0,0.0],[1.0,1.0]]\n",
    "y=[[-1],[1],[1],[1]]\n",
    "\n",
    "n_input=2\n",
    "n_output=1\n",
    "\n",
    "perceptron=Sequential()\n",
    "perceptron.add(Dense(units=n_output,activation='tanh',input_shape=(n_input,),kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "\n",
    "perceptron.compile(loss='mse',optimizer=SGD(learning_rate=0.1),metrics=['mse'])\n",
    "perceptron.fit(x,y,epochs=500,verbose=2)\n",
    "\n",
    "res=perceptron.predict(x)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uWV_bqyeHkzF",
   "metadata": {
    "id": "uWV_bqyeHkzF"
   },
   "source": [
    "# LeNet-5 사례 [LeCun1998] 재현해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "J50nGPjtHkzG",
   "metadata": {
    "id": "J50nGPjtHkzG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 6)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 16)        2416      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 120)         48120     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5880)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                494004    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 545546 (2.08 MB)\n",
      "Trainable params: 545546 (2.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "469/469 - 51s - loss: 0.2150 - accuracy: 0.9350 - val_loss: 0.0614 - val_accuracy: 0.9799 - 51s/epoch - 109ms/step\n",
      "Epoch 2/30\n",
      "469/469 - 45s - loss: 0.0560 - accuracy: 0.9829 - val_loss: 0.0350 - val_accuracy: 0.9882 - 45s/epoch - 95ms/step\n",
      "Epoch 3/30\n",
      "469/469 - 45s - loss: 0.0406 - accuracy: 0.9873 - val_loss: 0.0402 - val_accuracy: 0.9868 - 45s/epoch - 96ms/step\n",
      "Epoch 4/30\n",
      "469/469 - 46s - loss: 0.0305 - accuracy: 0.9907 - val_loss: 0.0313 - val_accuracy: 0.9889 - 46s/epoch - 98ms/step\n",
      "Epoch 5/30\n",
      "469/469 - 46s - loss: 0.0233 - accuracy: 0.9925 - val_loss: 0.0309 - val_accuracy: 0.9899 - 46s/epoch - 98ms/step\n",
      "Epoch 6/30\n",
      "469/469 - 48s - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.0272 - val_accuracy: 0.9909 - 48s/epoch - 103ms/step\n",
      "Epoch 7/30\n",
      "469/469 - 46s - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.0298 - val_accuracy: 0.9911 - 46s/epoch - 98ms/step\n",
      "Epoch 8/30\n",
      "469/469 - 45s - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0295 - val_accuracy: 0.9910 - 45s/epoch - 97ms/step\n",
      "Epoch 9/30\n",
      "469/469 - 45s - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0259 - val_accuracy: 0.9924 - 45s/epoch - 95ms/step\n",
      "Epoch 10/30\n",
      "469/469 - 45s - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.0441 - val_accuracy: 0.9880 - 45s/epoch - 95ms/step\n",
      "Epoch 11/30\n",
      "469/469 - 44s - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.0351 - val_accuracy: 0.9912 - 44s/epoch - 95ms/step\n",
      "Epoch 12/30\n",
      "469/469 - 46s - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0319 - val_accuracy: 0.9908 - 46s/epoch - 98ms/step\n",
      "Epoch 13/30\n",
      "469/469 - 45s - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0355 - val_accuracy: 0.9905 - 45s/epoch - 96ms/step\n",
      "Epoch 14/30\n",
      "469/469 - 45s - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0331 - val_accuracy: 0.9920 - 45s/epoch - 95ms/step\n",
      "Epoch 15/30\n",
      "469/469 - 45s - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0369 - val_accuracy: 0.9901 - 45s/epoch - 96ms/step\n",
      "Epoch 16/30\n",
      "469/469 - 45s - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0364 - val_accuracy: 0.9897 - 45s/epoch - 95ms/step\n",
      "Epoch 17/30\n",
      "469/469 - 44s - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0464 - val_accuracy: 0.9884 - 44s/epoch - 95ms/step\n",
      "Epoch 18/30\n",
      "469/469 - 45s - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.0468 - val_accuracy: 0.9891 - 45s/epoch - 96ms/step\n",
      "Epoch 19/30\n",
      "469/469 - 45s - loss: 0.0054 - accuracy: 0.9981 - val_loss: 0.0371 - val_accuracy: 0.9906 - 45s/epoch - 95ms/step\n",
      "Epoch 20/30\n",
      "469/469 - 45s - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0386 - val_accuracy: 0.9918 - 45s/epoch - 95ms/step\n",
      "Epoch 21/30\n",
      "469/469 - 45s - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0426 - val_accuracy: 0.9910 - 45s/epoch - 95ms/step\n",
      "Epoch 22/30\n",
      "469/469 - 45s - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0463 - val_accuracy: 0.9899 - 45s/epoch - 96ms/step\n",
      "Epoch 23/30\n",
      "469/469 - 45s - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0349 - val_accuracy: 0.9910 - 45s/epoch - 96ms/step\n",
      "Epoch 24/30\n",
      "469/469 - 45s - loss: 0.0045 - accuracy: 0.9985 - val_loss: 0.0374 - val_accuracy: 0.9922 - 45s/epoch - 96ms/step\n",
      "Epoch 25/30\n",
      "469/469 - 44s - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0514 - val_accuracy: 0.9904 - 44s/epoch - 95ms/step\n",
      "Epoch 26/30\n",
      "469/469 - 45s - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0475 - val_accuracy: 0.9910 - 45s/epoch - 95ms/step\n",
      "Epoch 27/30\n",
      "469/469 - 45s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0706 - val_accuracy: 0.9862 - 45s/epoch - 95ms/step\n",
      "Epoch 28/30\n",
      "469/469 - 45s - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0440 - val_accuracy: 0.9905 - 45s/epoch - 96ms/step\n",
      "Epoch 29/30\n",
      "469/469 - 45s - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0509 - val_accuracy: 0.9890 - 45s/epoch - 95ms/step\n",
      "Epoch 30/30\n",
      "469/469 - 44s - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0390 - val_accuracy: 0.9917 - 44s/epoch - 95ms/step\n",
      "정확률은 99.16999936103821\n",
      "Test Accuracy: 0.9916999936103821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# MNIST 데이터셋을 읽고 신경망에 입력할 형태로 변환\n",
    "(x_train,y_train),(x_test,y_test)= mnist.load_data()\n",
    "x_train=x_train.reshape(60000,28,28,1)\n",
    "x_test=x_test.reshape(10000,28,28,1)\n",
    "x_train=x_train.astype(np.float32)/255.0\n",
    "x_test=x_test.astype(np.float32)/255.0\n",
    "y_train=tf.keras.utils.to_categorical(y_train,10)\n",
    "y_test=tf.keras.utils.to_categorical(y_test,10)\n",
    "\n",
    "# LeNet-5 신경망 모델 설계\n",
    "cnn=Sequential()\n",
    "cnn.add(Conv2D(6,(5,5),padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(Conv2D(16,(5,5),padding='same',activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(Conv2D(120,(5,5),padding='same',activation='relu'))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(84,activation='relu'))\n",
    "cnn.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# 신경망 모델 학습\n",
    "cnn.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약 정보 출력\n",
    "cnn.summary()\n",
    "\n",
    "# 모델 학습\n",
    "hist=cnn.fit(x_train,y_train,batch_size=128,epochs=30,validation_data=(x_test,y_test),verbose=2)\n",
    "\n",
    "# 신경망 모델 정확률 평가\n",
    "res=cnn.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"정확률은\",res[1]*100)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_accuracy = cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ISXilCB7HkzG",
   "metadata": {
    "id": "ISXilCB7HkzG",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 정확률 그래프\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Validation'],loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 손실 함수 그래프\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train','Validation'],loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4iSi2Js5HkzG",
   "metadata": {
    "id": "4iSi2Js5HkzG"
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "t_xwvcOQHkzG",
   "metadata": {
    "id": "t_xwvcOQHkzG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\DEV\\miniconda3\\envs\\tf38_cpu\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From C:\\Users\\k8s\\AppData\\Local\\Temp\\ipykernel_1312\\4088922680.py:37: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\DEV\\miniconda3\\envs\\tf38_cpu\\lib\\site-packages\\keras\\src\\layers\\rnn\\legacy_cells.py:793: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k8s\\AppData\\Local\\Temp\\ipykernel_1312\\4088922680.py:31: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True, reuse=tf.compat.v1.AUTO_REUSE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.6075039 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "1 loss: 1.5085233 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "2 loss: 1.4093033 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "3 loss: 1.3171793 prediction:  [[3 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lllllo\n",
      "4 loss: 1.2236831 prediction:  [[3 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  lllllo\n",
      "5 loss: 1.1050217 prediction:  [[1 3 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  illllo\n",
      "6 loss: 0.9704117 prediction:  [[1 0 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihlllo\n",
      "7 loss: 0.8194322 prediction:  [[1 0 3 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihlllo\n",
      "8 loss: 0.68514854 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "9 loss: 0.57853156 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "10 loss: 0.4874437 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "11 loss: 0.39446962 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "12 loss: 0.3192653 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "13 loss: 0.25933978 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "14 loss: 0.20846474 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "15 loss: 0.16412276 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "16 loss: 0.12654108 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "17 loss: 0.097981445 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "18 loss: 0.07669979 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "19 loss: 0.06030238 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "20 loss: 0.048035037 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "21 loss: 0.03900448 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "22 loss: 0.03170626 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "23 loss: 0.025479449 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "24 loss: 0.020603342 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "25 loss: 0.017031448 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "26 loss: 0.014312551 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "27 loss: 0.012058134 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "28 loss: 0.010187729 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "29 loss: 0.008724932 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "30 loss: 0.0076216 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "31 loss: 0.0067727496 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "32 loss: 0.0060738274 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "33 loss: 0.0054627843 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "34 loss: 0.00492849 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "35 loss: 0.0044788225 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "36 loss: 0.0041105202 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "37 loss: 0.0038069163 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "38 loss: 0.003548708 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "39 loss: 0.0033209089 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "40 loss: 0.003114837 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "41 loss: 0.0029266812 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "42 loss: 0.0027557109 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "43 loss: 0.0026033719 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "44 loss: 0.0024705192 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "45 loss: 0.0023569993 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "46 loss: 0.0022601273 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "47 loss: 0.0021750783 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "48 loss: 0.0020966488 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n",
      "49 loss: 0.0020217702 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihello\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "\n",
    "tf.compat.v1.set_random_seed(777)  # reproducibility\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "# Teach hello: hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = np.array([[[1, 0, 0, 0, 0],   # h 0\n",
    "                       [0, 1, 0, 0, 0],   # i 1\n",
    "                       [1, 0, 0, 0, 0],   # h 0\n",
    "                       [0, 0, 1, 0, 0],   # e 2\n",
    "                       [0, 0, 0, 1, 0],   # l 3\n",
    "                       [0, 0, 0, 1, 0]]])  # l 3\n",
    "\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]    # ihello\n",
    "\n",
    "num_classes = 5\n",
    "input_dim = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.placeholder(\n",
    "    tf.float32, [None, sequence_length, input_dim])  # X one-hot\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "# Define LSTM cell\n",
    "cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "# Initialize cell state\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# Run dynamic RNN\n",
    "outputs, _states = tf.compat.v1.nn.dynamic_rnn(\n",
    "    cell, X, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "dense = tf.keras.layers.Dense(num_classes, activation=None)\n",
    "outputs = dense(X_for_fc)\n",
    "\n",
    "# Reshape output for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    y_true=Y, y_pred=outputs, from_logits=True)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
    "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\tPrediction str: \", ''.join(result_str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hr8CsTdeHkzG",
   "metadata": {
    "id": "Hr8CsTdeHkzG"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ur3NE0vDHkzH",
   "metadata": {
    "id": "ur3NE0vDHkzH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 2.7780056 Prediction:                          \n",
      "1 loss: 2.537793 Prediction:                          \n",
      "2 loss: 2.4559035 Prediction:         ....rrrrr    .r..\n",
      "3 loss: 2.153276 Prediction:  a  a aoo...ourrr a a....\n",
      "4 loss: 1.9309753 Prediction:  a  a aoo..oou'rr a a ...\n",
      "5 loss: 1.6279148 Prediction:  a  a aoo. oou'rr a a ll.\n",
      "6 loss: 1.3411411 Prediction:  a  a boo. ooy'r  a aill.\n",
      "7 loss: 1.1050122 Prediction:  a  a aou.'rr''rr a airr.\n",
      "8 loss: 1.2324615 Prediction:  a  a aoy. You're a airl.\n",
      "9 loss: 0.72222763 Prediction:  a  a aoy. You're a girl.\n",
      "10 loss: 0.6113104 Prediction:  a  a aoy. You're a girl.\n",
      "11 loss: 0.522328 Prediction:  a  a boy. You're a girl.\n",
      "12 loss: 0.40611264 Prediction:  a  a boy. You're a girl.\n",
      "13 loss: 0.3112709 Prediction:  a  a boy. You're a girl.\n",
      "14 loss: 0.25604975 Prediction:  a  a boy. You're a girl.\n",
      "15 loss: 0.2084834 Prediction:  a  a boy. You're a girl.\n",
      "16 loss: 0.16851464 Prediction:  a  a boy. You're a girl.\n",
      "17 loss: 0.13938501 Prediction:  a  a boy. You're a girl.\n",
      "18 loss: 0.11678416 Prediction:  a  a boy. You're a girl.\n",
      "19 loss: 0.0978776 Prediction:  a  a boy. You're a girl.\n",
      "20 loss: 0.0840225 Prediction:  a  a boy. You're a girl.\n",
      "21 loss: 0.072022535 Prediction:  am a boy. You're a girl.\n",
      "22 loss: 0.05939061 Prediction:  am a boy. You're a girl.\n",
      "23 loss: 0.048745446 Prediction:  am a boy. You're a girl.\n",
      "24 loss: 0.040393203 Prediction:  am a boy. You're a girl.\n",
      "25 loss: 0.03336793 Prediction:  am a boy. You're a girl.\n",
      "26 loss: 0.027019894 Prediction:  am a boy. You're a girl.\n",
      "27 loss: 0.021474054 Prediction:  am a boy. You're a girl.\n",
      "28 loss: 0.016921632 Prediction:  am a boy. You're a girl.\n",
      "29 loss: 0.013467258 Prediction:  am a boy. You're a girl.\n",
      "30 loss: 0.010963209 Prediction:  am a boy. You're a girl.\n",
      "31 loss: 0.009118058 Prediction:  am a boy. You're a girl.\n",
      "32 loss: 0.007636198 Prediction:  am a boy. You're a girl.\n",
      "33 loss: 0.006424887 Prediction:  am a boy. You're a girl.\n",
      "34 loss: 0.0054656817 Prediction:  am a boy. You're a girl.\n",
      "35 loss: 0.004717193 Prediction:  am a boy. You're a girl.\n",
      "36 loss: 0.004129221 Prediction:  am a boy. You're a girl.\n",
      "37 loss: 0.0036758354 Prediction:  am a boy. You're a girl.\n",
      "38 loss: 0.0033089584 Prediction:  am a boy. You're a girl.\n",
      "39 loss: 0.0030084185 Prediction:  am a boy. You're a girl.\n",
      "40 loss: 0.0027608166 Prediction:  am a boy. You're a girl.\n",
      "41 loss: 0.0025545426 Prediction:  am a boy. You're a girl.\n",
      "42 loss: 0.002377008 Prediction:  am a boy. You're a girl.\n",
      "43 loss: 0.0022206036 Prediction:  am a boy. You're a girl.\n",
      "44 loss: 0.0020798603 Prediction:  am a boy. You're a girl.\n",
      "45 loss: 0.0019510159 Prediction:  am a boy. You're a girl.\n",
      "46 loss: 0.0018324719 Prediction:  am a boy. You're a girl.\n",
      "47 loss: 0.001723505 Prediction:  am a boy. You're a girl.\n",
      "48 loss: 0.0016235943 Prediction:  am a boy. You're a girl.\n",
      "49 loss: 0.0015325829 Prediction:  am a boy. You're a girl.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "sample = \"I am a boy. You're a girl.\"\n",
    "idx2char = list(set(sample))  # index -> char\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n",
    "\n",
    "# hyper parameters\n",
    "dic_size = len(char2idx)  # RNN input size (one hot size)\n",
    "hidden_size = len(char2idx)  # RNN output size\n",
    "num_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\n",
    "batch_size = 1  # one sample data, one batch\n",
    "sequence_length = len(sample) - 1  # number of lstm rollings (unit #)\n",
    "learning_rate = 0.1\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
    "x_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\n",
    "y_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "x_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
    "\n",
    "# LSTM layer\n",
    "lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "outputs = lstm(x_one_hot)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=None)(X_for_fc)\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    y_data, outputs, from_logits=True)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "\n",
    "        print(i, \"loss:\", l, \"Prediction:\", ''.join(result_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrjLP4xdHkzH",
   "metadata": {
    "id": "wrjLP4xdHkzH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b2963-f963-435a-a217-af92b7b80f1a",
   "metadata": {
    "id": "1a6b2963-f963-435a-a217-af92b7b80f1a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf38_cpu",
   "language": "python",
   "name": "tf38_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
